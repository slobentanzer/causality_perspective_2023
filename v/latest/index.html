<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-UK" xml:lang="en-UK">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Sebastian Lobentanzer" />
  <meta name="author" content="Pablo Rodriguez-Mier" />
  <meta name="author" content="Stefan Bauer" />
  <meta name="author" content="Julio Saez-Rodriguez" />
  <meta name="dcterms.date" content="2024-01-15" />
  <meta name="keywords" content="systems biology, biomedicine, causality, modelling, machine learning, foundation models" />
  <title>Molecular causality in the advent of foundation models</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <!--
  Manubot generated metadata rendered from header-includes-template.html.
  Suggest improvements at https://github.com/manubot/manubot/blob/main/manubot/process/header-includes-template.html
  -->
  <meta name="dc.format" content="text/html" />
  <meta property="og:type" content="article" />
  <meta name="dc.title" content="Molecular causality in the advent of foundation models" />
  <meta name="citation_title" content="Molecular causality in the advent of foundation models" />
  <meta property="og:title" content="Molecular causality in the advent of foundation models" />
  <meta property="twitter:title" content="Molecular causality in the advent of foundation models" />
  <meta name="dc.date" content="2024-01-15" />
  <meta name="citation_publication_date" content="2024-01-15" />
  <meta property="article:published_time" content="2024-01-15" />
  <meta name="dc.modified" content="2024-01-15T18:19:54+00:00" />
  <meta property="article:modified_time" content="2024-01-15T18:19:54+00:00" />
  <meta name="dc.language" content="en-UK" />
  <meta name="citation_language" content="en-UK" />
  <meta name="dc.relation.ispartof" content="Manubot" />
  <meta name="dc.publisher" content="Manubot" />
  <meta name="citation_journal_title" content="Manubot" />
  <meta name="citation_technical_report_institution" content="Manubot" />
  <meta name="citation_author" content="Sebastian Lobentanzer" />
  <meta name="citation_author_institution" content="Heidelberg University, Faculty of Medicine and Heidelberg University Hospital, Institute for Computational Biomedicine, Heidelberg, Germany" />
  <meta name="citation_author_orcid" content="0000-0003-3399-6695" />
  <meta name="twitter:creator" content="@slobentanzer" />
  <meta name="citation_author" content="Pablo Rodriguez-Mier" />
  <meta name="citation_author_institution" content="Heidelberg University, Faculty of Medicine and Heidelberg University Hospital, Institute for Computational Biomedicine, Heidelberg, Germany" />
  <meta name="citation_author_orcid" content="0000-0002-4938-4418" />
  <meta name="citation_author" content="Stefan Bauer" />
  <meta name="citation_author_institution" content="Helmholtz AI and TU Munich, Munich, Germany" />
  <meta name="citation_author_orcid" content="0000-0003-1712-060X" />
  <meta name="citation_author" content="Julio Saez-Rodriguez" />
  <meta name="citation_author_institution" content="Heidelberg University, Faculty of Medicine and Heidelberg University Hospital, Institute for Computational Biomedicine, Heidelberg, Germany" />
  <meta name="citation_author_orcid" content="0000-0002-8552-8976" />
  <meta name="twitter:creator" content="@saezlab" />
  <link rel="canonical" href="https://slobentanzer.github.io/causality_perspective_2023/" />
  <meta property="og:url" content="https://slobentanzer.github.io/causality_perspective_2023/" />
  <meta property="twitter:url" content="https://slobentanzer.github.io/causality_perspective_2023/" />
  <meta name="citation_fulltext_html_url" content="https://slobentanzer.github.io/causality_perspective_2023/" />
  <meta name="citation_pdf_url" content="https://slobentanzer.github.io/causality_perspective_2023/manuscript.pdf" />
  <link rel="alternate" type="application/pdf" href="https://slobentanzer.github.io/causality_perspective_2023/manuscript.pdf" />
  <link rel="alternate" type="text/html" href="https://slobentanzer.github.io/causality_perspective_2023/v/3339a70e28f9fe1129f5a397502b065796656af0/" />
  <meta name="manubot_html_url_versioned" content="https://slobentanzer.github.io/causality_perspective_2023/v/3339a70e28f9fe1129f5a397502b065796656af0/" />
  <meta name="manubot_pdf_url_versioned" content="https://slobentanzer.github.io/causality_perspective_2023/v/3339a70e28f9fe1129f5a397502b065796656af0/manuscript.pdf" />
  <meta property="og:type" content="article" />
  <meta property="twitter:card" content="summary_large_image" />
  <link rel="icon" type="image/png" sizes="192x192" href="https://manubot.org/favicon-192x192.png" />
  <link rel="mask-icon" href="https://manubot.org/safari-pinned-tab.svg" color="#ad1457" />
  <meta name="theme-color" content="#ad1457" />
  <!-- end Manubot generated metadata -->
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Molecular causality in the advent of foundation models</h1>
</header>
<p><small><em>
This manuscript
(<a href="https://slobentanzer.github.io/causality_perspective_2023/v/3339a70e28f9fe1129f5a397502b065796656af0/">permalink</a>)
was automatically generated
from <a href="https://github.com/slobentanzer/causality_perspective_2023/tree/3339a70e28f9fe1129f5a397502b065796656af0">slobentanzer/causality_perspective_2023@3339a70</a>
on January 15, 2024.
</em></small></p>
<h2 id="authors">Authors</h2>
<ul>
<li><p><strong>Sebastian Lobentanzer</strong>
<sup><a href="#correspondence">✉</a></sup><br>
<img src="images/orcid.svg" class="inline_icon" width="16" height="16" alt="ORCID icon" />
<a href="https://orcid.org/0000-0003-3399-6695">0000-0003-3399-6695</a>
· <img src="images/github.svg" class="inline_icon" width="16" height="16" alt="GitHub icon" />
<a href="https://github.com/slobentanzer">slobentanzer</a>
· <img src="images/twitter.svg" class="inline_icon" width="16" height="16" alt="Twitter icon" />
<a href="https://twitter.com/slobentanzer">slobentanzer</a>
<br>
<small>
Heidelberg University, Faculty of Medicine and Heidelberg University Hospital, Institute for Computational Biomedicine, Heidelberg, Germany
</small></p></li>
<li><p><strong>Pablo Rodriguez-Mier</strong>
<br>
<img src="images/orcid.svg" class="inline_icon" width="16" height="16" alt="ORCID icon" />
<a href="https://orcid.org/0000-0002-4938-4418">0000-0002-4938-4418</a>
· <img src="images/github.svg" class="inline_icon" width="16" height="16" alt="GitHub icon" />
<a href="https://github.com/pablormier">pablormier</a>
<br>
<small>
Heidelberg University, Faculty of Medicine and Heidelberg University Hospital, Institute for Computational Biomedicine, Heidelberg, Germany
</small></p></li>
<li><p><strong>Stefan Bauer</strong>
<br>
<img src="images/orcid.svg" class="inline_icon" width="16" height="16" alt="ORCID icon" />
<a href="https://orcid.org/0000-0003-1712-060X">0000-0003-1712-060X</a>
· <img src="images/github.svg" class="inline_icon" width="16" height="16" alt="GitHub icon" />
<a href="https://github.com/Vis7i">Vis7i</a>
<br>
<small>
Helmholtz AI and TU Munich, Munich, Germany
</small></p></li>
<li><p><strong>Julio Saez-Rodriguez</strong>
<sup><a href="#correspondence">✉</a></sup><br>
<img src="images/orcid.svg" class="inline_icon" width="16" height="16" alt="ORCID icon" />
<a href="https://orcid.org/0000-0002-8552-8976">0000-0002-8552-8976</a>
· <img src="images/github.svg" class="inline_icon" width="16" height="16" alt="GitHub icon" />
<a href="https://github.com/saezrodriguez">saezrodriguez</a>
· <img src="images/twitter.svg" class="inline_icon" width="16" height="16" alt="Twitter icon" />
<a href="https://twitter.com/saezlab">saezlab</a>
<br>
<small>
Heidelberg University, Faculty of Medicine and Heidelberg University Hospital, Institute for Computational Biomedicine, Heidelberg, Germany
</small></p></li>
</ul>
<div id="correspondence">
<p>✉ — Correspondence possible via <a href="https://github.com/slobentanzer/causality_perspective_2023/issues">GitHub Issues</a>
or email to
Sebastian Lobentanzer &lt;sebastian.lobentanzer@gmail.com&gt;,
Julio Saez-Rodriguez &lt;pub.saez@uni-heidelberg.de&gt;.</p>
</div>
<h2 id="introduction">Introduction</h2>
<p>Correlation is not causation.
As simple as this widely agreed-upon statement may seem, scientifically defining causality and using it to drive our modern biomedical research is immensely challenging.
Since being described by Aristotle approximately 2500 years ago <span class="citation" data-cites="UFFRFvH4">[<a href="#ref-UFFRFvH4" role="doc-biblioref">1</a>]</span>, causal reasoning (CR) remained virtually unchanged until it experienced significant formal and mathematical advancements <span class="citation" data-cites="peGxtHPM w47lt0ah 712MuGug">[<a href="#ref-peGxtHPM" role="doc-biblioref">2</a>,<a href="#ref-w47lt0ah" role="doc-biblioref">3</a>,<a href="#ref-712MuGug" role="doc-biblioref">4</a>]</span> and a resurgence in the field of machine learning <span class="citation" data-cites="12vzXD9U2 4Dq1NQZ8">[<a href="#ref-12vzXD9U2" role="doc-biblioref">5</a>,<a href="#ref-4Dq1NQZ8" role="doc-biblioref">6</a>]</span> only in recent times.
In parallel, biomedicine has made major leaps in the past century, in particular in the development of high-throughput and large-scale methods.</p>
<p>In the field of systems biology, however, great hopes of causal insights from large-scale omics studies have largely been thwarted by the complexity of molecular mechanisms and the inability of existing methods to distinguish between correlation and causation <span class="citation" data-cites="IxBIB6CT 17kSbeSiE 17scpieH5">[<a href="#ref-IxBIB6CT" role="doc-biblioref">7</a>,<a href="#ref-17kSbeSiE" role="doc-biblioref">8</a>,<a href="#ref-17scpieH5" role="doc-biblioref">9</a>]</span>.</p>
<p>Randomised clinical trials (<a href="#randomized-clinical-trials">Glossary</a>) show that, in a lower-dimensional context, we can reliably identify causal effects.
By controlling “all” relevant covariates in a trial (via the principle of the gold-standard, randomised, double-blind, and placebo-controlled trial), we isolate the causal effect of the controlled variable, i.e., the treatment.
In the language of Pearl’s Do-Calculus <span class="citation" data-cites="12GOinshI">[<a href="#ref-12GOinshI" role="doc-biblioref">10</a>]</span> (<a href="#pearls-do-calculus">Glossary</a>), we measure the outcome of, for instance, do(“Treat with Vemurafenib”) when conducting a clinical trial on V600E-positive melanoma <span class="citation" data-cites="muFRX2ZL">[<a href="#ref-muFRX2ZL" role="doc-biblioref">11</a>]</span>.
However, translating this mode of reasoning into the high-dimensional space of modern omics poses enormous challenges.
The dramatically larger parameter space of models at the molecular level leads to problems in the performance of methods and the identifiability of results <span class="citation" data-cites="3e4s1VLp 15RkhPQiZ">[<a href="#ref-3e4s1VLp" role="doc-biblioref">12</a>,<a href="#ref-15RkhPQiZ" role="doc-biblioref">13</a>]</span>, as well as in model explainability <span class="citation" data-cites="18RVn064W">[<a href="#ref-18RVn064W" role="doc-biblioref">14</a>]</span>.
With this perspective, we discuss the current connections between CR and molecular systems biology in the context of these challenges.
We will elaborate on three main points:</p>
<ul>
<li><p>biases (<a href="#bias-machine-learning">Glossary</a>) and what they mean for CR, particularly in the context of biomedical data</p></li>
<li><p>the role of prior knowledge (PK, <a href="#prior-knowledge-1">Glossary</a>) in CR and how to translate PK into suitable biases</p></li>
<li><p>the role of foundation models in molecular systems biology and their relationship to CR</p></li>
</ul>
<h2 id="background">Background</h2>
<h3 id="causal-discovery-and-inference">Causal discovery and inference</h3>
<p>The field of CR distinguishes between causal discovery - the process of building causal hypotheses from data - and causal inference - the process of predicting specific outcomes given data and the causal relationships known about the system <em>a priori</em>.</p>
<p>Causal discovery is more expensive than inference both computationally and data-wise, because it needs to account for the variability in data generation while isolating generalisable relationships between single measured agents <span class="citation" data-cites="1CPlHia5R">[<a href="#ref-1CPlHia5R" role="doc-biblioref">15</a>]</span>.
For modern systems biology, this means that methods for causal discovery typically require large amounts of experiments. Highly parameterised models such as neural networks increase this requirement even further. As such, many regard causal discovery in molecular biomedicine as a scaling problem <span class="citation" data-cites="10mjWN2op 10AL1hWhU">[<a href="#ref-10mjWN2op" role="doc-biblioref">16</a>,<a href="#ref-10AL1hWhU" role="doc-biblioref">17</a>]</span> (<a href="#scaling-hypothesis">Glossary</a>).</p>
<p>Causal inference, in contrast, focuses on quantifying the causal effects of one variable on another within the framework of already hypothesised causal relationships.
This approach leverages PK about the assumed causal links, which in the causal field are often encoded using directed graphs.
Most inference mechanisms perform better when including PK at some point in the process, as has been observed in biomedical research <span class="citation" data-cites="qpg6x7P4">[<a href="#ref-qpg6x7P4" role="doc-biblioref">18</a>]</span>.
This allows researchers to represent both the causal connections between variables and their directionality, which is required to understand how changes in one variable might lead to changes in another.
For instance, in the case of the RAF-MEK-ERK signalling pathway, a graph would depict RAF activation leading to MEK activation, which in turn leads to ERK activation.
This clear representation of directionality is important for causal inference, as it ensures that analyses focus on the effect of upstream changes on downstream outcomes.
For example, in analysing phosphoproteomic data to assess the impact of inhibiting RAF, a graph-based approach would guide researchers to correctly attribute subsequent changes in ERK to this specific intervention.
Without this causal framework, one might mistakenly interpret correlations as bidirectional influences or overlook confounding factors, leading to incorrect conclusions.
However, inference is also very sensitive to the completeness of the PK that is applied; most biomedical PK is far from complete <span class="citation" data-cites="TUhGg6tD">[<a href="#ref-TUhGg6tD" role="doc-biblioref">19</a>]</span>.
For instance, the function of more than 95% of all the known phosphorylation events that occur in human cells is currently unknown <span class="citation" data-cites="4VxTzwTj yCFobrFQ">[<a href="#ref-4VxTzwTj" role="doc-biblioref">20</a>,<a href="#ref-yCFobrFQ" role="doc-biblioref">21</a>]</span>.
In contrast to causal discovery, scaling therefore plays a smaller role in causal inference; here, the main problems are incompleteness and identifying the “right” biases (<a href="#bias-machine-learning">Glossary</a>) to apply.</p>
<h3 id="the-ladder-of-causality">The ladder of causality</h3>
<p>Orthogonally to the distinction between causal discovery and inference, we can also distinguish between different levels of causality.
Pearl’s ladder of causality roughly distinguishes three types of CR in increasing order of power: observation, intervention, and counterfactuals <span class="citation" data-cites="185mDnD0M">[<a href="#ref-185mDnD0M" role="doc-biblioref">22</a>]</span>.
While the inferences we wish to make in biomedical research are often of the counterfactual type (e.g., “would RAF inhibition lead to decrease in ERK activation if the media contained Epidermal Growth Factors?”), the data we have available is typically observational (e.g., “the levels of RAF and MEK activity are correlated”) and sometimes interventional (e.g., “targeting RAF with CRISPR leads to a decrease in ERK activity”).
To generate interventional or even counterfactual inferences from observational data is a major challenge, if not impossible, depending on the characteristics of the system under study <span class="citation" data-cites="dJUlTjFg">[<a href="#ref-dJUlTjFg" role="doc-biblioref">23</a>]</span>.</p>
<p>There are approaches to delineate interventional inference from observational data, such as the ‘natural experiments’ framework <span class="citation" data-cites="w47lt0ah 712MuGug">[<a href="#ref-w47lt0ah" role="doc-biblioref">3</a>,<a href="#ref-712MuGug" role="doc-biblioref">4</a>]</span>.
However, these approaches are by their nature even more data-hungry than when using interventional data, as they often do not use the full breadth of the dataset <span class="citation" data-cites="1A5MoTSGF">[<a href="#ref-1A5MoTSGF" role="doc-biblioref">24</a>]</span>.
Therefore, in biomedical research, there has been a push towards generating large-scale interventional data, for instance through the use of CRISPR/Cas9 screens with single-cell resolution <span class="citation" data-cites="152yKY5w7">[<a href="#ref-152yKY5w7" role="doc-biblioref">25</a>]</span>.
Current developments of CR in the biomedical field thus mostly focus on these types of data.</p>
<h3 id="deduction-and-induction">Deduction and induction</h3>
<p>Lastly, in CR, we can also distinguish between deductive and inductive reasoning (<a href="#deductive-vs.-inductive-reasoning">Glossary</a>).
Deductive reasoning is the process of deriving a conclusion from a set of fixed and known premises.
“All men are mortal, Socrates is a man, therefore Socrates is mortal” is a classic example of deductive reasoning.
In biomedical research, this is typically the process of deriving a conclusion from a set of PK.
For instance, having PK of the linear activation cascade EGFR-&gt;RAS-&gt;RAF-&gt;MEK-&gt;ERK-&gt;Growth, and that Vemurafenib will inhibit RAF activity, allows us to deduce that giving Vemurafenib will inhibit growth of cancer cells <span class="citation" data-cites="muFRX2ZL">[<a href="#ref-muFRX2ZL" role="doc-biblioref">11</a>]</span>.</p>
<p>Inductive reasoning, on the other hand, involves making generalisations from specific observations.
Testing the hypothesis above, we apply Vemurafenib in a clinical trial of V600E-positive melanoma and find that it is clinically efficacious <span class="citation" data-cites="muFRX2ZL">[<a href="#ref-muFRX2ZL" role="doc-biblioref">11</a>]</span>.
Commonly, we then use induction to infer from this limited cohort that the treatment may be effective in the entire population.
We could further infer that Vemurafenib may be an effective remedy in other V600E-positive cancers as well, or that inhibiting this cascade may be a general mechanism of action of anti-cancer agents in cancers that display MAPK pathway overactivation <span class="citation" data-cites="1BSi2Dk6R">[<a href="#ref-1BSi2Dk6R" role="doc-biblioref">26</a>]</span>.
In the molecular realm, we could further infer that the inhibition of other components of the cascade, such as EGFR or MEK, may also be promising target leads <span class="citation" data-cites="ZkFzi3Xl">[<a href="#ref-ZkFzi3Xl" role="doc-biblioref">27</a>]</span>.</p>
<p>The main difference between deduction and induction is that the former is logically complete - i.e., if the premises are true and the argument is valid, the conclusion must also be true.
However, deduction is also more limited in scope than induction.
In biomedical research, we often have to rely on inductive reasoning because we cannot feasibly test all hypotheses in a deductive manner.
In consequence, the <em>inductive biases</em> we introduce into our models (i.e., those mechanisms in the model that help with inductive reasoning) are a pivotal part of performing CR in biomedical research.</p>
<h2 id="bias">Bias</h2>
<h3 id="meaning-and-examples-of-biases">Meaning and examples of biases</h3>
<p>Biases (<a href="#bias-machine-learning">Glossary</a>), generally, are systematic prejudices of a model towards certain outcomes.
Humans make frequent use of biases to function in a complex world with limited cognitive resources <span class="citation" data-cites="1DMQH5kzt">[<a href="#ref-1DMQH5kzt" role="doc-biblioref">28</a>]</span>.
In fact, we often presume causality from observation (i.e., we “jump to conclusions”), which is indicative of a strong inductive bias <span class="citation" data-cites="iWcDZtHu">[<a href="#ref-iWcDZtHu" role="doc-biblioref">29</a>]</span>.
A good <em>heuristic</em> is the application of a suitable bias to a problem, such that the solution can be considered acceptable despite limited resources.</p>
<p>In machine learning, we can distinguish between useful and harmful biases.
Harmful biases are common issues in the technical process of training models; they include, for instance, sampling bias, selection bias, and confirmation bias <span class="citation" data-cites="B5WSzZkm">[<a href="#ref-B5WSzZkm" role="doc-biblioref">30</a>]</span>.
While addressing harmful biases is a crucial part of machine learning, we will not discuss them further in this perspective.</p>
<p>Useful biases, on the other hand, are biases that are introduced into a model to improve its performance.
Since most models developed in biomedical research and the broader machine learning community are inductive models, one of the most discussed useful biases is <em>inductive bias</em> <span class="citation" data-cites="2RptKLT2">[<a href="#ref-2RptKLT2" role="doc-biblioref">31</a>]</span>.
For instance, PK on protein interactions can impact inference on activation cascades; only upstream proteins can activate downstream proteins, not vice versa.</p>
<h3 id="why-do-we-need-biases">Why do we need biases?</h3>
<p>Humans will be the gold standard for common-sense reasoning for the foreseeable future.
However, human reasoning is limited by our sensory and mnemonic capacity; we cannot reason about high-dimensional data since we can neither perceive it nor keep it in memory.
Machine learning seems like the ideal solution, but the “No Free Lunch” theorems (<a href="#no-free-lunch-theorems">Glossary</a>) present a fundamental challenge: no single learning algorithm may be universally superior across all problem domains <span class="citation" data-cites="nT3xJkyD">[<a href="#ref-nT3xJkyD" role="doc-biblioref">32</a>]</span>.
Although they have recently been challenged <span class="citation" data-cites="D3JIQ7Oe">[<a href="#ref-D3JIQ7Oe" role="doc-biblioref">33</a>]</span>, these theorems highlight the inherent difficulty in designing algorithms that generalise well from specific training data to new, unseen data.
Inductive biases guide algorithms in making educated guesses about unseen data, thereby improving their generalisation capabilities <span class="citation" data-cites="11wELIlTc">[<a href="#ref-11wELIlTc" role="doc-biblioref">34</a>]</span>.</p>
<p>This need for inductive biases is particularly apparent in the realm of biomedicine <span class="citation" data-cites="1AZn5l2ah">[<a href="#ref-1AZn5l2ah" role="doc-biblioref">35</a>]</span>.
Biomedical research operates within a framework constrained by limited and often high-dimensional data, stemming from the high costs of experiments, the scarcity of samples, and the inherent complexity of biological systems.
Coupled with the natural variability of biological measurements, these factors result in a low signal-to-noise ratio, making it challenging to discern meaningful patterns.
Inductive biases direct the learning process towards more relevant solutions by incorporating assumptions that enable more effective learning and interpretation, ensuring that models are not just statistically sound but also biologically meaningful.</p>
<p>Some central questions then arise:</p>
<ul>
<li><p>How explicit should we be in introducing biases, i.e., should the model determine its own biases, or do we force them on the model?</p></li>
<li><p>How do we choose the right biases to introduce?</p></li>
<li><p>How do we evaluate the biases we introduce?</p></li>
</ul>
<h2 id="bias-from-prior-knowledge">Bias from prior knowledge</h2>
<p>The first question alone is highly debated in the wider field of machine learning; it is related to the concept of the bias-variance tradeoff (<a href="#bias-variance-tradeoff">Glossary</a>).
The frequently quoted “Bitter Lesson” posits that we should refrain from inducing all but the most basic biases (<a href="#bias-machine-learning">Glossary</a>) in our models, and that we should not view metrics as the ultimate measure of performance, but rather whether the model gets us closer to some truth <span class="citation" data-cites="ydpntqD3">[<a href="#ref-ydpntqD3" role="doc-biblioref">36</a>]</span>.
However, it has been argued that many improvements that led to the models of today, such as convolution or attention (<a href="#attention-deep-learning">Glossary</a>), disprove this theory <span class="citation" data-cites="rh7nCPVE">[<a href="#ref-rh7nCPVE" role="doc-biblioref">37</a>]</span>, and that the intrinsic complexity of real-world systems does not obviate, but rather necessitate, the integration of human insight into our learning frameworks <span class="citation" data-cites="eT8vyMzx AT0GCO31">[<a href="#ref-eT8vyMzx" role="doc-biblioref">38</a>,<a href="#ref-AT0GCO31" role="doc-biblioref">39</a>]</span>.</p>
<p>In systems biology, specifically, there is much interest in finding models with suitable biases to deal with constraints specific to the field, such as data availability and the completeness of PK <span class="citation" data-cites="URCTSFCA T7D6XA6s vm2M7mI5 17scpieH5 11wELIlTc">[<a href="#ref-17scpieH5" role="doc-biblioref">9</a>,<a href="#ref-11wELIlTc" role="doc-biblioref">34</a>,<a href="#ref-URCTSFCA" role="doc-biblioref">40</a>,<a href="#ref-T7D6XA6s" role="doc-biblioref">41</a>,<a href="#ref-vm2M7mI5" role="doc-biblioref">42</a>]</span>.
Considering these constraints, the question is not whether to include PK in our reasoning, but which knowledge, when, and how <span class="citation" data-cites="AT0GCO31">[<a href="#ref-AT0GCO31" role="doc-biblioref">39</a>]</span>.</p>
<h3 id="prior-knowledge">Prior knowledge</h3>
<p>PK (<a href="#prior-knowledge-1">Glossary</a>) refers to information or data that is available to inform a learning process, enhancing the performance of the trained models and their ability to generalise.
It can be used to inform the inductive biases of a model, either explicitly through the design choices and assumptions embedded into the models, or implicitly through the data and methods used in training.
For this to be possible, biomedical entities and relationships must be clearly defined and represented unambiguously.
Additionally, the diversity in our tasks and knowledge sources requires a flexible representation.
Knowledge representation frameworks can aid in this process <span class="citation" data-cites="tr1XjZ1R">[<a href="#ref-tr1XjZ1R" role="doc-biblioref">43</a>]</span>.</p>
<p>In the biomedical field, there is a rich tradition of documenting biological knowledge at various levels of detail and focusing on different aspects of biology.
Detailed mechanistic models provide mathematical descriptions of the dynamic interactions at a molecular or cellular level.
Genome-scale networks, including metabolic and gene regulatory networks, offer comprehensive views of metabolic processes and gene interactions <span class="citation" data-cites="s2EFuVEM">[<a href="#ref-s2EFuVEM" role="doc-biblioref">44</a>]</span>.
Protein-protein interaction databases recapitulate either causal or non-causal interactions between proteins <span class="citation" data-cites="s2EFuVEM">[<a href="#ref-s2EFuVEM" role="doc-biblioref">44</a>]</span>.</p>
<h3 id="modelling-on-prior-knowledge">Modelling on prior knowledge</h3>
<p>The integration of PK into models is a non-trivial but essential process for moving from correlation to causation.
PK can be used to derive inductive biases either <em>explicitly</em> or <em>implicitly</em>.</p>
<p>The explicit case typically involves a mathematical framework where a set of assumptions is explicitly stated and integrated into the model.
Ordinary Differential Equation (ODE) models, logic-based models, rule-based models, and constraint-based models <span class="citation" data-cites="94lBUqNp">[<a href="#ref-94lBUqNp" role="doc-biblioref">45</a>]</span>, all of which are commonly used in systems biology, explicitly incorporate different types of PK, can be fitted to data, and then be used to answer different types of causal questions.
In the field of CR, Structural Causal Models (<a href="#structural-causal-models-scms">Glossary</a>) can be used when mechanisms are unknown <span class="citation" data-cites="4Dq1NQZ8">[<a href="#ref-4Dq1NQZ8" role="doc-biblioref">6</a>]</span>.
Their advantage is high efficiency in the face of scarce data, but they are highly reliant on the quality and comprehensiveness of the underlying PK <span class="citation" data-cites="xjVh8IQ5">[<a href="#ref-xjVh8IQ5" role="doc-biblioref">46</a>]</span>.</p>
<p>In contrast, implicit integration of PK in models involves learning useful representations directly from the data, without the explicit inclusion of biological assumptions or causal knowledge.
Learning mechanisms intruduced as implicit biases can be simple (e.g., sparsity) or elaborate.
Simple implicit biases include regularisation techniques that help models generalise by preventing overfitting <span class="citation" data-cites="kX2zf6UE">[<a href="#ref-kX2zf6UE" role="doc-biblioref">47</a>]</span> (<a href="#overfitting">Glossary</a>), or decisions about the types of prior distributions in bayesian models <span class="citation" data-cites="cKqFMtL2">[<a href="#ref-cKqFMtL2" role="doc-biblioref">48</a>]</span>.
More elaborate are neural networks which employ specific architectural designs, such as Convolutional Neural Networks (CNNs) <span class="citation" data-cites="bZ3hxYDX">[<a href="#ref-bZ3hxYDX" role="doc-biblioref">49</a>]</span>, Recurrent Neural Networks (RNNs) <span class="citation" data-cites="x4dbEYer">[<a href="#ref-x4dbEYer" role="doc-biblioref">50</a>]</span>, or Transformers <span class="citation" data-cites="rh7nCPVE">[<a href="#ref-rh7nCPVE" role="doc-biblioref">37</a>]</span>.
Their advantages and disadvantages are inverse to those of explicit models <span class="citation" data-cites="xjVh8IQ5">[<a href="#ref-xjVh8IQ5" role="doc-biblioref">46</a>]</span>.</p>
<p>As a result, choosing the best way to derive inductive biases from PK is not straightforward.
Models that explicitly incorporate PK are more interpretable and can generalise effectively even when data is scarce <span class="citation" data-cites="xjVh8IQ5">[<a href="#ref-xjVh8IQ5" role="doc-biblioref">46</a>]</span>.
However, they are constrained by the accuracy of the existing knowledge and often struggle to scale to larger datasets <span class="citation" data-cites="6W1y3ZrT OlEfQKqu">[<a href="#ref-6W1y3ZrT" role="doc-biblioref">51</a>,<a href="#ref-OlEfQKqu" role="doc-biblioref">52</a>]</span>.
Models with implicit biases, on the other hand, particularly those typically found in deep learning architectures, excel at learning from large, high-dimensional datasets and offer flexibility across diverse domains.
Yet, they suffer from limited interpretability, are prone to overfitting, and typically do not generalise well to scenarios not encountered during training, such as predicting the effects of new drugs or drug combinations, largely due to their lack of causal knowledge.</p>
<p>Hybrid models make a tradeoff between those extremes, which is why they have been found to be useful in systems biology, where data are currently scarce <span class="citation" data-cites="bMaT0Vyc KZ19R8ZY aLhTIs9 4smautjA YOm0bqVR JkqcQgM7 1Xej0UJj">[<a href="#ref-bMaT0Vyc" role="doc-biblioref">53</a>,<a href="#ref-KZ19R8ZY" role="doc-biblioref">54</a>,<a href="#ref-aLhTIs9" role="doc-biblioref">55</a>,<a href="#ref-4smautjA" role="doc-biblioref">56</a>,<a href="#ref-YOm0bqVR" role="doc-biblioref">57</a>,<a href="#ref-JkqcQgM7" role="doc-biblioref">58</a>,<a href="#ref-1Xej0UJj" role="doc-biblioref">59</a>]</span>.
While their implementation details differ, they often employ two learners side-by-side, one of which is driven by explicit biases from PK, while the other learns from data.
Frequently, these learners are also coupled in an end-to-end learning process, i.e., they “learn together.” This mode of learning aims to benefit from the “bias-free” nature of neural networks while simultaneously improving model performance in the face of scarce data via the added explicit bias.</p>
<h2 id="causality-in-foundation-models">Causality in foundation models</h2>
<p>There has been an enormous spike of interest in attention-based neural network models, in large part due to the success of Large Language Models (LLMs, <a href="#large-language-models">Glossary</a>).
While the high performance of LLMs is based on myriad technical improvements, the introduction of attention (<a href="#attention-deep-learning">Glossary</a>) as an architectural bias (<a href="#bias-machine-learning">Glossary</a>) has been a major contributor to their success <span class="citation" data-cites="rh7nCPVE">[<a href="#ref-rh7nCPVE" role="doc-biblioref">37</a>]</span>.
This has led to the development of attention-based molecular models (most commonly for gene expression), which can also be considered “GPT” models: Generative Pre-trained Transformers <span class="citation" data-cites="gAQyFCbW VmzWBJUJ r5y0HbhJ">[<a href="#ref-gAQyFCbW" role="doc-biblioref">60</a>,<a href="#ref-VmzWBJUJ" role="doc-biblioref">61</a>,<a href="#ref-r5y0HbhJ" role="doc-biblioref">62</a>]</span>.
Attention as a learning mechanism enables the integration of non-local information in a flexible manner.
In a molecular model that reasons about gene expression, such as Geneformer, attention allows the integration of distant regulatory elements <span class="citation" data-cites="VmzWBJUJ">[<a href="#ref-VmzWBJUJ" role="doc-biblioref">61</a>]</span>.
Notably, this mechanism comes with a computational cost that increases exponentially with respect to the length of the input sequence <span class="citation" data-cites="1DSO3BUly">[<a href="#ref-1DSO3BUly" role="doc-biblioref">63</a>]</span>.</p>
<p>The generalist capabilities of LLMs have led to the designation of “foundation models” <span class="citation" data-cites="U6LC2Ufe">[<a href="#ref-U6LC2Ufe" role="doc-biblioref">64</a>]</span>.
Foundation models (<a href="#foundation-model">Glossary</a>) are models that achieve high performance by training a generic architecture on extremely large amounts of data in an unsupervised manner.
They can be fine-tuned for more specific tasks, because they are thought to derive generalisable representations and mechanisms by training on an amount of data large enough to learn the complexity of real-world systems.
However, recent molecular foundation model benchmarks highlight clear discrepancies between the “foundational” aspirations of the pre-trained models and the real-world evaluation of their performance <span class="citation" data-cites="WEYqVcYG OFczH7ba">[<a href="#ref-WEYqVcYG" role="doc-biblioref">65</a>,<a href="#ref-OFczH7ba" role="doc-biblioref">66</a>]</span>.
Briefly, the benchmarks found that, on single cell classification tasks, the proposed foundation models did not outperform simple baselines consistently.
State-of-the-art methods such as scVI <span class="citation" data-cites="1ELFXHA51">[<a href="#ref-1ELFXHA51" role="doc-biblioref">67</a>]</span> and even the mere selection of highly variable genes was often statistically indistinguishable from the highly parameterised methods, and sometimes even yielded better classification outcomes.
However, these are early models, and it could still be argued that, in line with the scaling hypothesis (<a href="#scaling-hypothesis">Glossary</a>), models may improve via a combination of the right architecture with sufficient amounts of data <span class="citation" data-cites="1GGrqZlzU">[<a href="#ref-1GGrqZlzU" role="doc-biblioref">68</a>]</span>.</p>
<p>Indeed, molecular foundation models lag behind in size: while current-generation LLMs have around 100 billion parameters or more and are trained on enormous text corpuses (hundreds of billions to trillions of tokens), molecular foundation models have tens of millions of parameters (scGPT: 53M, Geneformer: 10M) and are trained on corpuses of tens of millions of cells, which (optimistically) yields hundreds of billions of individual data points.
Thus, LLMs are currently about 2000 times larger than molecular foundation models, while arguably also dealing with a less complicated system.
The question whether scaling will lead to the emergence of “foundational behaviour” in molecular models is still a matter of much debate.</p>
<h3 id="attention---and-large-amounts-of-data---is-all-you-need">Attention - and large amounts of data - is all you need?</h3>
<p>Given enough data to train on - and ample funds for compute - is attention “all you need” to induce reliable biases in your model?
While there are doubts regarding the reasoning capabilities of LLMs, GPT arguably “understands” language very well already, to the point where it can flawlessly communicate and synthesise information <span class="citation" data-cites="19EQh1DNG">[<a href="#ref-19EQh1DNG" role="doc-biblioref">69</a>]</span>.
This is what the term “foundation model” implies: the model has derived a generalisable representation of language, a tool that can be fine-tuned for a variety of language-related tasks.
This behaviour is not possible without assuming some form of causality, even if it is not explicitly encoded in the model <span class="citation" data-cites="10mjWN2op">[<a href="#ref-10mjWN2op" role="doc-biblioref">16</a>]</span>.</p>
<p>In this light, what are the reasons to be sceptical about the capacity of molecular foundation models to understand the “grammar” of the cell?</p>
<p><strong>Explainability</strong>: For one, large transformer models (i.e., billions of parameters) are not explainable due to their high complexity.
As such, there is often no way to scrutinise their reasoning beyond the output they produce <span class="citation" data-cites="1GbAsSOZV 15hYXC3QB">[<a href="#ref-1GbAsSOZV" role="doc-biblioref">70</a>,<a href="#ref-15hYXC3QB" role="doc-biblioref">71</a>]</span>.
What seems simple in the case of language models - the famous Turing test can be performed by any human with a basic understanding of language - is exceedingly difficult in the molecular space, where many causal relationships are still unknown <span class="citation" data-cites="19EQh1DNG">[<a href="#ref-19EQh1DNG" role="doc-biblioref">69</a>]</span>.
Yet the only way to scrutinise and subsequently improve the reasoning capabilities of a model is precisely this explicit validation of its predictions in an interpretable setting.</p>
<p>While the creation of explicit molecular models (e.g., logic, structural causal, or ODE-based models) and the self-supervised training of molecular foundation models are methodically very different, both can provide a hypothesis on causal structure that can be formulated as a network.
Theodoris et al. explore the attention layers of their Geneformer foundation model to explain the model’s reasoning <span class="citation" data-cites="VmzWBJUJ">[<a href="#ref-VmzWBJUJ" role="doc-biblioref">61</a>]</span>.
While some layers show clear patterns of attention, such as attending to highly connected or highly expressed genes, other layers are not as readily interpretable, much less so than explicit molecular models.</p>
<p><strong>Benchmarking</strong>: Whether these complex layers reflect the true complexity of the underlying biology or are rather evidence for overfitting (<a href="#overfitting">Glossary</a>) to the training data is not clear.
One argument in favour of overfitting is the poor generalisation of the model in independent benchmarks <span class="citation" data-cites="WEYqVcYG OFczH7ba">[<a href="#ref-WEYqVcYG" role="doc-biblioref">65</a>,<a href="#ref-OFczH7ba" role="doc-biblioref">66</a>]</span>.
To determine whether molecular foundation models indeed capture generalisable causal representations of biology, dedicated benchmarks are needed.</p>
<p><strong>Causal bias</strong>: The GPT-3 architecture that led to the recent breakthrough in LLM capabilities employs “causal self-attention,” describing an implicit architectural bias that prevents the model from “looking into the future”: for predicting the next token, only the previous tokens in the sentence can be used <span class="citation" data-cites="1DSO3BUly">[<a href="#ref-1DSO3BUly" role="doc-biblioref">63</a>]</span>.
This leverages the implicit causality present in language, which incidentally is similar to one of the earliest formal descriptions of causality, that “the effect has regularly followed the cause in the past” <span class="citation" data-cites="WmT8ZU5I">[<a href="#ref-WmT8ZU5I" role="doc-biblioref">72</a>]</span>.
Compared to language, the data that form the input of molecular foundation models do not implicitly contain causal information.
The individual cells are in general not on a known trajectory, and the genes that are masked as part of the training objective are masked at random, not because they are downstream (in some form) of the genes used for prediction.
This fundamental difference between language and molecular models has so far not been explored theoretically or empirically.</p>
<h3 id="causal-latent-spaces">Causal latent spaces</h3>
<p>Due to the fundamental limitation of human perception, dimensionality reduction is a popular workflow for data interpretation, typically via methods such as PCA, t-SNE, or UMAP <span class="citation" data-cites="13qoNo4Fj">[<a href="#ref-13qoNo4Fj" role="doc-biblioref">73</a>]</span>.
The hope is that exploration and explanation in the lower-dimensional embedding space may be less challenging than in the original data, which assumes that the most important aspects of variability in the original data are captured in the reduced dimensions <span class="citation" data-cites="AO84A4MA">[<a href="#ref-AO84A4MA" role="doc-biblioref">74</a>]</span>.
However, without explicit supervision, which is rare in typical biomedical datasets, the resulting latent spaces are rarely interpretable, and do not lend themselves to causal interpretation.
In addition, they often suffer from biases that result from technical rather than biological factors <span class="citation" data-cites="6McXkHVo">[<a href="#ref-6McXkHVo" role="doc-biblioref">75</a>]</span>.
In consequence, biological insight during the exploration of these latent spaces is often challenging due to the dominance of biases over the biological generative mechanism.</p>
<p>Performing causal inference in latent spaces could potentially solve some of these issues.
“Moving through the latent space” reduces the number of variables that change upon intervention, making exploration simpler in theory.
In practice, however, ease and sensibility of exploration depend completely on whether the inductive biases in the embedding process capture the underlying biology.
In addition, latent spaces have no trivial connection to the real-world measurements they are based on.
Each model instance generates its own, independent latent space; in consequence, the exploration of latent spaces is challenging and time-consuming.</p>
<p>Even if a given latent space can be explored, there is often no guarantee that interpolation between sensible latent representations also leads to sensible results.
As an example, consider a prevailing issue of visual generative models in drawing human hands: images of hands typically involve mangled anatomy and an incorrect number of digits <span class="citation" data-cites="iE5sGWcB">[<a href="#ref-iE5sGWcB" role="doc-biblioref">76</a>]</span>.
Even though there is a section in the latent space that represents hands, this does not represent the concept of a hand, but rather is guided by learning on many diverse pictures of hands.
A section of this latent space may represent only a finger, and carry some information that next to a finger there usually is another finger.
However, when generating the image, there is no mechanism to keep track of how many digits to add to any generated hand, leading to wrong anatomy.
Similarly, when exploring the latent space of a model of molecular signalling, there may be no guarantees that the model respects the concept of a given pathway when generating the signalling molecules involved.</p>
<p>If mastered, exploring and performing interventions in latent spaces promises many benefits: better generalisation and improved sample efficiency <span class="citation" data-cites="T7D6XA6s">[<a href="#ref-T7D6XA6s" role="doc-biblioref">41</a>]</span>, predicting the outcomes of interventions not observed at training time <span class="citation" data-cites="MhOZ3PWC">[<a href="#ref-MhOZ3PWC" role="doc-biblioref">77</a>]</span>, or insights into the effect of different inductive biases in the model <span class="citation" data-cites="FVLAWGsX">[<a href="#ref-FVLAWGsX" role="doc-biblioref">78</a>]</span>.
However, to achieve this, gaining a better understanding of properties of the learned embeddings and variables is essential, for instance by performing “imagined interventions” in the latent space <span class="citation" data-cites="r5mjeIhV">[<a href="#ref-r5mjeIhV" role="doc-biblioref">79</a>]</span> or by using model uncertainty for guiding the optimisation process in the latent space <span class="citation" data-cites="eEfUqiI4">[<a href="#ref-eEfUqiI4" role="doc-biblioref">80</a>]</span>.
Of note, many of the proposed solutions for more explainable latent spaces depend on architectures that may scale significantly worse than transformers <span class="citation" data-cites="6W1y3ZrT OlEfQKqu">[<a href="#ref-6W1y3ZrT" role="doc-biblioref">51</a>,<a href="#ref-OlEfQKqu" role="doc-biblioref">52</a>]</span>.</p>
<h2 id="discussion-conclusion">Discussion / Conclusion</h2>
<h3 id="dichotomy-of-scaling-data-driven-and-bias-injection-knowledge-driven">Dichotomy of scaling (data-driven) and bias injection (knowledge-driven)</h3>
<p>The debate between adopting scaling strategies (<a href="#scaling-hypothesis">Glossary</a>) versus the injection of biases (<a href="#bias-machine-learning">Glossary</a>) from PK (<a href="#prior-knowledge-1">Glossary</a>) highlights a fundamental tension in modern biomedical research.
The “Bitter Lesson” suggests a preference for general-purpose learning algorithms that scale with computational resources, implicitly learning biases from data.
However, complex models often pose significant computational challenges; many models are limited to network sizes unfeasibly small for biological inference, and feedback loops are often excluded.
Conversely, explicitly injecting biases from PK can lead to more specialised and efficient models that can generalise using relatively little training data, but may not scale.
Hybrid models represent a promising middle ground, combining the scalability of generalist models with the efficiency and specificity provided by tailored biases.
Researchers often rely on intuition to determine which biases to inject, understanding that while no single model may universally excel (reflecting the “No Free Lunch” theorems, <a href="#no-free-lunch-theorems">Glossary</a>), the blend of generalisation through scaling and specialisation through bias injection might provide a robust framework for tackling complex biomedical challenges.</p>
<h3 id="theoretical-foundations-interventions-and-inductive-biases">Theoretical foundations: interventions and inductive biases</h3>
<p>Theoretical work emphasises the need for interventions in causal discovery but does not yet address the influence of inductive biases <span class="citation" data-cites="S1VP202R">[<a href="#ref-S1VP202R" role="doc-biblioref">81</a>]</span>.
The number of required interventions might be reduced significantly when complemented with high-quality observational data and appropriate biases, as suggested by neural causal models <span class="citation" data-cites="7HxYpmt4">[<a href="#ref-7HxYpmt4" role="doc-biblioref">82</a>]</span>.
However, the precise nature of these biases and their impact remains understudied theoretically as well as empirically.
The comparative effectiveness and theoretical underpinnings of explicit models versus implicit models are particularly understudied.
Foundation models (<a href="#foundation-model">Glossary</a>) have embraced causal self-attention as a step towards integrating causality, but this alone may be insufficient.
Empirical studies and more robust theories are needed to understand these dynamics, including the potential existence and validation of causal latent spaces.</p>
<h3 id="data-types-observational-vs.-interventional">Data types: observational vs. interventional</h3>
<p>The choice between training on observational versus interventional data (or a mixture of both) is critical in the development of models.
While large-scale data collection is vital, the type of data collected can significantly influence model performance and the ability to generalise and make accurate causal inferences.
The complexity and high cost of collecting data requires an efficient experimental design to maximise causal discovery with limited resources.
Observational data are more readily available but may lead to confounded or biassed insights.
Interventional data, while more challenging to obtain, provide clearer causal pathways and can greatly enhance the model’s understanding of underlying biological processes <span class="citation" data-cites="3MP7gokd zXrfFfft">[<a href="#ref-3MP7gokd" role="doc-biblioref">83</a>,<a href="#ref-zXrfFfft" role="doc-biblioref">84</a>]</span>.
A balanced approach, possibly incorporating both observational and interventional data, coupled with mechanisms for deciding the right number and type of interventions, might provide a more nuanced understanding and improve model robustness and interpretability.</p>
<h3 id="foundation-models-architectural-biases-and-the-no-free-lunch-theorems">Foundation models: architectural biases and the No Free Lunch theorems</h3>
<p>Foundation models challenge the “No Free Lunch” theorems by suggesting that certain architectural biases, learned from vast amounts of data, can yield generalisable and high-performing models <span class="citation" data-cites="D3JIQ7Oe">[<a href="#ref-D3JIQ7Oe" role="doc-biblioref">33</a>]</span>.
These biases, and how to transfer them from LLMs (<a href="#large-language-models">Glossary</a>) to systems biology, necessitate careful evaluation.
As the biomedical field looks to these models for answers, it becomes crucial to develop frameworks that facilitate rapid development and exploration of ideas <span class="citation" data-cites="tr1XjZ1R Ex1JrMxh">[<a href="#ref-tr1XjZ1R" role="doc-biblioref">43</a>,<a href="#ref-Ex1JrMxh" role="doc-biblioref">85</a>]</span>.
A crucial aspect of these frameworks will be establishing benchmarks in the face of missing biological ground truth.</p>
<h3 id="systems-biology-and-causality---finding-a-balance">Systems biology and causality - finding a balance</h3>
<p>Systems biology has historically followed both knowledge-driven (bottom-up) and data-driven (top-down) approaches.
Bottom-up systems biology, aiming to understand specific molecular mechanisms driving biological phenomena, has <em>de facto</em> been doing CR, despite both fields being largely disconnected.
Meanwhile, top-down systems biology, inspired more by machine learning principles, has struggled with moving from correlation to causality.
New methods and models offer the potential to converge these complementary approaches and scale our understanding to larger, more complex systems.
However, it remains to be seen whether the future of biological modelling will be dominated by the generation of vast datasets for generalist models or by more nuanced, bias-inclusive architectures.</p>
<p>While the allure of generalist models trained on extensive datasets is strong, the unique challenges of biomedical research may necessitate a more tailored approach.
Including explicit favourable biases, informed by deep domain knowledge and specific data types (observational or interventional), could lead to breakthroughs in understanding complex biological systems.
The field must explore these possibilities, balancing the drive for large-scale data with the need for precision and specificity, to realise the full potential of modern systems biology.</p>
<h2 id="glossary">Glossary</h2>
<h3 id="attention-deep-learning">Attention (deep learning)</h3>
<p>A mechanism in deep learning that allows the model to focus on specific parts of the input data.
Attention mechanisms are often used in natural language processing to focus on specific words in a sentence, but can also be used in other domains.</p>
<h3 id="bias-machine-learning">Bias (machine learning)</h3>
<p>Bias can be understood in two ways in the context of machine learning.</p>
<ol type="1">
<li><p>The first definition, and the one predominantly used in this Perspective, is also referred to as statistical bias; a technical term referring to the assumptions made by a model to make predictions.
This bias is a necessary part of any machine learning model.
A model with high bias (low variance) pays very little attention to the training data and oversimplifies the model, which can lead to <a href="#underfitting">underfitting</a>.
This means it does not capture the complexity of the data and fails to learn the underlying patterns effectively.
Conversely, a model with low bias (high variance) makes complex assumptions to fit the data closely, which can lead to <a href="#overfitting">overfitting</a>, where the model captures noise in the data as if it were a true pattern.
See also the <a href="#bias-variance-tradeoff">bias-variance tradeoff</a>.</p></li>
<li><p>The second definition is also known as algorithmic bias, and refers to the systematic and repeatable errors in a model due to faulty assumptions or data.
It often reflect existing biases in the real world that the training data are derived from, but can also result from architectural choices in the model.
As such, algorithmic bias can result from any stage in model training, from data collection to model deployment.</p></li>
</ol>
<h3 id="bias-variance-tradeoff">Bias-variance tradeoff</h3>
<p>The concept in machine learning that <a href="#bias-machine-learning">bias</a> and <a href="#variance-machine-learning">variance</a> of a model are inversely related.
The term implies that an optimal model finds a balance between bias (impact of the model on predictions) and variance (impact of the data on predictions).
This balance depends on the complexity of model and data.</p>
<h3 id="deductive-vs.-inductive-reasoning">Deductive vs. Inductive Reasoning</h3>
<p>Deductive reasoning involves drawing specific conclusions from general statements or premises, whereas inductive reasoning involves making broad generalizations from specific observations.
Deductive reasoning is often seen as more logically sound but less informative about the real world, while inductive reasoning is more exploratory but can lead to less certain conclusions.</p>
<h3 id="do-calculus">Do-Calculus</h3>
<p>See Pearl’s Do-Calculus.</p>
<h3 id="foundation-model">Foundation model</h3>
<p>A model that is trained on a large amount of data and can be used as a starting point for further model development (also referred to as fine-tuning).
Foundation models are assumed to have learned generalisable patterns from their input data.
To achieve this, they require large amounts of data and compute power.</p>
<h3 id="large-language-models">Large Language Models</h3>
<p>Large Language Models are advanced AI models trained on extensive text data.
They are capable of understanding and generating human-like text, making them useful in various applications like translation, summarization, and conversation.
LLMs leverage vast amounts of training data to grasp nuances of language, context, and even some elements of human communication.
They are the first commercially successful example of foundation models.</p>
<h3 id="no-free-lunch-theorems">‘No Free Lunch’ Theorems</h3>
<p>These theorems in optimization and machine learning suggest that no single algorithm is best for every problem.
The performance of an algorithm is contingent on the specificities of the task and data at hand.
This highlights the importance of choosing or designing algorithms that are well-suited to the particular characteristics of the problem being addressed.
Related to the bias-variance tradeoff, partly opposed to the scaling hypothesis and foundation models.</p>
<h3 id="overfitting">Overfitting</h3>
<p>A technical term referring to a model that captures noise in the data as if it were a true pattern.
Overfitting tends to lead to high performance on the training data but poor performance on the test data.
If a model has overfitted also to the test data, it will also perform poorly on new data, i.e., it will not generalise well.
This is a common occurrence if there has been data leakage between training, validation, and test data.</p>
<h3 id="pearls-do-calculus">Pearl’s Do-Calculus</h3>
<p>Developed by Judea Pearl, Do-Calculus is a formal mathematical framework used in causal inference.
It provides a set of rules for calculating the effects of interventions in probabilistic models, allowing researchers to infer causality from observational data.</p>
<h3 id="prior-knowledge-1">Prior knowledge</h3>
<p>A term referring to information that is available to inform a learning process.
Often, this is the result of previous research.
The tasks of collecting and integrating prior knowledge are often referred to as knowledge engineering and form a crucial part of model development in systems biology.</p>
<h3 id="randomized-clinical-trials">Randomized Clinical Trials</h3>
<p>Randomized clinical trials are experiments designed to test the efficacy of medical interventions.
Participants are randomly assigned to groups receiving different treatments, including a control group, which often receives a placebo or gold-standard treatment.
To further minimise confounding factors, participants and administering doctors are often blinded to the treatment given.
This method is considered the gold standard in clinical research for its ability to minimize bias and establish causality between a treatment and its outcomes.</p>
<h3 id="scaling-hypothesis">Scaling hypothesis</h3>
<p>The scaling hypothesis posits that the performance of a model increases with the amount of data it is trained on.
Recently, it has come to describe the idea that, given enough data, complex model behaviours can emerge.
The enormous success of current Large Language Models has been attributed to scaling, with emergence of human-like language capabilities around the time of GPT-3.
The ability to scale depends on several factors: the availability of data, parallelisation of training, adequate compute power with a parallel architecture, and a model architecture that can digest large amounts of data effectively.</p>
<h3 id="structural-causal-models-scms">Structural Causal Models (SCMs)</h3>
<p>SCMs are a type of statistical model used to represent and analyze causal relationships.
They consist of variables and equations that describe how these variables interact causally.
SCMs are particularly useful in causal inference as they allow for the analysis of how changes in one variable may cause changes in another.</p>
<h3 id="underfitting">Underfitting</h3>
<p>A technical term referring to a model that does not capture the complexity of the data.
Underfitting tends to lead to poor performance on both the training and test data.</p>
<h3 id="variance-machine-learning">Variance (machine learning)</h3>
<p>A technical term referring to the sensitivity of a model to the training data.
Describes how much the predictions of a model vary given different training data.
High variance (low bias) in a model can lead to <a href="#overfitting">overfitting</a> and thus harm generalisation.
Conversely, low variance (high bias) can lead to <a href="#underfitting">underfitting</a> and thus to a model that does not capture the complexity of the data.
See also the <a href="#bias-variance-tradeoff">bias-variance tradeoff</a>.</p>
<h2 class="page_break_before" id="acknowledgements">Acknowledgements</h2>
<p>We thank Aurelien Dugourd, Philipp Schäfer, Loan Vulliard, and Jan Lanzer for their helpful comments on the manuscript.</p>
<h2 id="funding">Funding</h2>
<p>This work was supported by the European Union’s Horizon 2020 Programme under PerMedCoE (951773) and DECIDER (965193).</p>
<h2 id="conflict-of-interest">Conflict of Interest</h2>
<p>JSR reports funding from GSK, Pfizer and Sanofi and fees/honoraria from Travere Therapeutics, Stadapharm, Astex, Pfizer and Grunenthal.</p>
<h2 class="page_break_before" id="references">References</h2>
<!-- Explicitly insert bibliography here -->
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-UFFRFvH4" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline"><strong>ORGANON OR LOGICAL TREATISES O</strong> <div class="csl-block">Aristotle, Octavius Freire 1816?-1873 Owen</div> <em>Wentworth Press</em> (2016) <div class="csl-block">ISBN: 9781372537233</div></div>
</div>
<div id="ref-peGxtHPM" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline"><strong>Causality</strong> <div class="csl-block">Judea Pearl</div> <em>Cambridge University Press</em> (2009-09-14) <a href="https://doi.org/ggd72q">https://doi.org/ggd72q</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1017/cbo9780511803161">10.1017/cbo9780511803161</a></div></div>
</div>
<div id="ref-w47lt0ah" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline"><strong>Identification of Causal Effects Using Instrumental Variables</strong> <div class="csl-block">Joshua D Angrist, Guido W Imbens, Donald B Rubin</div> <em>Journal of the American Statistical Association</em> (1996-06) <a href="https://doi.org/gdz4f4">https://doi.org/gdz4f4</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1080/01621459.1996.10476902">10.1080/01621459.1996.10476902</a></div></div>
</div>
<div id="ref-712MuGug" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline"><strong>Myth and measurement: the new economics of the minimum wage</strong> <div class="csl-block">David E Card, Alan B Krueger</div> <em>Princeton University Press</em> (2016) <div class="csl-block">ISBN: 9781400880874</div></div>
</div>
<div id="ref-12vzXD9U2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline"><strong>Causal Machine Learning: A Survey and Open Problems</strong> <div class="csl-block">Jean Kaddour, Aengus Lynch, Qi Liu, Matt J Kusner, Ricardo Silva</div> (2022) <a href="https://arxiv.org/abs/2206.15475">https://arxiv.org/abs/2206.15475</a></div>
</div>
<div id="ref-4Dq1NQZ8" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline"><strong>Causal machine learning for single-cell genomics</strong> <div class="csl-block">Alejandro Tejada-Lapuerta, Paul Bertin, Stefan Bauer, Hananeh Aliee, Yoshua Bengio, Fabian J Theis</div> <em>arXiv</em> (2023) <a href="https://doi.org/gtb97p">https://doi.org/gtb97p</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2310.14935">10.48550/arxiv.2310.14935</a></div></div>
</div>
<div id="ref-IxBIB6CT" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline"><strong>A map of human genome variation from population-scale sequencing</strong> <em>Nature</em> (2010-10-27) <a href="https://doi.org/fmk7rw">https://doi.org/fmk7rw</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/nature09534">10.1038/nature09534</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/20981092">20981092</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3042601">PMC3042601</a></div></div>
</div>
<div id="ref-17kSbeSiE" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline"><strong>Causality in digital medicine</strong> <div class="csl-block">Nature Communications</div> (2021-09-15) <a href="https://doi.org/gtcfvh">https://doi.org/gtcfvh</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41467-021-25743-9">10.1038/s41467-021-25743-9</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/34526509">34526509</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8443583">PMC8443583</a></div></div>
</div>
<div id="ref-17scpieH5" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline"><strong>The perpetual motion machine of AI-generated data and the distraction of ChatGPT-as-scientist</strong> <div class="csl-block">Jennifer Listgarten</div> <em>arXiv</em> (2023) <a href="https://doi.org/gs8pnp">https://doi.org/gs8pnp</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2312.00818">10.48550/arxiv.2312.00818</a></div></div>
</div>
<div id="ref-12GOinshI" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline"><strong>The Do-Calculus Revisited</strong> <div class="csl-block">Judea Pearl</div> <em>arXiv</em> (2012) <a href="https://doi.org/gtbf4r">https://doi.org/gtbf4r</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1210.4852">10.48550/arxiv.1210.4852</a></div></div>
</div>
<div id="ref-muFRX2ZL" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline"><strong>Improved Survival with Vemurafenib in Melanoma with BRAF V600E Mutation</strong> <div class="csl-block">Paul B Chapman, Axel Hauschild, Caroline Robert, John B Haanen, Paolo Ascierto, James Larkin, Reinhard Dummer, Claus Garbe, Alessandro Testori, Michele Maio, … Grant A McArthur</div> <em>New England Journal of Medicine</em> (2011-06-30) <a href="https://doi.org/dsbxxt">https://doi.org/dsbxxt</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1056/nejmoa1103782">10.1056/nejmoa1103782</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/21639808">21639808</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3549296">PMC3549296</a></div></div>
</div>
<div id="ref-3e4s1VLp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">12. </div><div class="csl-right-inline"><strong>Reliable interpretability of biology-inspired deep neural networks</strong> <div class="csl-block">Wolfgang Esser-Skala, Nikolaus Fortelny</div> <em>npj Systems Biology and Applications</em> (2023-10-10) <a href="https://doi.org/gtb95c">https://doi.org/gtb95c</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41540-023-00310-8">10.1038/s41540-023-00310-8</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/37816807">37816807</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10564878">PMC10564878</a></div></div>
</div>
<div id="ref-15RkhPQiZ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">13. </div><div class="csl-right-inline"><strong>Structural Identifiability of Systems Biology Models: A Critical Comparison of Methods</strong> <div class="csl-block">Oana-Teodora Chis, Julio R Banga, Eva Balsa-Canto</div> <em>PLoS ONE</em> (2011-11-22) <a href="https://doi.org/fch6rc">https://doi.org/fch6rc</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1371/journal.pone.0027755">10.1371/journal.pone.0027755</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22132135">22132135</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3222653">PMC3222653</a></div></div>
</div>
<div id="ref-18RVn064W" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">14. </div><div class="csl-right-inline"><strong>The role of causality in explainable artificial intelligence</strong> <div class="csl-block">Gianluca Carloni, Andrea Berti, Sara Colantonio</div> <em>arXiv</em> (2023) <a href="https://doi.org/gtb95k">https://doi.org/gtb95k</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2309.09901">10.48550/arxiv.2309.09901</a></div></div>
</div>
<div id="ref-1CPlHia5R" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">15. </div><div class="csl-right-inline"><strong>Causal Structure Learning</strong> <div class="csl-block">Christina Heinze-Deml, Marloes H Maathuis, Nicolai Meinshausen</div> <em>Annual Review of Statistics and Its Application</em> (2018-03-07) <a href="https://doi.org/ggh4pj">https://doi.org/ggh4pj</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1146/annurev-statistics-031017-100630">10.1146/annurev-statistics-031017-100630</a></div></div>
</div>
<div id="ref-10mjWN2op" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">16. </div><div class="csl-right-inline"><strong>Can Foundation Models Talk Causality?</strong> <div class="csl-block">Moritz Willig, Matej Zečević, Devendra Singh Dhami, Kristian Kersting</div> <em>arXiv</em> (2022) <a href="https://doi.org/gtb9wb">https://doi.org/gtb9wb</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2206.10591">10.48550/arxiv.2206.10591</a></div></div>
</div>
<div id="ref-10AL1hWhU" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">17. </div><div class="csl-right-inline"><strong>The Scaling Hypothesis</strong> <div class="csl-block">Gwern Branwen</div> (2020-05-28) <a href="https://gwern.net/scaling-hypothesis">https://gwern.net/scaling-hypothesis</a></div>
</div>
<div id="ref-qpg6x7P4" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">18. </div><div class="csl-right-inline"><strong>Inferring causal molecular networks: empirical assessment through a community-based effort</strong> <div class="csl-block">Steven M Hill, Laura M Heiser, Thomas Cokelaer, Michael Unger, Nicole K Nesser, Daniel E Carlin, Yang Zhang, Artem Sokolov, Evan O Paull, … Sach Mukherjee</div> <em>Nature Methods</em> (2016-02-22) <a href="https://doi.org/f3t7t4">https://doi.org/f3t7t4</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/nmeth.3773">10.1038/nmeth.3773</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26901648">26901648</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4854847">PMC4854847</a></div></div>
</div>
<div id="ref-TUhGg6tD" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">19. </div><div class="csl-right-inline"><strong>Integrating knowledge and omics to decipher mechanisms via large‐scale models of signaling networks</strong> <div class="csl-block">Martin Garrido‐Rodriguez, Katharina Zirngibl, Olga Ivanova, Sebastian Lobentanzer, Julio Saez‐Rodriguez</div> <em>Molecular Systems Biology</em> (2022-07) <a href="https://doi.org/gtb9v8">https://doi.org/gtb9v8</a> <div class="csl-block">DOI: <a href="https://doi.org/10.15252/msb.202211036">10.15252/msb.202211036</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/35880747">35880747</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9316933">PMC9316933</a></div></div>
</div>
<div id="ref-4VxTzwTj" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">20. </div><div class="csl-right-inline"><strong>Illuminating the dark phosphoproteome</strong> <div class="csl-block">Elise J Needham, Benjamin L Parker, Timur Burykin, David E James, Sean J Humphrey</div> <em>Science Signaling</em> (2019-01-22) <a href="https://doi.org/gf8c3h">https://doi.org/gf8c3h</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1126/scisignal.aau8645">10.1126/scisignal.aau8645</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/30670635">30670635</a></div></div>
</div>
<div id="ref-yCFobrFQ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">21. </div><div class="csl-right-inline"><strong>The functional landscape of the human phosphoproteome</strong> <div class="csl-block">David Ochoa, Andrew F Jarnuczak, Cristina Viéitez, Maja Gehre, Margaret Soucheray, André Mateus, Askar A Kleefeldt, Anthony Hill, Luz Garcia-Alonso, Frank Stein, … Pedro Beltrao</div> <em>Nature Biotechnology</em> (2019-12-09) <a href="https://doi.org/ggd8n7">https://doi.org/ggd8n7</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41587-019-0344-3">10.1038/s41587-019-0344-3</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/31819260">31819260</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7100915">PMC7100915</a></div></div>
</div>
<div id="ref-185mDnD0M" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">22. </div><div class="csl-right-inline"><strong>The book of why: the new science of cause and effect</strong> <div class="csl-block">Judea Pearl, Dana Mackenzie</div> <em>Basic Books</em> (2018) <div class="csl-block">ISBN: 9780465097616</div></div>
</div>
<div id="ref-dJUlTjFg" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">23. </div><div class="csl-right-inline"><strong>Causal inference in statistics: An overview</strong> <div class="csl-block">Judea Pearl</div> <em>Statistics Surveys</em> (2009-01-01) <a href="https://doi.org/drt748">https://doi.org/drt748</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1214/09-ss057">10.1214/09-ss057</a></div></div>
</div>
<div id="ref-1A5MoTSGF" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">24. </div><div class="csl-right-inline"><strong>Regression discontinuity designs: A guide to practice</strong> <div class="csl-block">Guido W Imbens, Thomas Lemieux</div> <em>Journal of Econometrics</em> (2008-02) <a href="https://doi.org/bzx7rb">https://doi.org/bzx7rb</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.jeconom.2007.05.001">10.1016/j.jeconom.2007.05.001</a></div></div>
</div>
<div id="ref-152yKY5w7" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">25. </div><div class="csl-right-inline"><strong>Perturb-Seq: Dissecting Molecular Circuits with Scalable Single-Cell RNA Profiling of Pooled Genetic Screens</strong> <div class="csl-block">Atray Dixit, Oren Parnas, Biyu Li, Jenny Chen, Charles P Fulco, Livnat Jerby-Arnon, Nemanja D Marjanovic, Danielle Dionne, Tyler Burks, Raktima Raychowdhury, … Aviv Regev</div> <em>Cell</em> (2016-12) <a href="https://doi.org/f9prjd">https://doi.org/f9prjd</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.cell.2016.11.038">10.1016/j.cell.2016.11.038</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27984732">27984732</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5181115">PMC5181115</a></div></div>
</div>
<div id="ref-1BSi2Dk6R" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">26. </div><div class="csl-right-inline"><strong>Vemurafenib: the first drug approved for BRAF-mutant cancer</strong> <div class="csl-block">Gideon Bollag, James Tsai, Jiazhong Zhang, Chao Zhang, Prabha Ibrahim, Keith Nolop, Peter Hirth</div> <em>Nature Reviews Drug Discovery</em> (2012-10-12) <a href="https://doi.org/f4b975">https://doi.org/f4b975</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/nrd3847">10.1038/nrd3847</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23060265">23060265</a></div></div>
</div>
<div id="ref-ZkFzi3Xl" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">27. </div><div class="csl-right-inline"><strong>Targeting the ERK Signaling Pathway in Melanoma</strong> <div class="csl-block">Paola Savoia, Paolo Fava, Filippo Casoni, Ottavio Cremona</div> <em>International Journal of Molecular Sciences</em> (2019-03-25) <a href="https://doi.org/gtb9v9">https://doi.org/gtb9v9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.3390/ijms20061483">10.3390/ijms20061483</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/30934534">30934534</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6472057">PMC6472057</a></div></div>
</div>
<div id="ref-1DMQH5kzt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">28. </div><div class="csl-right-inline"><strong>A Theory of Causal Learning in Children: Causal Maps and Bayes Nets.</strong> <div class="csl-block">Alison Gopnik, Clark Glymour, David M Sobel, Laura E Schulz, Tamar Kushnir, David Danks</div> <em>Psychological Review</em> (2004) <a href="https://doi.org/fkj59s">https://doi.org/fkj59s</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1037/0033-295x.111.1.3">10.1037/0033-295x.111.1.3</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/14756583">14756583</a></div></div>
</div>
<div id="ref-iWcDZtHu" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">29. </div><div class="csl-right-inline"><strong>How to Grow a Mind: Statistics, Structure, and Abstraction</strong> <div class="csl-block">Joshua B Tenenbaum, Charles Kemp, Thomas L Griffiths, Noah D Goodman</div> <em>Science</em> (2011-03-11) <a href="https://doi.org/d8vvm9">https://doi.org/d8vvm9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1126/science.1192788">10.1126/science.1192788</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/21393536">21393536</a></div></div>
</div>
<div id="ref-B5WSzZkm" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">30. </div><div class="csl-right-inline"><strong>A Survey on Bias and Fairness in Machine Learning</strong> <div class="csl-block">Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, Aram Galstyan</div> <em>arXiv</em> (2019) <a href="https://doi.org/gtb9wn">https://doi.org/gtb9wn</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1908.09635">10.48550/arxiv.1908.09635</a></div></div>
</div>
<div id="ref-2RptKLT2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">31. </div><div class="csl-right-inline"><strong>A Model of Inductive Bias Learning</strong> <div class="csl-block">J Baxter</div> <em>Journal of Artificial Intelligence Research</em> (2000-03-01) <a href="https://doi.org/gg66h8">https://doi.org/gg66h8</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1613/jair.731">10.1613/jair.731</a></div></div>
</div>
<div id="ref-nT3xJkyD" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">32. </div><div class="csl-right-inline"><strong>No Free Lunch Theorems for Search</strong> <div class="csl-block">David H Wolpert, William G Macready</div> <em>Santa Fe Institute</em> (1995) <a href="https://ideas.repec.org/p/wop/safiwp/95-02-010.html">https://ideas.repec.org/p/wop/safiwp/95-02-010.html</a></div>
</div>
<div id="ref-D3JIQ7Oe" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">33. </div><div class="csl-right-inline"><strong>The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning</strong> <div class="csl-block">Micah Goldblum, Marc Finzi, Keefer Rowan, Andrew Gordon Wilson</div> <em>arXiv</em> (2023) <a href="https://doi.org/gtb9wp">https://doi.org/gtb9wp</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2304.05366">10.48550/arxiv.2304.05366</a></div></div>
</div>
<div id="ref-11wELIlTc" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">34. </div><div class="csl-right-inline"><strong>Inductive biases for deep learning of higher-level cognition</strong> <div class="csl-block">Anirudh Goyal, Yoshua Bengio</div> <em>Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences</em> (2022-10) <a href="https://doi.org/gs39f8">https://doi.org/gs39f8</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1098/rspa.2021.0068">10.1098/rspa.2021.0068</a></div></div>
</div>
<div id="ref-1AZn5l2ah" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">35. </div><div class="csl-right-inline"><strong>Current progress and open challenges for applying deep learning across the biosciences</strong> <div class="csl-block">Nicolae Sapoval, Amirali Aghazadeh, Michael G Nute, Dinler A Antunes, Advait Balaji, Richard Baraniuk, CJ Barberan, Ruth Dannenfelser, Chen Dun, Mohammadamin Edrisi, … Todd J Treangen</div> <em>Nature Communications</em> (2022-04-01) <a href="https://doi.org/gp26xk">https://doi.org/gp26xk</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41467-022-29268-7">10.1038/s41467-022-29268-7</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/35365602">35365602</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8976012">PMC8976012</a></div></div>
</div>
<div id="ref-ydpntqD3" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">36. </div><div class="csl-right-inline"><strong>The Bitter Lesson</strong> <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">http://www.incompleteideas.net/IncIdeas/BitterLesson.html</a></div>
</div>
<div id="ref-rh7nCPVE" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">37. </div><div class="csl-right-inline"><strong>Attention Is All You Need</strong> <div class="csl-block">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin</div> <em>arXiv</em> (2017) <a href="https://doi.org/gpnmtv">https://doi.org/gpnmtv</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1706.03762">10.48550/arxiv.1706.03762</a></div></div>
</div>
<div id="ref-eT8vyMzx" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">38. </div><div class="csl-right-inline"><strong>A Better Lesson – Rodney Brooks</strong> (2019-03-19) <a href="https://rodneybrooks.com/a-better-lesson/">https://rodneybrooks.com/a-better-lesson/</a></div>
</div>
<div id="ref-AT0GCO31" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">39. </div><div class="csl-right-inline"><strong>Thread by @shimon8282: "Rich Sutton has a new blog post entitled “The Bitter Lesson” (incompleteideas.net/IncIdeas/Bitte…) that I strongly disagree with. In it, he […]"</strong> <div class="csl-block">https://twitter.com/shimon8282</div> <a href="https://threadreaderapp.com/thread/1106534178676506624.html">https://threadreaderapp.com/thread/1106534178676506624.html</a></div>
</div>
<div id="ref-URCTSFCA" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">40. </div><div class="csl-right-inline"><strong>Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations</strong> <div class="csl-block">Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, Olivier Bachem</div> <em>arXiv</em> (2018) <a href="https://doi.org/grx79c">https://doi.org/grx79c</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1811.12359">10.48550/arxiv.1811.12359</a></div></div>
</div>
<div id="ref-T7D6XA6s" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">41. </div><div class="csl-right-inline"><strong>Toward Causal Representation Learning</strong> <div class="csl-block">Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, Yoshua Bengio</div> <em>Proceedings of the IEEE</em> (2021-05) <a href="https://doi.org/gjhqrh">https://doi.org/gjhqrh</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/jproc.2021.3058954">10.1109/jproc.2021.3058954</a></div></div>
</div>
<div id="ref-vm2M7mI5" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">42. </div><div class="csl-right-inline"><strong>Beyond Predictions in Neural ODEs: Identification and Interventions</strong> <div class="csl-block">Hananeh Aliee, Fabian J Theis, Niki Kilbertus</div> <em>arXiv</em> (2021) <a href="https://doi.org/gszw4d">https://doi.org/gszw4d</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2106.12430">10.48550/arxiv.2106.12430</a></div></div>
</div>
<div id="ref-tr1XjZ1R" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">43. </div><div class="csl-right-inline"><strong>Democratizing knowledge representation with BioCypher</strong> <div class="csl-block">Sebastian Lobentanzer, Patrick Aloy, Jan Baumbach, Balazs Bohar, Vincent J Carey, Pornpimol Charoentong, Katharina Danhauser, Tunca Doğan, Johann Dreo, Ian Dunham, … Julio Saez-Rodriguez</div> <em>Nature Biotechnology</em> (2023-06-19) <a href="https://doi.org/gszqjr">https://doi.org/gszqjr</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41587-023-01848-y">10.1038/s41587-023-01848-y</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/37337100">37337100</a></div></div>
</div>
<div id="ref-s2EFuVEM" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">44. </div><div class="csl-right-inline"><strong>Quantitative and logic modelling of molecular and gene networks</strong> <div class="csl-block">Nicolas Le Novère</div> <em>Nature Reviews Genetics</em> (2015-02-03) <a href="https://doi.org/f6299z">https://doi.org/f6299z</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/nrg3885">10.1038/nrg3885</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25645874">25645874</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4604653">PMC4604653</a></div></div>
</div>
<div id="ref-94lBUqNp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">45. </div><div class="csl-right-inline"><strong>Constraint-based models predict metabolic and associated cellular functions</strong> <div class="csl-block">Aarash Bordbar, Jonathan M Monk, Zachary A King, Bernhard O Palsson</div> <em>Nature Reviews Genetics</em> (2014-01-16) <a href="https://doi.org/f5sk8s">https://doi.org/f5sk8s</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/nrg3643">10.1038/nrg3643</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24430943">24430943</a></div></div>
</div>
<div id="ref-xjVh8IQ5" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">46. </div><div class="csl-right-inline"><strong>Model scale versus domain knowledge in statistical forecasting of chaotic systems</strong> <div class="csl-block">William Gilpin</div> <em>Physical Review Research</em> (2023-12-15) <a href="https://doi.org/gs9xz3">https://doi.org/gs9xz3</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1103/physrevresearch.5.043252">10.1103/physrevresearch.5.043252</a></div></div>
</div>
<div id="ref-kX2zf6UE" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">47. </div><div class="csl-right-inline"><strong>Regression Shrinkage and Selection Via the Lasso</strong> <div class="csl-block">Robert Tibshirani</div> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> (1996-01) <a href="https://doi.org/gfn45m">https://doi.org/gfn45m</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1111/j.2517-6161.1996.tb02080.x">10.1111/j.2517-6161.1996.tb02080.x</a></div></div>
</div>
<div id="ref-cKqFMtL2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">48. </div><div class="csl-right-inline"><strong>A general and flexible method for signal extraction from single-cell RNA-seq data</strong> <div class="csl-block">Davide Risso, Fanny Perraudeau, Svetlana Gribkova, Sandrine Dudoit, Jean-Philippe Vert</div> <em>Nature Communications</em> (2018-01-18) <a href="https://doi.org/gcv5w7">https://doi.org/gcv5w7</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41467-017-02554-5">10.1038/s41467-017-02554-5</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/29348443">29348443</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5773593">PMC5773593</a></div></div>
</div>
<div id="ref-bZ3hxYDX" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">49. </div><div class="csl-right-inline"><strong>Backpropagation Applied to Handwritten Zip Code Recognition</strong> <div class="csl-block">Y LeCun, B Boser, JS Denker, D Henderson, RE Howard, W Hubbard, LD Jackel</div> <em>Neural Computation</em> (1989-12) <a href="https://doi.org/bknd8g">https://doi.org/bknd8g</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1162/neco.1989.1.4.541">10.1162/neco.1989.1.4.541</a></div></div>
</div>
<div id="ref-x4dbEYer" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">50. </div><div class="csl-right-inline"><strong>Long Short-Term Memory</strong> <div class="csl-block">Sepp Hochreiter, Jürgen Schmidhuber</div> <em>Neural Computation</em> (1997-11-01) <a href="https://doi.org/bxd65w">https://doi.org/bxd65w</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1162/neco.1997.9.8.1735">10.1162/neco.1997.9.8.1735</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/9377276">9377276</a></div></div>
</div>
<div id="ref-6W1y3ZrT" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">51. </div><div class="csl-right-inline"><strong>Scaling Laws for Neural Language Models</strong> <div class="csl-block">Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei</div> <em>arXiv</em> (2020) <a href="https://doi.org/gtb96w">https://doi.org/gtb96w</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2001.08361">10.48550/arxiv.2001.08361</a></div></div>
</div>
<div id="ref-OlEfQKqu" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">52. </div><div class="csl-right-inline"><strong>Investigating Power laws in Deep Representation Learning</strong> <div class="csl-block">Arna Ghosh, Arnab Kumar Mondal, Kumar Krishna Agrawal, Blake Richards</div> <em>arXiv</em> (2022) <a href="https://doi.org/gtb966">https://doi.org/gtb966</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2202.05808">10.48550/arxiv.2202.05808</a></div></div>
</div>
<div id="ref-bMaT0Vyc" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">53. </div><div class="csl-right-inline"><strong>Differentiable biology: using deep learning for biophysics-based and data-driven modeling of molecular mechanisms</strong> <div class="csl-block">Mohammed AlQuraishi, Peter K Sorger</div> <em>Nature Methods</em> (2021-10) <a href="https://doi.org/gm2b58">https://doi.org/gm2b58</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41592-021-01283-4">10.1038/s41592-021-01283-4</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/34608321">34608321</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8793939">PMC8793939</a></div></div>
</div>
<div id="ref-KZ19R8ZY" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">54. </div><div class="csl-right-inline"><strong>Artificial neural networks enable genome-scale simulations of intracellular signaling</strong> <div class="csl-block">Avlant Nilsson, Joshua M Peters, Nikolaos Meimetis, Bryan Bryson, Douglas A Lauffenburger</div> <em>Nature Communications</em> (2022-06-02) <a href="https://doi.org/gqd9j9">https://doi.org/gqd9j9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41467-022-30684-y">10.1038/s41467-022-30684-y</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/35654811">35654811</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9163072">PMC9163072</a></div></div>
</div>
<div id="ref-aLhTIs9" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">55. </div><div class="csl-right-inline"><strong>A neural-mechanistic hybrid approach improving the predictive power of genome-scale metabolic models</strong> <div class="csl-block">Léon Faure, Bastien Mollet, Wolfram Liebermeister, Jean-Loup Faulon</div> <em>Nature Communications</em> (2023-08-03) <a href="https://doi.org/gtb96p">https://doi.org/gtb96p</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41467-023-40380-0">10.1038/s41467-023-40380-0</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/37537192">37537192</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10400647">PMC10400647</a></div></div>
</div>
<div id="ref-4smautjA" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">56. </div><div class="csl-right-inline"><strong>Predicting transcriptional outcomes of novel multigene perturbations with GEARS</strong> <div class="csl-block">Yusuf Roohani, Kexin Huang, Jure Leskovec</div> <em>Nature Biotechnology</em> (2023-08-17) <a href="https://doi.org/gtb96r">https://doi.org/gtb96r</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41587-023-01905-6">10.1038/s41587-023-01905-6</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/37592036">37592036</a></div></div>
</div>
<div id="ref-YOm0bqVR" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">57. </div><div class="csl-right-inline"><strong>Knowledge-primed neural networks enable biologically interpretable deep learning on single-cell sequencing data</strong> <div class="csl-block">Nikolaus Fortelny, Christoph Bock</div> <em>Genome Biology</em> (2020-08-03) <a href="https://doi.org/gg8ws9">https://doi.org/gg8ws9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1186/s13059-020-02100-5">10.1186/s13059-020-02100-5</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/32746932">32746932</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7397672">PMC7397672</a></div></div>
</div>
<div id="ref-JkqcQgM7" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">58. </div><div class="csl-right-inline"><strong>Biologically informed deep learning to query gene programs in single-cell atlases</strong> <div class="csl-block">Mohammad Lotfollahi, Sergei Rybakov, Karin Hrovatin, Soroor Hediyeh-zadeh, Carlos Talavera-López, Alexander V Misharin, Fabian J Theis</div> <em>Nature Cell Biology</em> (2023-02-02) <a href="https://doi.org/gtb96q">https://doi.org/gtb96q</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41556-022-01072-x">10.1038/s41556-022-01072-x</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/36732632">36732632</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9928587">PMC9928587</a></div></div>
</div>
<div id="ref-1Xej0UJj" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">59. </div><div class="csl-right-inline"><strong>CellBox: Interpretable Machine Learning for Perturbation Biology with Application to the Design of Cancer Combination Therapy</strong> <div class="csl-block">Bo Yuan, Ciyue Shen, Augustin Luna, Anil Korkut, Debora S Marks, John Ingraham, Chris Sander</div> <em>Cell Systems</em> (2021-02) <a href="https://doi.org/ght4v6">https://doi.org/ght4v6</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.cels.2020.11.013">10.1016/j.cels.2020.11.013</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/33373583">33373583</a></div></div>
</div>
<div id="ref-gAQyFCbW" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">60. </div><div class="csl-right-inline"><strong>Effective gene expression prediction from sequence by integrating long-range interactions</strong> <div class="csl-block">Žiga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, David R Kelley</div> <em>Nature Methods</em> (2021-10) <a href="https://doi.org/gm2wv4">https://doi.org/gm2wv4</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41592-021-01252-x">10.1038/s41592-021-01252-x</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/34608324">34608324</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8490152">PMC8490152</a></div></div>
</div>
<div id="ref-VmzWBJUJ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">61. </div><div class="csl-right-inline"><strong>Transfer learning enables predictions in network biology</strong> <div class="csl-block">Christina V Theodoris, Ling Xiao, Anant Chopra, Mark D Chaffin, Zeina R Al Sayed, Matthew C Hill, Helene Mantineo, Elizabeth M Brydon, Zexian Zeng, XShirley Liu, Patrick T Ellinor</div> <em>Nature</em> (2023-05-31) <a href="https://doi.org/gr9x63">https://doi.org/gr9x63</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41586-023-06139-9">10.1038/s41586-023-06139-9</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/37258680">37258680</a></div></div>
</div>
<div id="ref-r5y0HbhJ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">62. </div><div class="csl-right-inline"><strong>scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI</strong> <div class="csl-block">Haotian Cui, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Bo Wang</div> <em>Cold Spring Harbor Laboratory</em> (2023-05-01) <a href="https://doi.org/gshk6p">https://doi.org/gshk6p</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2023.04.30.538439">10.1101/2023.04.30.538439</a></div></div>
</div>
<div id="ref-1DSO3BUly" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">63. </div><div class="csl-right-inline"><strong>HyperAttention: Long-context Attention in Near-Linear Time</strong> <div class="csl-block">Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David P Woodruff, Amir Zandieh</div> <em>arXiv</em> (2023) <a href="https://doi.org/gtb9wc">https://doi.org/gtb9wc</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2310.05869">10.48550/arxiv.2310.05869</a></div></div>
</div>
<div id="ref-U6LC2Ufe" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">64. </div><div class="csl-right-inline"><strong>Stanford CRFM</strong> <a href="https://crfm.stanford.edu/">https://crfm.stanford.edu/</a></div>
</div>
<div id="ref-WEYqVcYG" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">65. </div><div class="csl-right-inline"><strong>Assessing the limits of zero-shot foundation models in single-cell biology</strong> <div class="csl-block">Kasia Z Kedzierska, Lorin Crawford, Ava P Amini, Alex X Lu</div> <em>Cold Spring Harbor Laboratory</em> (2023-10-17) <a href="https://doi.org/gszxk9">https://doi.org/gszxk9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2023.10.16.561085">10.1101/2023.10.16.561085</a></div></div>
</div>
<div id="ref-OFczH7ba" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">66. </div><div class="csl-right-inline"><strong>A Deep Dive into Single-Cell RNA Sequencing Foundation Models</strong> <div class="csl-block">Rebecca Boiarsky, Nalini Singh, Alejandro Buendia, Gad Getz, David Sontag</div> <em>Cold Spring Harbor Laboratory</em> (2023-10-23) <a href="https://doi.org/gszxmb">https://doi.org/gszxmb</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2023.10.19.563100">10.1101/2023.10.19.563100</a></div></div>
</div>
<div id="ref-1ELFXHA51" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">67. </div><div class="csl-right-inline"><strong>Deep generative modeling for single-cell transcriptomics</strong> <div class="csl-block">Romain Lopez, Jeffrey Regier, Michael B Cole, Michael I Jordan, Nir Yosef</div> <em>Nature Methods</em> (2018-11-30) <a href="https://doi.org/gfkw5z">https://doi.org/gfkw5z</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41592-018-0229-2">10.1038/s41592-018-0229-2</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/30504886">30504886</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6289068">PMC6289068</a></div></div>
</div>
<div id="ref-1GGrqZlzU" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">68. </div><div class="csl-right-inline"><strong>Low-resource finetuning of foundation models beats state-of-the-art in histopathology</strong> <div class="csl-block">Benedikt Roth, Valentin Koch, Sophia J Wagner, Julia A Schnabel, Carsten Marr, Tingying Peng</div> <em>arXiv</em> (2024) <a href="https://doi.org/gtdf95">https://doi.org/gtdf95</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2401.04720">10.48550/arxiv.2401.04720</a></div></div>
</div>
<div id="ref-19EQh1DNG" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">69. </div><div class="csl-right-inline"><strong>ChatGPT broke the Turing test — the race is on for new ways to assess AI</strong> <div class="csl-block">Celeste Biever</div> <em>Nature</em> (2023-07-25) <a href="https://doi.org/gskd92">https://doi.org/gskd92</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/d41586-023-02361-7">10.1038/d41586-023-02361-7</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/37491395">37491395</a></div></div>
</div>
<div id="ref-1GbAsSOZV" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">70. </div><div class="csl-right-inline"><strong>On the Opportunities and Risks of Foundation Models</strong> <div class="csl-block">Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, … Percy Liang</div> <em>arXiv</em> (2021) <a href="https://doi.org/hw3v">https://doi.org/hw3v</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2108.07258">10.48550/arxiv.2108.07258</a></div></div>
</div>
<div id="ref-15hYXC3QB" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">71. </div><div class="csl-right-inline"><strong>Designing an Interpretability-Based Model to Explain the Artificial Intelligence Algorithms in Healthcare</strong> <div class="csl-block">Mohammad Ennab, Hamid Mcheick</div> <em>Diagnostics</em> (2022-06-26) <a href="https://doi.org/gtdf94">https://doi.org/gtdf94</a> <div class="csl-block">DOI: <a href="https://doi.org/10.3390/diagnostics12071557">10.3390/diagnostics12071557</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/35885463">35885463</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9319389">PMC9319389</a></div></div>
</div>
<div id="ref-WmT8ZU5I" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">72. </div><div class="csl-right-inline"><strong>An enquiry concerning human understanding</strong> <div class="csl-block">David Hume, PF Millican</div> <em>Oxford University Press</em> (2007) <div class="csl-block">ISBN: 9780199211586</div></div>
</div>
<div id="ref-13qoNo4Fj" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">73. </div><div class="csl-right-inline"><strong>Review of Dimension Reduction Methods</strong> <div class="csl-block">Salifu Nanga, Ahmed Tijani Bawah, Benjamin Ansah Acquaye, Mac-Issaka Billa, Francis Delali Baeta, Nii Afotey Odai, Samuel Kwaku Obeng, Ampem Darko Nsiah</div> <em>Journal of Data Analysis and Information Processing</em> (2021) <a href="https://doi.org/gtb96v">https://doi.org/gtb96v</a> <div class="csl-block">DOI: <a href="https://doi.org/10.4236/jdaip.2021.93013">10.4236/jdaip.2021.93013</a></div></div>
</div>
<div id="ref-AO84A4MA" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">74. </div><div class="csl-right-inline"><strong>Why the simplest explanation isn’t always the best</strong> <div class="csl-block">Eva L Dyer, Konrad Kording</div> <em>Proceedings of the National Academy of Sciences</em> (2023-12-20) <a href="https://doi.org/gtbqms">https://doi.org/gtbqms</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1073/pnas.2319169120">10.1073/pnas.2319169120</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/38117857">38117857</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10756184">PMC10756184</a></div></div>
</div>
<div id="ref-6McXkHVo" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">75. </div><div class="csl-right-inline"><strong>The specious art of single-cell genomics</strong> <div class="csl-block">Tara Chari, Lior Pachter</div> <em>PLOS Computational Biology</em> (2023-08-17) <a href="https://doi.org/gtb96t">https://doi.org/gtb96t</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1371/journal.pcbi.1011288">10.1371/journal.pcbi.1011288</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/37590228">37590228</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10434946">PMC10434946</a></div></div>
</div>
<div id="ref-iE5sGWcB" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">76. </div><div class="csl-right-inline"><strong>The Uncanny Failure of A.I.-Generated Hands</strong> <div class="csl-block">Kyle Chayka</div> <em>The New Yorker</em> (2023-03-10) <a href="https://www.newyorker.com/culture/rabbit-holes/the-uncanny-failures-of-ai-generated-hands">https://www.newyorker.com/culture/rabbit-holes/the-uncanny-failures-of-ai-generated-hands</a></div>
</div>
<div id="ref-MhOZ3PWC" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">77. </div><div class="csl-right-inline"><strong>Identifying Representations for Intervention Extrapolation</strong> <div class="csl-block">Sorawit Saengkyongam, Elan Rosenfeld, Pradeep Ravikumar, Niklas Pfister, Jonas Peters</div> <em>arXiv</em> (2023) <a href="https://doi.org/gtb97m">https://doi.org/gtb97m</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2310.04295">10.48550/arxiv.2310.04295</a></div></div>
</div>
<div id="ref-FVLAWGsX" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">78. </div><div class="csl-right-inline"><strong>The Causal-Neural Connection: Expressiveness, Learnability, and Inference</strong> <div class="csl-block">Kevin Xia, Kai-Zhan Lee, Yoshua Bengio, Elias Bareinboim</div> <em>arXiv</em> (2021) <a href="https://doi.org/grw6m7">https://doi.org/grw6m7</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2107.00793">10.48550/arxiv.2107.00793</a></div></div>
</div>
<div id="ref-r5mjeIhV" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">79. </div><div class="csl-right-inline"><strong>Exploring the Latent Space of Autoencoders with Interventional Assays</strong> <div class="csl-block">Felix Leeb, Stefan Bauer, Michel Besserve, Bernhard Schölkopf</div> <em>arXiv</em> (2021) <a href="https://doi.org/gtb96x">https://doi.org/gtb96x</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2106.16091">10.48550/arxiv.2106.16091</a></div></div>
</div>
<div id="ref-eEfUqiI4" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">80. </div><div class="csl-right-inline"><strong>Improving black-box optimization in VAE latent space using decoder uncertainty</strong> <div class="csl-block">Pascal Notin, José Miguel Hernández-Lobato, Yarin Gal</div> <em>arXiv</em> (2021) <a href="https://doi.org/gtb962">https://doi.org/gtb962</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2107.00096">10.48550/arxiv.2107.00096</a></div></div>
</div>
<div id="ref-S1VP202R" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">81. </div><div class="csl-right-inline"><strong>On the Number of Experiments Sufficient and in the Worst Case Necessary to Identify All Causal Relations Among N Variables</strong> <div class="csl-block">Frederick Eberhardt, Clark Glymour, Richard Scheines</div> <em>arXiv</em> (2012) <a href="https://doi.org/gtb997">https://doi.org/gtb997</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1207.1389">10.48550/arxiv.1207.1389</a></div></div>
</div>
<div id="ref-7HxYpmt4" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">82. </div><div class="csl-right-inline"><strong>Learning Neural Causal Models from Unknown Interventions</strong> <div class="csl-block">Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Bernhard Schölkopf, Michael C Mozer, Chris Pal, Yoshua Bengio</div> <em>arXiv</em> (2019) <a href="https://doi.org/grw6nc">https://doi.org/grw6nc</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1910.01075">10.48550/arxiv.1910.01075</a></div></div>
</div>
<div id="ref-3MP7gokd" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">83. </div><div class="csl-right-inline"><strong>DiscoBAX - Discovery of optimal intervention sets in genomic experiment design</strong> <div class="csl-block">Clare Lyle, Arash Mehrjou, Pascal Notin, Andrew Jesson, Stefan Bauer, Yarin Gal, Patrick Schwab</div> (2023) <a href="https://openreview.net/forum?id=mBkUeW8rpD6">https://openreview.net/forum?id=mBkUeW8rpD6</a></div>
</div>
<div id="ref-zXrfFfft" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">84. </div><div class="csl-right-inline"><strong>Interventions, Where and How? Experimental Design for Causal Models at Scale</strong> <div class="csl-block">Panagiotis Tigas, Yashas Annadani, Andrew Jesson, Bernhard Schölkopf, Yarin Gal, Stefan Bauer</div> <em>arXiv</em> (2022) <a href="https://doi.org/gtcfvk">https://doi.org/gtcfvk</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2203.02016">10.48550/arxiv.2203.02016</a></div></div>
</div>
<div id="ref-Ex1JrMxh" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">85. </div><div class="csl-right-inline"><strong>CausalBench: A Large-scale Benchmark for Network Inference from Single-cell Perturbation Data</strong> <div class="csl-block">Mathieu Chevalley, Yusuf Roohani, Arash Mehrjou, Jure Leskovec, Patrick Schwab</div> <em>arXiv</em> (2022) <a href="https://doi.org/gtcbbc">https://doi.org/gtcbbc</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2210.17283">10.48550/arxiv.2210.17283</a></div></div>
</div>
</div>
<!-- default theme -->

<style>
  /* import google fonts */
  @import url("https://fonts.googleapis.com/css?family=Open+Sans:400,600,700");
  @import url("https://fonts.googleapis.com/css?family=Source+Code+Pro");

  /* -------------------------------------------------- */
  /* global */
  /* -------------------------------------------------- */

  /* all elements */
  * {
    /* force sans-serif font unless specified otherwise */
    font-family: "Open Sans", "Helvetica", sans-serif;

    /* prevent text inflation on some mobile browsers */
    -webkit-text-size-adjust: none !important;
    -moz-text-size-adjust: none !important;
    -o-text-size-adjust: none !important;
    text-size-adjust: none !important;
  }

  @media only screen {
    /* "page" element */
    body {
      position: relative;
      box-sizing: border-box;
      font-size: 12pt;
      line-height: 1.5;
      max-width: 8.5in;
      margin: 20px auto;
      padding: 40px;
      border-radius: 5px;
      border: solid 1px #bdbdbd;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      background: #ffffff;
    }
  }

  /* when on screen < 8.5in wide */
  @media only screen and (max-width: 8.5in) {
    /* "page" element */
    body {
      padding: 20px;
      margin: 0;
      border-radius: 0;
      border: none;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05) inset;
      background: none;
    }
  }

  /* -------------------------------------------------- */
  /* headings */
  /* -------------------------------------------------- */

  /* all headings */
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    margin: 20px 0;
    padding: 0;
    font-weight: bold;
  }

  /* biggest heading */
  h1 {
    margin: 40px 0;
    text-align: center;
  }

  /* second biggest heading */
  h2 {
    margin-top: 30px;
    padding-bottom: 5px;
    border-bottom: solid 1px #bdbdbd;
  }

  /* heading font sizes */
  h1 {
    font-size: 2em;
  }
  h2 {
    font-size: 1.5em;
  }
  h3 {
    font-size: 1.35em;
  }
  h4 {
    font-size: 1.25em;
  }
  h5 {
    font-size: 1.15em;
  }
  h6 {
    font-size: 1em;
  }

  /* -------------------------------------------------- */
  /* manuscript header */
  /* -------------------------------------------------- */

  /* manuscript title */
  header > h1 {
    margin: 0;
  }

  /* manuscript title caption text (ie "automatically generated on") */
  header + p {
    text-align: center;
    margin-top: 10px;
  }

  /* -------------------------------------------------- */
  /* text elements */
  /* -------------------------------------------------- */

  /* links */
  a {
    color: #2196f3;
    overflow-wrap: break-word;
  }

  /* superscripts and subscripts */
  sub,
  sup {
    /* prevent from affecting line height */
    line-height: 0;
  }

  /* unordered and ordered lists*/
  ul,
  ol {
    padding-left: 20px;
  }

  /* class for styling text semibold */
  .semibold {
    font-weight: 600;
  }

  /* class for styling elements horizontally left aligned */
  .left {
    display: block;
    text-align: left;
    margin-left: auto;
    margin-right: 0;
    justify-content: left;
  }

  /* class for styling elements horizontally centered */
  .center {
    display: block;
    text-align: center;
    margin-left: auto;
    margin-right: auto;
    justify-content: center;
  }

  /* class for styling elements horizontally right aligned */
  .right {
    display: block;
    text-align: right;
    margin-left: 0;
    margin-right: auto;
    justify-content: right;
  }

  /* -------------------------------------------------- */
  /* section elements */
  /* -------------------------------------------------- */

  /* horizontal divider line */
  hr {
    border: none;
    height: 1px;
    background: #bdbdbd;
  }

  /* paragraphs, horizontal dividers, figures, tables, code */
  p,
  hr,
  figure,
  table,
  pre {
    /* treat all as "paragraphs", with consistent vertical margins */
    margin-top: 20px;
    margin-bottom: 20px;
  }

  /* -------------------------------------------------- */
  /* figures */
  /* -------------------------------------------------- */

  /* figure */
  figure {
    max-width: 100%;
    margin-left: auto;
    margin-right: auto;
  }

  /* figure caption */
  figcaption {
    padding: 0;
    padding-top: 10px;
  }

  /* figure image element */
  figure > img,
  figure > svg {
    max-width: 100%;
    display: block;
    margin-left: auto;
    margin-right: auto;
  }

  /* figure auto-number */
  img + figcaption > span:first-of-type,
  svg + figcaption > span:first-of-type {
    font-weight: bold;
    margin-right: 5px;
  }

  /* -------------------------------------------------- */
  /* tables */
  /* -------------------------------------------------- */

  /* table */
  table {
    border-collapse: collapse;
    border-spacing: 0;
    width: 100%;
    margin-left: auto;
    margin-right: auto;
  }

  /* table cells */
  th,
  td {
    border: solid 1px #bdbdbd;
    padding: 10px;
    /* squash table if too wide for page by forcing line breaks */
    overflow-wrap: break-word;
    word-break: break-word;
  }

  /* header row and even rows */
  th,
  tr:nth-child(2n) {
    background-color: #fafafa;
  }

  /* odd rows */
  tr:nth-child(2n + 1) {
    background-color: #ffffff;
  }

  /* table caption */
  caption {
    text-align: left;
    padding: 0;
    padding-bottom: 10px;
  }

  /* table auto-number */
  table > caption > span:first-of-type {
    font-weight: bold;
    margin-right: 5px;
  }

  /* -------------------------------------------------- */
  /* code */
  /* -------------------------------------------------- */

  /* multi-line code block */
  pre {
    padding: 10px;
    background-color: #eeeeee;
    color: #000000;
    border-radius: 5px;
    break-inside: avoid;
    text-align: left;
  }

  /* inline code, ie code within normal text */
  :not(pre) > code {
    padding: 0 4px;
    background-color: #eeeeee;
    color: #000000;
    border-radius: 5px;
  }

  /* code text */
  /* apply all children, to reach syntax highlighting sub-elements */
  code,
  code * {
    /* force monospace font */
    font-family: "Source Code Pro", "Courier New", monospace;
  }

  /* -------------------------------------------------- */
  /* quotes */
  /* -------------------------------------------------- */

  /* quoted text */
  blockquote {
    margin: 0;
    padding: 0;
    border-left: 4px solid #bdbdbd;
    padding-left: 16px;
    break-inside: avoid;
  }

  /* -------------------------------------------------- */
  /* banners */
  /* -------------------------------------------------- */

  /* info banners */
  .banner {
    box-sizing: border-box;
    display: block;
    position: relative;
    width: 100%;
    margin-top: 20px;
    margin-bottom: 20px;
    padding: 20px;
    text-align: center;
  }

  /* paragraph in banner */
  .banner > p {
    margin: 0;
  }

  /* -------------------------------------------------- */
  /* highlight colors */
  /* -------------------------------------------------- */

  .white {
    background: #ffffff;
  }
  .lightgrey {
    background: #eeeeee;
  }
  .grey {
    background: #757575;
  }
  .darkgrey {
    background: #424242;
  }
  .black {
    background: #000000;
  }
  .lightred {
    background: #ffcdd2;
  }
  .lightyellow {
    background: #ffecb3;
  }
  .lightgreen {
    background: #dcedc8;
  }
  .lightblue {
    background: #e3f2fd;
  }
  .lightpurple {
    background: #f3e5f5;
  }
  .red {
    background: #f44336;
  }
  .orange {
    background: #ff9800;
  }
  .yellow {
    background: #ffeb3b;
  }
  .green {
    background: #4caf50;
  }
  .blue {
    background: #2196f3;
  }
  .purple {
    background: #9c27b0;
  }
  .white,
  .lightgrey,
  .lightred,
  .lightyellow,
  .lightgreen,
  .lightblue,
  .lightpurple,
  .orange,
  .yellow,
  .white a,
  .lightgrey a,
  .lightred a,
  .lightyellow a,
  .lightgreen a,
  .lightblue a,
  .lightpurple a,
  .orange a,
  .yellow a {
    color: #000000;
  }
  .grey,
  .darkgrey,
  .black,
  .red,
  .green,
  .blue,
  .purple,
  .grey a,
  .darkgrey a,
  .black a,
  .red a,
  .green a,
  .blue a,
  .purple a {
    color: #ffffff;
  }

  /* -------------------------------------------------- */
  /* buttons */
  /* -------------------------------------------------- */

  /* class for styling links like buttons */
  .button {
    display: inline-flex;
    justify-content: center;
    align-items: center;
    margin: 5px;
    padding: 10px 20px;
    font-size: 0.75em;
    font-weight: 600;
    text-transform: uppercase;
    text-decoration: none;
    letter-spacing: 1px;
    background: none;
    color: #2196f3;
    border: solid 1px #bdbdbd;
    border-radius: 5px;
  }

  /* buttons when hovered */
  .button:hover:not([disabled]),
  .icon_button:hover:not([disabled]) {
    cursor: pointer;
    background: #f5f5f5;
  }

  /* buttons when disabled */
  .button[disabled],
  .icon_button[disabled] {
    opacity: 0.35;
    pointer-events: none;
  }

  /* class for styling buttons containg only single icon */
  .icon_button {
    display: inline-flex;
    justify-content: center;
    align-items: center;
    text-decoration: none;
    margin: 0;
    padding: 0;
    background: none;
    border-radius: 5px;
    border: none;
    width: 20px;
    height: 20px;
    min-width: 20px;
    min-height: 20px;
  }

  /* icon button inner svg image */
  .icon_button > svg {
    height: 16px;
  }

  /* -------------------------------------------------- */
  /* icons */
  /* -------------------------------------------------- */

  /* class for styling icons inline with text */
  .inline_icon {
    height: 1em;
    position: relative;
    top: 0.125em;
  }

  /* -------------------------------------------------- */
  /* references */
  /* -------------------------------------------------- */

  .csl-entry {
    margin-top: 15px;
    margin-bottom: 15px;
  }

  /* -------------------------------------------------- */
  /* print control */
  /* -------------------------------------------------- */

  @media print {
    @page {
      /* suggested printing margin */
      margin: 0.5in;
    }

    /* document and "page" elements */
    html,
    body {
      margin: 0;
      padding: 0;
      width: 100%;
      height: 100%;
    }

    /* "page" element */
    body {
      font-size: 11pt !important;
      line-height: 1.35;
    }

    /* all headings */
    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      margin: 15px 0;
    }

    /* figures and tables */
    figure,
    table {
      font-size: 0.85em;
    }

    /* table cells */
    th,
    td {
      padding: 5px;
    }

    /* shrink font awesome icons */
    i.fas,
    i.fab,
    i.far,
    i.fal {
      transform: scale(0.85);
    }

    /* decrease banner margins */
    .banner {
      margin-top: 15px;
      margin-bottom: 15px;
      padding: 15px;
    }

    /* class for centering an element vertically on its own page */
    .page_center {
      margin: auto;
      width: 100%;
      height: 100%;
      display: flex;
      align-items: center;
      vertical-align: middle;
      break-before: page;
      break-after: page;
    }

    /* always insert a page break before the element */
    .page_break_before {
      break-before: page;
    }

    /* always insert a page break after the element */
    .page_break_after {
      break-after: page;
    }

    /* avoid page break before the element */
    .page_break_before_avoid {
      break-before: avoid;
    }

    /* avoid page break after the element */
    .page_break_after_avoid {
      break-after: avoid;
    }

    /* avoid page break inside the element */
    .page_break_inside_avoid {
      break-inside: avoid;
    }
  }

  /* -------------------------------------------------- */
  /* override pandoc css quirks */
  /* -------------------------------------------------- */

  .sourceCode {
    /* prevent unsightly overflow in wide code blocks */
    overflow: auto !important;
  }

  div.sourceCode {
    /* prevent background fill on top-most code block  container */
    background: none !important;
  }

  .sourceCode * {
    /* force consistent line spacing */
    line-height: 1.5 !important;
  }

  div.sourceCode {
    /* style code block margins same as <pre> element */
    margin-top: 20px;
    margin-bottom: 20px;
  }

  /* -------------------------------------------------- */
  /* tablenos */
  /* -------------------------------------------------- */

  /* tablenos wrapper */
  .tablenos {
    width: 100%;
    margin: 20px 0;
  }

  .tablenos > table {
    /* move margins from table to table_wrapper to allow margin collapsing */
    margin: 0;
  }

  @media only screen {
    /* tablenos wrapper */
    .tablenos {
      /* show scrollbar on tables if necessary to prevent overflow */
      overflow-x: auto !important;
    }

    .tablenos th,
    .tablenos td {
      overflow-wrap: unset !important;
      word-break: unset !important;
    }

    /* table in wrapper */
    .tablenos table,
    .tablenos table * {
      /* don't break table words */
      overflow-wrap: normal !important;
    }
  }
</style>
<!-- 
    Plugin Core

    Functions needed for and shared across all first-party plugins.
-->

<script>
  // get element that is target of hash (from link element or url)
  function getHashTarget(link) {
    const hash = link ? link.hash : window.location.hash;
    const id = hash.slice(1);
    let target = document.querySelector(`[id="${id}"]`);
    if (!target) return;

    // if figure or table, modify target to get expected element
    if (id.indexOf("fig:") === 0) target = target.querySelector("figure");
    if (id.indexOf("tbl:") === 0) target = target.querySelector("table");

    return target;
  }

  // get position/dimensions of element or viewport
  function getRectInView(element) {
    let rect = {};
    rect.left = 0;
    rect.top = 0;
    rect.right = document.documentElement.clientWidth;
    rect.bottom = document.documentElement.clientHeight;
    let style = {};

    if (element instanceof HTMLElement) {
      rect = element.getBoundingClientRect();
      style = window.getComputedStyle(element);
    }

    const margin = {};
    margin.left = parseFloat(style.marginLeftWidth) || 0;
    margin.top = parseFloat(style.marginTopWidth) || 0;
    margin.right = parseFloat(style.marginRightWidth) || 0;
    margin.bottom = parseFloat(style.marginBottomWidth) || 0;

    const border = {};
    border.left = parseFloat(style.borderLeftWidth) || 0;
    border.top = parseFloat(style.borderTopWidth) || 0;
    border.right = parseFloat(style.borderRightWidth) || 0;
    border.bottom = parseFloat(style.borderBottomWidth) || 0;

    const newRect = {};
    newRect.left = rect.left + margin.left + border.left;
    newRect.top = rect.top + margin.top + border.top;
    newRect.right = rect.right + margin.right + border.right;
    newRect.bottom = rect.bottom + margin.bottom + border.bottom;
    newRect.width = newRect.right - newRect.left;
    newRect.height = newRect.bottom - newRect.top;

    return newRect;
  }

  // get position of element relative to page
  function getRectInPage(element) {
    const rect = getRectInView(element);
    const body = getRectInView(document.body);

    const newRect = {};
    newRect.left = rect.left - body.left;
    newRect.top = rect.top - body.top;
    newRect.right = rect.right - body.left;
    newRect.bottom = rect.bottom - body.top;
    newRect.width = rect.width;
    newRect.height = rect.height;

    return newRect;
  }

  // get closest element before specified element that matches query
  function firstBefore(element, query) {
    while (element && element !== document.body && !element.matches(query))
      element = element.previousElementSibling || element.parentNode;

    return element;
  }

  // check if element is part of collapsed heading
  function isCollapsed(element) {
    while (element && element !== document.body) {
      if (element.dataset.collapsed === "true") return true;
      element = element.parentNode;
    }
    return false;
  }

  // expand any collapsed parent containers of element if necessary
  function expandElement(element) {
    if (isCollapsed(element)) {
      // accordion plugin
      const heading = firstBefore(element, "h2");
      if (heading) heading.click();
      // details/summary HTML element
      const summary = firstBefore(element, "summary");
      if (summary) summary.click();
    }
  }

  // scroll to and focus element
  function goToElement(element, offset) {
    // expand accordion section if collapsed
    expandElement(element);
    const y =
      getRectInView(element).top -
      getRectInView(document.documentElement).top -
      (offset || 0);

    // trigger any function listening for "onscroll" event
    window.dispatchEvent(new Event("scroll"));
    window.scrollTo(0, y);
    document.activeElement.blur();
    element.focus();
  }

  // get list of elements after a start element up to element matching query
  function nextUntil(element, query, exclude) {
    const elements = [];
    while (((element = element.nextElementSibling), element)) {
      if (element.matches(query)) break;
      if (!element.matches(exclude)) elements.push(element);
    }
    return elements;
  }
</script>
<!--
  Accordion Plugin

  Allows sections of content under h2 headings to be collapsible.
-->

<script type="module">
  // whether to always start expanded ('false'), always start collapsed
  // ('true'), or start collapsed when screen small ('auto')
  const startCollapsed = "auto";

  // start script
  function start() {
    // run through each <h2> heading
    const headings = document.querySelectorAll("h2");
    for (const heading of headings) {
      addArrow(heading);

      // start expanded/collapsed based on option
      if (
        startCollapsed === "true" ||
        (startCollapsed === "auto" && isSmallScreen()) ||
        heading.dataset.collapsed === "true"
      )
        collapseHeading(heading);
      else expandElement(heading);
    }

    // attach hash change listener to window
    window.addEventListener("hashchange", onHashChange);
  }

  // when hash (eg manuscript.html#introduction) changes
  function onHashChange() {
    const target = getHashTarget();
    if (target) goToElement(target);
  }

  // add arrow to heading
  function addArrow(heading) {
    // add arrow button
    const arrow = document.createElement("button");
    arrow.innerHTML = document.querySelector(".icon_angle_down").innerHTML;
    arrow.classList.add("icon_button", "accordion_arrow");
    heading.insertBefore(arrow, heading.firstChild);

    // attach click listener to heading and button
    heading.addEventListener("click", onHeadingClick);
    arrow.addEventListener("click", onArrowClick);
  }

  // determine if on mobile-like device with small screen
  function isSmallScreen() {
    return Math.min(window.innerWidth, window.innerHeight) < 480;
  }

  // when <h2> heading is clicked
  function onHeadingClick(event) {
    // only collapse if <h2> itself is target of click (eg, user did
    // not click on anchor within <h2>)
    if (event.target === this) toggleCollapse(this);
  }

  // when arrow button is clicked
  function onArrowClick() {
    toggleCollapse(this.parentNode);
  }

  // collapse section if expanded, expand if collapsed
  function toggleCollapse(heading) {
    if (heading.dataset.collapsed === "false") collapseHeading(heading);
    else expandElement(heading);
  }

  // elements to exclude from collapse, such as table of contents panel,
  // hypothesis panel, etc
  const exclude = "#toc_panel, div.annotator-frame, #lightbox_overlay";

  // collapse section
  function collapseHeading(heading) {
    heading.setAttribute("data-collapsed", "true");
    const children = getChildren(heading);
    for (const child of children) child.setAttribute("data-collapsed", "true");
  }

  // expand section
  function expandElement(heading) {
    heading.setAttribute("data-collapsed", "false");
    const children = getChildren(heading);
    for (const child of children) child.setAttribute("data-collapsed", "false");
  }

  // get list of elements between this <h2> and next <h2> or <h1>
  // ("children" of the <h2> section)
  function getChildren(heading) {
    return nextUntil(heading, "h2, h1", exclude);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- angle down icon -->

<template class="icon_angle_down">
  <!-- modified from: https://fontawesome.com/icons/angle-down -->
  <svg width="16" height="16" viewBox="0 0 448 512">
    <path
      fill="currentColor"
      d="M207.029 381.476L12.686 187.132c-9.373-9.373-9.373-24.569 0-33.941l22.667-22.667c9.357-9.357 24.522-9.375 33.901-.04L224 284.505l154.745-154.021c9.379-9.335 24.544-9.317 33.901.04l22.667 22.667c9.373 9.373 9.373 24.569 0 33.941L240.971 381.476c-9.373 9.372-24.569 9.372-33.942 0z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* accordion arrow button */
    .accordion_arrow {
      margin-right: 10px;
    }

    /* arrow icon when <h2> data-collapsed attribute true */
    h2[data-collapsed="true"] > .accordion_arrow > svg {
      transform: rotate(-90deg);
    }

    /* all elements (except <h2>'s) when data-collapsed attribute true */
    *:not(h2)[data-collapsed="true"] {
      display: none;
    }

    /* accordion arrow button when hovered and <h2>'s when hovered */
    .accordion_arrow:hover,
    h2[data-collapsed="true"]:hover,
    h2[data-collapsed="false"]:hover {
      cursor: pointer;
    }
  }

  /* always hide accordion arrow button on print */
  @media only print {
    .accordion_arrow {
      display: none;
    }
  }
</style>
<!--
  Anchors Plugin

  Adds an anchor next to each of a certain type of element that provides a
  human-readable url to that specific item/position in the document (e.g.
  "manuscript.html#abstract"). It also makes it such that scrolling out of view
  of a target removes its identifier from the url.
-->

<script type="module">
  // which types of elements to add anchors next to, in "document.querySelector"
  // format
  const typesQuery =
    'h1, h2, h3, div[id^="fig:"], div[id^="tbl:"], span[id^="eq:"]';

  // start script
  function start() {
    // add anchor to each element of specified types
    const elements = document.querySelectorAll(typesQuery);
    for (const element of elements) addAnchor(element);

    // attach scroll listener to window
    window.addEventListener("scroll", onScroll);
  }

  // when window is scrolled
  function onScroll() {
    // if url has hash and user has scrolled out of view of hash
    // target, remove hash from url
    const tolerance = 100;
    const target = getHashTarget();
    if (target) {
      if (
        target.getBoundingClientRect().top > window.innerHeight + tolerance ||
        target.getBoundingClientRect().bottom < 0 - tolerance
      )
        history.pushState(null, null, " ");
    }
  }

  // add anchor to element
  function addAnchor(element) {
    let addTo; // element to add anchor button to

    // if figure or table, modify withId and addTo to get expected
    // elements
    if (element.id.indexOf("fig:") === 0) {
      addTo = element.querySelector("figcaption");
    } else if (element.id.indexOf("tbl:") === 0) {
      addTo = element.querySelector("caption");
    } else if (element.id.indexOf("eq:") === 0) {
      addTo = element.querySelector(".eqnos-number");
    }

    addTo = addTo || element;
    const id = element.id || null;

    // do not add anchor if element doesn't have assigned id.
    // id is generated by pandoc and is assumed to be unique and
    // human-readable
    if (!id) return;

    // create anchor button
    const anchor = document.createElement("a");
    anchor.innerHTML = document.querySelector(".icon_link").innerHTML;
    anchor.title = "Link to this part of the document";
    anchor.classList.add("icon_button", "anchor");
    anchor.dataset.ignore = "true";
    anchor.href = "#" + id;
    addTo.appendChild(anchor);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- link icon -->

<template class="icon_link">
  <!-- modified from: https://fontawesome.com/icons/link -->
  <svg width="16" height="16" viewBox="0 0 512 512">
    <path
      fill="currentColor"
      d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* anchor button */
    .anchor {
      opacity: 0;
      margin-left: 5px;
    }

    /* anchor buttons within <h2>'s */
    h2 .anchor {
      margin-left: 10px;
    }

    /* anchor buttons when hovered/focused and anything containing an anchor button when hovered */
    *:hover > .anchor,
    .anchor:hover,
    .anchor:focus {
      opacity: 1;
    }

    /* anchor button when hovered */
    .anchor:hover {
      cursor: pointer;
    }
  }

  /* always show anchor button on devices with no mouse/hover ability */
  @media (hover: none) {
    .anchor {
      opacity: 1;
    }
  }

  /* always hide anchor button on print */
  @media only print {
    .anchor {
      display: none;
    }
  }
</style>
<!-- 
    Attributes Plugin

    Allows arbitrary HTML attributes to be attached to (almost) any element.
    Place an HTML comment inside or next to the desired element with the content:
    $attribute="value"
-->

<script type="module">
  // start script
  function start() {
    // get list of comments in document
    const comments = findComments();

    for (const comment of comments)
      if (comment.parentElement)
        addAttributes(comment.parentElement, comment.nodeValue.trim());
  }

  // add html attributes to specified element based on string of
  // html attributes and values
  function addAttributes(element, text) {
    // regex's for finding attribute/value pairs in the format of
    // attribute="value" or attribute='value
    const regex2 = /\$([a-zA-Z\-]+)?=\"(.+?)\"/;
    const regex1 = /\$([a-zA-Z\-]+)?=\'(.+?)\'/;

    // loop through attribute/value pairs
    let match;
    while ((match = text.match(regex2) || text.match(regex1))) {
      // get attribute and value from regex capture groups
      let attribute = match[1];
      let value = match[2];

      // remove from string
      text = text.substring(match.index + match[0].length);

      if (!attribute || !value) break;

      // set attribute of parent element
      try {
        element.setAttribute(attribute, value);
      } catch (error) {
        console.log(error);
      }

      // special case for colspan
      if (attribute === "colspan") removeTableCells(element, value);
    }
  }

  // get list of comment elements in document
  function findComments() {
    const comments = [];

    // iterate over comment nodes in document
    function acceptNode(node) {
      return NodeFilter.FILTER_ACCEPT;
    }
    const iterator = document.createNodeIterator(
      document.body,
      NodeFilter.SHOW_COMMENT,
      acceptNode
    );
    let node;
    while ((node = iterator.nextNode())) comments.push(node);

    return comments;
  }

  // remove certain number of cells after specified cell
  function removeTableCells(cell, number) {
    number = parseInt(number);
    if (!number) return;

    // remove elements
    for (; number > 1; number--) {
      if (cell.nextElementSibling) cell.nextElementSibling.remove();
    }
  }

  // start script on DOMContentLoaded instead of load to ensure this plugins
  // runs before other plugins
  window.addEventListener("DOMContentLoaded", start);
</script>
<!--
  Jump to First Plugin

  Adds a button next to each reference entry, figure, and table that jumps the
  page to the first occurrence of a link to that item in the manuscript.
-->

<script type="module">
  // whether to add buttons next to reference entries
  const references = "true";
  // whether to add buttons next to figures
  const figures = "true";
  // whether to add buttons next to tables
  const tables = "true";

  // start script
  function start() {
    if (references !== "false")
      makeButtons(`div[id^="ref-"]`, ".csl-left-margin", "reference");
    if (figures !== "false")
      makeButtons(`div[id^="fig:"]`, "figcaption", "figure");
    if (tables !== "false") makeButtons(`div[id^="tbl:"]`, "caption", "table");
  }

  // when jump button clicked
  function onButtonClick() {
    const first = getFirstOccurrence(this.dataset.id);
    if (!first) return;

    // update url hash so navigating "back" in history will return user to button
    window.location.hash = this.dataset.id;
    // scroll to link
    const timeout = function () {
      goToElement(first, window.innerHeight * 0.5);
    };
    window.setTimeout(timeout, 0);
  }

  // get first occurrence of link to item in document
  function getFirstOccurrence(id) {
    let query = "a";
    query += '[href="#' + id + '"]';
    // exclude buttons, anchor links, toc links, etc
    query += ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    return document.querySelector(query);
  }

  // add button next to each reference entry, figure, or table
  function makeButtons(query, containerQuery, subject) {
    const elements = document.querySelectorAll(query);
    for (const element of elements) {
      const id = element.id;
      const buttonContainer = element.querySelector(containerQuery);
      const first = getFirstOccurrence(id);

      // if can't find link to reference or place to put button, ignore
      if (!first || !buttonContainer) continue;

      // make jump button
      let button = document.createElement("button");
      button.classList.add("icon_button", "jump_arrow");
      button.title = `Jump to the first occurrence of this ${subject} in the document`;
      const icon = document.querySelector(".icon_angle_double_up");
      button.innerHTML = icon.innerHTML;
      button.dataset.id = id;
      button.dataset.ignore = "true";
      button.addEventListener("click", onButtonClick);
      buttonContainer.prepend(button);
    }
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- angle double up icon -->

<template class="icon_angle_double_up">
  <!-- modified from: https://fontawesome.com/icons/angle-double-up -->
  <svg width="16" height="16" viewBox="0 0 320 512">
    <path
      fill="currentColor"
      d="M177 255.7l136 136c9.4 9.4 9.4 24.6 0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9 0L160 351.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9 0L7 425.7c-9.4-9.4-9.4-24.6 0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1zm-34-192L7 199.7c-9.4 9.4-9.4 24.6 0 33.9l22.6 22.6c9.4 9.4 24.6 9.4 33.9 0l96.4-96.4 96.4 96.4c9.4 9.4 24.6 9.4 33.9 0l22.6-22.6c9.4-9.4 9.4-24.6 0-33.9l-136-136c-9.2-9.4-24.4-9.4-33.8 0z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* jump button */
    .jump_arrow {
      position: relative;
      top: 0.125em;
      margin-right: 5px;
    }
  }

  /* always hide jump button on print */
  @media only print {
    .jump_arrow {
      display: none;
    }
  }
</style>
<!-- 
    Lightbox Plugin

    Makes it such that when a user clicks on an image, the image fills the
    screen and the user can pan/drag/zoom the image and navigate between other
    images in the document.
-->

<script type="module">
  // list of possible zoom/scale factors
  const zooms =
    "0.1, 0.25, 0.333333, 0.5, 0.666666, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.5, 3, 3.5, 4, 5, 6, 7, 8";
  // whether to fit image to view ('fit'), display at 100% and shrink if
  // necessary ('shrink'), or always display at 100% ('100')
  const defaultZoom = "fit";
  // whether to zoom in/out toward center of view ('true') or mouse ('false')
  const centerZoom = "false";

  // start script
  function start() {
    // run through each <img> element
    const imgs = document.querySelectorAll("figure > img");
    let count = 1;
    for (const img of imgs) {
      img.classList.add("lightbox_document_img");
      img.dataset.number = count;
      img.dataset.total = imgs.length;
      img.addEventListener("click", openLightbox);
      count++;
    }

    // attach mouse and key listeners to window
    window.addEventListener("mousemove", onWindowMouseMove);
    window.addEventListener("keyup", onKeyUp);
  }

  // when mouse is moved anywhere in window
  function onWindowMouseMove(event) {
    window.mouseX = event.clientX;
    window.mouseY = event.clientY;
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    switch (event.key) {
      // trigger click of prev button
      case "ArrowLeft":
        const prevButton = document.getElementById("lightbox_prev_button");
        if (prevButton) prevButton.click();
        break;
      // trigger click of next button
      case "ArrowRight":
        const nextButton = document.getElementById("lightbox_next_button");
        if (nextButton) nextButton.click();
        break;
      // close on esc
      case "Escape":
        closeLightbox();
        break;
    }
  }

  // open lightbox
  function openLightbox() {
    const lightbox = makeLightbox(this);
    if (!lightbox) return;

    blurBody(lightbox);
    document.body.appendChild(lightbox);
  }

  // make lightbox
  function makeLightbox(img) {
    // delete lightbox if it exists, start fresh
    closeLightbox();

    // create screen overlay containing lightbox
    const overlay = document.createElement("div");
    overlay.id = "lightbox_overlay";

    // create image info boxes
    const numberInfo = document.createElement("div");
    const zoomInfo = document.createElement("div");
    numberInfo.id = "lightbox_number_info";
    zoomInfo.id = "lightbox_zoom_info";

    // create container for image
    const imageContainer = document.createElement("div");
    imageContainer.id = "lightbox_image_container";
    const lightboxImg = makeLightboxImg(
      img,
      imageContainer,
      numberInfo,
      zoomInfo
    );
    imageContainer.appendChild(lightboxImg);

    // create bottom container for caption and navigation buttons
    const bottomContainer = document.createElement("div");
    bottomContainer.id = "lightbox_bottom_container";
    const caption = makeCaption(img);
    const prevButton = makePrevButton(img);
    const nextButton = makeNextButton(img);
    bottomContainer.appendChild(prevButton);
    bottomContainer.appendChild(caption);
    bottomContainer.appendChild(nextButton);

    // attach top middle and bottom to overlay
    overlay.appendChild(numberInfo);
    overlay.appendChild(zoomInfo);
    overlay.appendChild(imageContainer);
    overlay.appendChild(bottomContainer);

    return overlay;
  }

  // make <img> object that is intuitively draggable and zoomable
  function makeLightboxImg(sourceImg, container, numberInfoBox, zoomInfoBox) {
    // create copy of source <img>
    const img = sourceImg.cloneNode(true);
    img.classList.remove("lightbox_document_img");
    img.removeAttribute("id");
    img.removeAttribute("width");
    img.removeAttribute("height");
    img.style.position = "unset";
    img.style.margin = "0";
    img.style.padding = "0";
    img.style.width = "";
    img.style.height = "";
    img.style.minWidth = "";
    img.style.minHeight = "";
    img.style.maxWidth = "";
    img.style.maxHeight = "";
    img.id = "lightbox_img";

    // build sorted list of zoomSteps
    const zoomSteps = zooms.split(/[^0-9.]/).map((step) => parseFloat(step));
    zoomSteps.sort((a, b) => a - b);

    // <img> object property variables
    let zoom = 1;
    let translateX = 0;
    let translateY = 0;
    let clickMouseX = undefined;
    let clickMouseY = undefined;
    let clickTranslateX = undefined;
    let clickTranslateY = undefined;

    updateNumberInfo();

    // update image numbers displayed in info box
    function updateNumberInfo() {
      numberInfoBox.innerHTML =
        sourceImg.dataset.number + " of " + sourceImg.dataset.total;
    }

    // update zoom displayed in info box
    function updateZoomInfo() {
      let zoomInfo = zoom * 100;
      if (!Number.isInteger(zoomInfo)) zoomInfo = zoomInfo.toFixed(2);
      zoomInfoBox.innerHTML = zoomInfo + "%";
    }

    // move to closest zoom step above current zoom
    const zoomIn = function () {
      for (const zoomStep of zoomSteps) {
        if (zoomStep > zoom) {
          zoom = zoomStep;
          break;
        }
      }
      updateTransform();
    };

    // move to closest zoom step above current zoom
    const zoomOut = function () {
      zoomSteps.reverse();
      for (const zoomStep of zoomSteps) {
        if (zoomStep < zoom) {
          zoom = zoomStep;
          break;
        }
      }
      zoomSteps.reverse();

      updateTransform();
    };

    // update display of <img> based on scale/translate properties
    const updateTransform = function () {
      // set transform
      img.style.transform =
        "translate(" +
        (translateX || 0) +
        "px," +
        (translateY || 0) +
        "px) scale(" +
        (zoom || 1) +
        ")";

      // get new width/height after scale
      const rect = img.getBoundingClientRect();
      // limit translate
      translateX = Math.max(translateX, -rect.width / 2);
      translateX = Math.min(translateX, rect.width / 2);
      translateY = Math.max(translateY, -rect.height / 2);
      translateY = Math.min(translateY, rect.height / 2);

      // set transform
      img.style.transform =
        "translate(" +
        (translateX || 0) +
        "px," +
        (translateY || 0) +
        "px) scale(" +
        (zoom || 1) +
        ")";

      updateZoomInfo();
    };

    // fit <img> to container
    const fit = function () {
      // no x/y offset, 100% zoom by default
      translateX = 0;
      translateY = 0;
      zoom = 1;

      // widths of <img> and container
      const imgWidth = img.naturalWidth;
      const imgHeight = img.naturalHeight;
      const containerWidth = parseFloat(
        window.getComputedStyle(container).width
      );
      const containerHeight = parseFloat(
        window.getComputedStyle(container).height
      );

      // how much zooming is needed to fit <img> to container
      const xRatio = imgWidth / containerWidth;
      const yRatio = imgHeight / containerHeight;
      const maxRatio = Math.max(xRatio, yRatio);
      const newZoom = 1 / maxRatio;

      // fit <img> to container according to option
      if (defaultZoom === "shrink") {
        if (maxRatio > 1) zoom = newZoom;
      } else if (defaultZoom === "fit") zoom = newZoom;

      updateTransform();
    };

    // when mouse wheel is rolled anywhere in container
    const onContainerWheel = function (event) {
      if (!event) return;

      // let ctrl + mouse wheel to zoom behave as normal
      if (event.ctrlKey) return;

      // prevent normal scroll behavior
      event.preventDefault();
      event.stopPropagation();

      // point around which to scale img
      const viewRect = container.getBoundingClientRect();
      const viewX = (viewRect.left + viewRect.right) / 2;
      const viewY = (viewRect.top + viewRect.bottom) / 2;
      const originX = centerZoom === "true" ? viewX : mouseX;
      const originY = centerZoom === "true" ? viewY : mouseY;

      // get point on image under origin
      const oldRect = img.getBoundingClientRect();
      const oldPercentX = (originX - oldRect.left) / oldRect.width;
      const oldPercentY = (originY - oldRect.top) / oldRect.height;

      // increment/decrement zoom
      if (event.deltaY < 0) zoomIn();
      if (event.deltaY > 0) zoomOut();

      // get offset between previous image point and origin
      const newRect = img.getBoundingClientRect();
      const offsetX = originX - (newRect.left + newRect.width * oldPercentX);
      const offsetY = originY - (newRect.top + newRect.height * oldPercentY);

      // translate image to keep image point under origin
      translateX += offsetX;
      translateY += offsetY;

      // perform translate
      updateTransform();
    };

    // when container is clicked
    function onContainerClick(event) {
      // if container itself is target of click, and not other
      // element above it
      if (event.target === this) closeLightbox();
    }

    // when mouse button is pressed on image
    const onImageMouseDown = function (event) {
      // store original mouse position relative to image
      clickMouseX = window.mouseX;
      clickMouseY = window.mouseY;
      clickTranslateX = translateX;
      clickTranslateY = translateY;
      event.stopPropagation();
      event.preventDefault();
    };

    // when mouse button is released anywhere in window
    const onWindowMouseUp = function (event) {
      // reset original mouse position
      clickMouseX = undefined;
      clickMouseY = undefined;
      clickTranslateX = undefined;
      clickTranslateY = undefined;

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("mouseup", onWindowMouseUp);
    };

    // when mouse is moved anywhere in window
    const onWindowMouseMove = function (event) {
      if (
        clickMouseX === undefined ||
        clickMouseY === undefined ||
        clickTranslateX === undefined ||
        clickTranslateY === undefined
      )
        return;

      // offset image based on original and current mouse position
      translateX = clickTranslateX + window.mouseX - clickMouseX;
      translateY = clickTranslateY + window.mouseY - clickMouseY;
      updateTransform();
      event.preventDefault();

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("mousemove", onWindowMouseMove);
    };

    // when window is resized
    const onWindowResize = function (event) {
      fit();

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("resize", onWindowResize);
    };

    // attach the necessary event listeners
    img.addEventListener("dblclick", fit);
    img.addEventListener("mousedown", onImageMouseDown);
    container.addEventListener("wheel", onContainerWheel);
    container.addEventListener("mousedown", onContainerClick);
    container.addEventListener("touchstart", onContainerClick);
    window.addEventListener("mouseup", onWindowMouseUp);
    window.addEventListener("mousemove", onWindowMouseMove);
    window.addEventListener("resize", onWindowResize);

    // run fit() after lightbox atttached to document and <img> Loaded
    // so needed container and img dimensions available
    img.addEventListener("load", fit);

    return img;
  }

  // make caption
  function makeCaption(img) {
    const caption = document.createElement("div");
    caption.id = "lightbox_caption";
    const captionSource = img.nextElementSibling;
    if (captionSource.tagName.toLowerCase() === "figcaption") {
      const captionCopy = makeCopy(captionSource);
      caption.innerHTML = captionCopy.innerHTML;
    }

    caption.addEventListener("touchstart", function (event) {
      event.stopPropagation();
    });

    return caption;
  }

  // make carbon copy of html dom element
  function makeCopy(source) {
    const sourceCopy = source.cloneNode(true);

    // delete elements marked with ignore (eg anchor and jump buttons)
    const deleteFromCopy = sourceCopy.querySelectorAll('[data-ignore="true"]');
    for (const element of deleteFromCopy) element.remove();

    // delete certain element attributes
    const attributes = [
      "id",
      "data-collapsed",
      "data-selected",
      "data-highlighted",
      "data-glow",
    ];
    for (const attribute of attributes) {
      sourceCopy.removeAttribute(attribute);
      const elements = sourceCopy.querySelectorAll("[" + attribute + "]");
      for (const element of elements) element.removeAttribute(attribute);
    }

    return sourceCopy;
  }

  // make button to jump to previous image in document
  function makePrevButton(img) {
    const prevButton = document.createElement("button");
    prevButton.id = "lightbox_prev_button";
    prevButton.title = "Jump to the previous image in the document [←]";
    prevButton.classList.add("icon_button", "lightbox_button");
    prevButton.innerHTML = document.querySelector(".icon_caret_left").innerHTML;

    // attach click listeners to button
    prevButton.addEventListener("click", function () {
      getPrevImg(img).click();
    });

    return prevButton;
  }

  // make button to jump to next image in document
  function makeNextButton(img) {
    const nextButton = document.createElement("button");
    nextButton.id = "lightbox_next_button";
    nextButton.title = "Jump to the next image in the document [→]";
    nextButton.classList.add("icon_button", "lightbox_button");
    nextButton.innerHTML = document.querySelector(
      ".icon_caret_right"
    ).innerHTML;

    // attach click listeners to button
    nextButton.addEventListener("click", function () {
      getNextImg(img).click();
    });

    return nextButton;
  }

  // get previous image in document
  function getPrevImg(img) {
    const imgs = document.querySelectorAll(".lightbox_document_img");

    // find index of provided img
    let index;
    for (index = 0; index < imgs.length; index++) {
      if (imgs[index] === img) break;
    }

    // wrap index to other side if < 1
    if (index - 1 >= 0) index--;
    else index = imgs.length - 1;
    return imgs[index];
  }

  // get next image in document
  function getNextImg(img) {
    const imgs = document.querySelectorAll(".lightbox_document_img");

    // find index of provided img
    let index;
    for (index = 0; index < imgs.length; index++) {
      if (imgs[index] === img) break;
    }

    // wrap index to other side if > total
    if (index + 1 <= imgs.length - 1) index++;
    else index = 0;
    return imgs[index];
  }

  // close lightbox
  function closeLightbox() {
    focusBody();

    const lightbox = document.getElementById("lightbox_overlay");
    if (lightbox) lightbox.remove();
  }

  // make all elements behind lightbox non-focusable
  function blurBody(overlay) {
    const all = document.querySelectorAll("*");
    for (const element of all) element.tabIndex = -1;
    document.body.classList.add("body_no_scroll");
  }

  // make all elements focusable again
  function focusBody() {
    const all = document.querySelectorAll("*");
    for (const element of all) element.removeAttribute("tabIndex");
    document.body.classList.remove("body_no_scroll");
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
  <!-- modified from: https://fontawesome.com/icons/caret-left -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
    ></path>
  </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
  <!-- modified from: https://fontawesome.com/icons/caret-right -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* regular <img> in document when hovered */
    img.lightbox_document_img:hover {
      cursor: pointer;
    }

    .body_no_scroll {
      overflow: hidden !important;
    }

    /* screen overlay */
    #lightbox_overlay {
      display: flex;
      flex-direction: column;
      position: fixed;
      left: 0;
      top: 0;
      right: 0;
      bottom: 0;
      background: rgba(0, 0, 0, 0.75);
      z-index: 3;
    }

    /* middle area containing lightbox image */
    #lightbox_image_container {
      flex-grow: 1;
      display: flex;
      justify-content: center;
      align-items: center;
      overflow: hidden;
      position: relative;
      padding: 20px;
    }

    /* bottom area containing caption */
    #lightbox_bottom_container {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100px;
      min-height: 100px;
      max-height: 100px;
      background: rgba(0, 0, 0, 0.5);
    }

    /* image number info text box */
    #lightbox_number_info {
      position: absolute;
      color: #ffffff;
      font-weight: 600;
      left: 2px;
      top: 0;
      z-index: 4;
    }

    /* zoom info text box */
    #lightbox_zoom_info {
      position: absolute;
      color: #ffffff;
      font-weight: 600;
      right: 2px;
      top: 0;
      z-index: 4;
    }

    /* copy of image caption */
    #lightbox_caption {
      box-sizing: border-box;
      display: inline-block;
      width: 100%;
      max-height: 100%;
      padding: 10px 0;
      text-align: center;
      overflow-y: auto;
      color: #ffffff;
    }

    /* navigation previous/next button */
    .lightbox_button {
      width: 100px;
      height: 100%;
      min-width: 100px;
      min-height: 100%;
      color: #ffffff;
    }

    /* navigation previous/next button when hovered */
    .lightbox_button:hover {
      background: none !important;
    }

    /* navigation button icon */
    .lightbox_button > svg {
      height: 25px;
    }

    /* figure auto-number */
    #lightbox_caption > span:first-of-type {
      font-weight: bold;
      margin-right: 5px;
    }

    /* lightbox image when hovered */
    #lightbox_img:hover {
      cursor: grab;
    }

    /* lightbox image when grabbed */
    #lightbox_img:active {
      cursor: grabbing;
    }
  }

  /* when on screen < 480px wide */
  @media only screen and (max-width: 480px) {
    /* make navigation buttons skinnier on small screens to make more room for caption text */
    .lightbox_button {
      width: 50px;
      min-width: 50px;
    }
  }

  /* always hide lightbox on print */
  @media only print {
    #lightbox_overlay {
      display: none;
    }
  }
</style>
<!-- 
  Link Highlight Plugin

  Makes it such that when a user hovers or focuses a link, other links that have
  the same target will be highlighted. It also makes it such that when clicking
  a link, the target of the link (eg reference, figure, table) is briefly
  highlighted.
-->

<script type="module">
  // whether to also highlight links that go to external urls
  const externalLinks = "false";
  // whether user must click off to unhighlight instead of just
  // un-hovering
  const clickUnhighlight = "false";
  // whether to also highlight links that are unique
  const highlightUnique = "true";

  // start script
  function start() {
    const links = getLinks();
    for (const link of links) {
      // attach mouse and focus listeners to link
      link.addEventListener("mouseenter", onLinkFocus);
      link.addEventListener("focus", onLinkFocus);
      link.addEventListener("mouseleave", onLinkUnhover);
    }

    // attach click and hash change listeners to window
    window.addEventListener("click", onClick);
    window.addEventListener("touchstart", onClick);
    window.addEventListener("hashchange", onHashChange);

    // run hash change on window load in case user has navigated
    // directly to hash
    onHashChange();
  }

  // when link is focused (tabbed to) or hovered
  function onLinkFocus() {
    highlight(this);
  }

  // when link is unhovered
  function onLinkUnhover() {
    if (clickUnhighlight !== "true") unhighlightAll();
  }

  // when the mouse is clicked anywhere in window
  function onClick(event) {
    unhighlightAll();
  }

  // when hash (eg manuscript.html#introduction) changes
  function onHashChange() {
    const target = getHashTarget();
    if (target) glowElement(target);
  }

  // start glow sequence on an element
  function glowElement(element) {
    const startGlow = function () {
      onGlowEnd();
      element.dataset.glow = "true";
      element.addEventListener("animationend", onGlowEnd);
    };
    const onGlowEnd = function () {
      element.removeAttribute("data-glow");
      element.removeEventListener("animationend", onGlowEnd);
    };
    startGlow();
  }

  // highlight link and all others with same target
  function highlight(link) {
    // force unhighlight all to start fresh
    unhighlightAll();

    // get links with same target
    if (!link) return;
    const sameLinks = getSameLinks(link);

    // if link unique and option is off, exit and don't highlight
    if (sameLinks.length <= 1 && highlightUnique !== "true") return;

    // highlight all same links, and "select" (special highlight) this
    // one
    for (const sameLink of sameLinks) {
      if (sameLink === link) sameLink.setAttribute("data-selected", "true");
      else sameLink.setAttribute("data-highlighted", "true");
    }
  }

  // unhighlight all links
  function unhighlightAll() {
    const links = getLinks();
    for (const link of links) {
      link.setAttribute("data-selected", "false");
      link.setAttribute("data-highlighted", "false");
    }
  }

  // get links with same target
  function getSameLinks(link) {
    const results = [];
    const links = getLinks();
    for (const otherLink of links) {
      if (otherLink.getAttribute("href") === link.getAttribute("href"))
        results.push(otherLink);
    }
    return results;
  }

  // get all links of types we wish to handle
  function getLinks() {
    let query = "a";
    if (externalLinks !== "true") query += '[href^="#"]';
    // exclude buttons, anchor links, toc links, etc
    query += ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    return document.querySelectorAll(query);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<style>
  @media only screen {
    /* anything with data-highlighted attribute true */
    [data-highlighted="true"] {
      background: #ffeb3b;
    }

    /* anything with data-selected attribute true */
    [data-selected="true"] {
      background: #ff8a65 !important;
    }

    /* animation definition for glow */
    @keyframes highlight_glow {
      0% {
        background: none;
      }
      10% {
        background: #bbdefb;
      }
      100% {
        background: none;
      }
    }

    /* anything with data-glow attribute true */
    [data-glow="true"] {
      animation: highlight_glow 2s;
    }
  }
</style>
<!--
  Table of Contents Plugin

  Provides a "table of contents" (toc) panel on the side of the document that
  allows the user to conveniently navigate between sections of the document.
-->

<script type="module">
  // which types of elements to add links for, in "document.querySelector" format
  const typesQuery = "h1, h2, h3";
  // whether toc starts open. use 'true' or 'false', or 'auto' to
  // use 'true' behavior when screen wide enough and 'false' when not
  const startOpen = "false";
  // whether toc closes when clicking on toc link. use 'true' or
  // 'false', or 'auto' to use 'false' behavior when screen wide
  // enough and 'true' when not
  const clickClose = "auto";
  // if list item is more than this many characters, text will be
  // truncated
  const charLimit = "50";
  // whether or not to show bullets next to each toc item
  const bullets = "false";

  // start script
  function start() {
    // make toc panel and populate with entries (links to document
    // sections)
    const panel = makePanel();
    if (!panel) return;
    makeEntries(panel);
    // attach panel to document after making entries, so 'toc' heading
    // in panel isn't included in toc
    document.body.insertBefore(panel, document.body.firstChild);

    // initial panel state
    if (startOpen === "true" || (startOpen === "auto" && !isSmallScreen()))
      openPanel();
    else closePanel();

    // attach click, scroll, and hash change listeners to window
    window.addEventListener("click", onClick);
    window.addEventListener("scroll", onScroll);
    window.addEventListener("hashchange", onScroll);
    window.addEventListener("keyup", onKeyUp);
    onScroll();

    // add class to push document body down out of way of toc button
    document.body.classList.add("toc_body_nudge");
  }

  // determine if screen wide enough to fit toc panel
  function isSmallScreen() {
    // in default theme:
    // 816px = 8.5in = width of "page" (<body>) element
    // 260px = min width of toc panel (*2 for both sides of <body>)
    return window.innerWidth < 816 + 260 * 2;
  }

  // when mouse is clicked anywhere in window
  function onClick() {
    if (isSmallScreen()) closePanel();
  }

  // when window is scrolled or hash changed
  function onScroll() {
    highlightViewed();
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    // close on esc
    if (event.key === "Escape") closePanel();
  }

  // find entry of currently viewed document section in toc and highlight
  function highlightViewed() {
    const firstId = getFirstInView(typesQuery);

    // get toc entries (links), unhighlight all, then highlight viewed
    const list = document.getElementById("toc_list");
    if (!firstId || !list) return;
    const links = list.querySelectorAll("a");
    for (const link of links) link.dataset.viewing = "false";
    const link = list.querySelector('a[href="#' + firstId + '"]');
    if (!link) return;
    link.dataset.viewing = "true";
  }

  // get first or previous toc listed element in top half of view
  function getFirstInView(query) {
    // get all elements matching query and with id
    const elements = document.querySelectorAll(query);
    const elementsWithIds = [];
    for (const element of elements) {
      if (element.id) elementsWithIds.push(element);
    }

    // get first or previous element in top half of view
    for (let i = 0; i < elementsWithIds.length; i++) {
      const element = elementsWithIds[i];
      const prevElement = elementsWithIds[Math.max(0, i - 1)];
      if (element.getBoundingClientRect().top >= 0) {
        if (element.getBoundingClientRect().top < window.innerHeight / 2)
          return element.id;
        else return prevElement.id;
      }
    }
  }

  // make panel
  function makePanel() {
    // create panel
    const panel = document.createElement("div");
    panel.id = "toc_panel";
    if (bullets === "true") panel.dataset.bullets = "true";

    // create header
    const header = document.createElement("div");
    header.id = "toc_header";

    // create toc button
    const button = document.createElement("button");
    button.id = "toc_button";
    button.innerHTML = document.querySelector(".icon_th_list").innerHTML;
    button.title = "Table of Contents";
    button.classList.add("icon_button");

    // create header text
    const text = document.createElement("h4");
    text.innerHTML = "Table of Contents";

    // create container for toc list
    const list = document.createElement("div");
    list.id = "toc_list";

    // attach click listeners
    panel.addEventListener("click", onPanelClick);
    header.addEventListener("click", onHeaderClick);
    button.addEventListener("click", onButtonClick);

    // attach elements
    header.appendChild(button);
    header.appendChild(text);
    panel.appendChild(header);
    panel.appendChild(list);

    return panel;
  }

  // create toc entries (links) to each element of the specified types
  function makeEntries(panel) {
    const elements = document.querySelectorAll(typesQuery);
    for (const element of elements) {
      // do not add link if element doesn't have assigned id
      if (!element.id) continue;

      // create link/list item
      const link = document.createElement("a");
      link.classList.add("toc_link");
      switch (element.tagName.toLowerCase()) {
        case "h1":
          link.dataset.level = "1";
          break;
        case "h2":
          link.dataset.level = "2";
          break;
        case "h3":
          link.dataset.level = "3";
          break;
        case "h4":
          link.dataset.level = "4";
          break;
      }
      link.title = element.innerText;
      let text = element.innerText;
      if (text.length > charLimit) text = text.slice(0, charLimit) + "...";
      link.innerHTML = text;
      link.href = "#" + element.id;
      link.addEventListener("click", onLinkClick);

      // attach link
      panel.querySelector("#toc_list").appendChild(link);
    }
  }

  // when panel is clicked
  function onPanelClick(event) {
    // stop click from propagating to window/document and closing panel
    event.stopPropagation();
  }

  // when header itself is clicked
  function onHeaderClick(event) {
    togglePanel();
  }

  // when button is clicked
  function onButtonClick(event) {
    togglePanel();
    // stop header underneath button from also being clicked
    event.stopPropagation();
  }

  // when link is clicked
  function onLinkClick(event) {
    if (clickClose === "true" || (clickClose === "auto" && isSmallScreen()))
      closePanel();
    else openPanel();
  }

  // open panel if closed, close if opened
  function togglePanel() {
    const panel = document.getElementById("toc_panel");
    if (!panel) return;

    if (panel.dataset.open === "true") closePanel();
    else openPanel();
  }

  // open panel
  function openPanel() {
    const panel = document.getElementById("toc_panel");
    if (panel) panel.dataset.open = "true";
  }

  // close panel
  function closePanel() {
    const panel = document.getElementById("toc_panel");
    if (panel) panel.dataset.open = "false";
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- th list icon -->

<template class="icon_th_list">
  <!-- modified from: https://fontawesome.com/icons/th-list -->
  <svg width="16" height="16" viewBox="0 0 512 512" tabindex="-1">
    <path
      fill="currentColor"
      d="M96 96c0 26.51-21.49 48-48 48S0 122.51 0 96s21.49-48 48-48 48 21.49 48 48zM48 208c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm0 160c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm96-236h352c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"
      tabindex="-1"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* toc panel */
    #toc_panel {
      box-sizing: border-box;
      position: fixed;
      top: 0;
      left: 0;
      background: #ffffff;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      z-index: 2;
    }

    /* toc panel when closed */
    #toc_panel[data-open="false"] {
      min-width: 60px;
      width: 60px;
      height: 60px;
      border-right: solid 1px #bdbdbd;
      border-bottom: solid 1px #bdbdbd;
    }

    /* toc panel when open */
    #toc_panel[data-open="true"] {
      min-width: 260px;
      max-width: 480px;
      /* keep panel edge consistent distance away from "page" edge */
      width: calc(((100vw - 8.5in) / 2) - 30px - 40px);
      bottom: 0;
      border-right: solid 1px #bdbdbd;
    }

    /* toc panel header */
    #toc_header {
      box-sizing: border-box;
      display: flex;
      flex-direction: row;
      align-items: center;
      height: 60px;
      margin: 0;
      padding: 20px;
    }

    /* toc panel header when hovered */
    #toc_header:hover {
      cursor: pointer;
    }

    /* toc panel header when panel open */
    #toc_panel[data-open="true"] > #toc_header {
      border-bottom: solid 1px #bdbdbd;
    }

    /* toc open/close header button */
    #toc_button {
      margin-right: 20px;
    }

    /* hide toc list and header text when closed */
    #toc_panel[data-open="false"] > #toc_header > *:not(#toc_button),
    #toc_panel[data-open="false"] > #toc_list {
      display: none;
    }

    /* toc list of entries */
    #toc_list {
      box-sizing: border-box;
      width: 100%;
      padding: 20px;
      position: absolute;
      top: calc(60px + 1px);
      bottom: 0;
      overflow: auto;
    }

    /* toc entry, link to section in document */
    .toc_link {
      display: block;
      padding: 5px;
      position: relative;
      font-weight: 600;
      text-decoration: none;
    }

    /* toc entry when hovered or when "viewed" */
    .toc_link:hover,
    .toc_link[data-viewing="true"] {
      background: #f5f5f5;
    }

    /* toc entry, level 1 indentation */
    .toc_link[data-level="1"] {
      margin-left: 0;
    }

    /* toc entry, level 2 indentation */
    .toc_link[data-level="2"] {
      margin-left: 20px;
    }

    /* toc entry, level 3 indentation */
    .toc_link[data-level="3"] {
      margin-left: 40px;
    }

    /* toc entry, level 4 indentation */
    .toc_link[data-level="4"] {
      margin-left: 60px;
    }

    /* toc entry bullets */
    #toc_panel[data-bullets="true"] .toc_link[data-level]:before {
      position: absolute;
      left: -15px;
      top: -1px;
      font-size: 1.5em;
    }

    /* toc entry, level 2 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="2"]:before {
      content: "\2022";
    }

    /* toc entry, level 3 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="3"]:before {
      content: "\25AB";
    }

    /* toc entry, level 4 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="4"]:before {
      content: "-";
    }
  }

  /* when on screen < 8.5in wide */
  @media only screen and (max-width: 8.5in) {
    /* push <body> ("page") element down to make room for toc icon */
    .toc_body_nudge {
      padding-top: 60px;
    }

    /* toc icon when panel closed and not hovered */
    #toc_panel[data-open="false"]:not(:hover) {
      background: rgba(255, 255, 255, 0.75);
    }
  }

  /* always hide toc panel on print */
  @media only print {
    #toc_panel {
      display: none;
    }
  }
</style>
<!-- 
  Tooltips Plugin

  Makes it such that when the user hovers or focuses a link to a citation or
  figure, a tooltip appears with a preview of the reference content, along with
  arrows to navigate between instances of the same reference in the document.
-->

<script type="module">
  // whether user must click off to close tooltip instead of just un-hovering
  const clickClose = "false";
  // delay (in ms) between opening and closing tooltip
  const delay = "100";

  // start script
  function start() {
    const links = getLinks();
    for (const link of links) {
      // attach hover and focus listeners to link
      link.addEventListener("mouseover", onLinkHover);
      link.addEventListener("mouseleave", onLinkUnhover);
      link.addEventListener("focus", onLinkFocus);
      link.addEventListener("touchend", onLinkTouch);
    }

    // attach mouse, key, and resize listeners to window
    window.addEventListener("mousedown", onClick);
    window.addEventListener("touchstart", onClick);
    window.addEventListener("keyup", onKeyUp);
    window.addEventListener("resize", onResize);
  }

  // when link is hovered
  function onLinkHover() {
    // function to open tooltip
    const delayOpenTooltip = function () {
      openTooltip(this);
    }.bind(this);

    // run open function after delay
    this.openTooltipTimer = window.setTimeout(delayOpenTooltip, delay);
  }

  // when mouse leaves link
  function onLinkUnhover() {
    // cancel opening tooltip
    window.clearTimeout(this.openTooltipTimer);

    // don't close on unhover if option specifies
    if (clickClose === "true") return;

    // function to close tooltip
    const delayCloseTooltip = function () {
      // if tooltip open and if mouse isn't over tooltip, close
      const tooltip = document.getElementById("tooltip");
      if (tooltip && !tooltip.matches(":hover")) closeTooltip();
    };

    // run close function after delay
    this.closeTooltipTimer = window.setTimeout(delayCloseTooltip, delay);
  }

  // when link is focused (tabbed to)
  function onLinkFocus(event) {
    openTooltip(this);
  }

  // when link is touched on touch screen
  function onLinkTouch(event) {
    // attempt to force hover state on first tap always, and trigger
    // regular link click (and navigation) on second tap
    if (event.target === document.activeElement) event.target.click();
    else {
      document.activeElement.blur();
      event.target.focus();
    }
    if (event.cancelable) event.preventDefault();
    event.stopPropagation();
    return false;
  }

  // when mouse is clicked anywhere in window
  function onClick(event) {
    closeTooltip();
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    switch (event.key) {
      // trigger click of prev button
      case "ArrowLeft":
        const prevButton = document.getElementById("tooltip_prev_button");
        if (prevButton) prevButton.click();
        break;
      // trigger click of next button
      case "ArrowRight":
        const nextButton = document.getElementById("tooltip_next_button");
        if (nextButton) nextButton.click();
        break;
      // close on esc
      case "Escape":
        closeTooltip();
        break;
    }
  }

  // when window is resized or zoomed
  function onResize() {
    closeTooltip();
  }

  // get all links of types we wish to handle
  function getLinks() {
    const queries = [];
    // exclude buttons, anchor links, toc links, etc
    const exclude =
      ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    queries.push('a[href^="#ref-"]' + exclude); // citation links
    queries.push('a[href^="#fig:"]' + exclude); // figure links
    const query = queries.join(", ");
    return document.querySelectorAll(query);
  }

  // get links with same target, get index of link in set, get total
  // same links
  function getSameLinks(link) {
    const sameLinks = [];
    const links = getLinks();
    for (const otherLink of links) {
      if (otherLink.getAttribute("href") === link.getAttribute("href"))
        sameLinks.push(otherLink);
    }

    return {
      elements: sameLinks,
      index: sameLinks.indexOf(link),
      total: sameLinks.length,
    };
  }

  // open tooltip
  function openTooltip(link) {
    // delete tooltip if it exists, start fresh
    closeTooltip();

    // make tooltip element
    const tooltip = makeTooltip(link);

    // if source couldn't be found and tooltip not made, exit
    if (!tooltip) return;

    // make navbar elements
    const navBar = makeNavBar(link);
    if (navBar) tooltip.firstElementChild.appendChild(navBar);

    // attach tooltip to page
    document.body.appendChild(tooltip);

    // position tooltip
    const position = function () {
      positionTooltip(link);
    };
    position();

    // if tooltip contains images, position again after they've loaded
    const imgs = tooltip.querySelectorAll("img");
    for (const img of imgs) img.addEventListener("load", position);
  }

  // close (delete) tooltip
  function closeTooltip() {
    const tooltip = document.getElementById("tooltip");
    if (tooltip) tooltip.remove();
  }

  // make tooltip
  function makeTooltip(link) {
    // get target element that link points to
    const source = getSource(link);

    // if source can't be found, exit
    if (!source) return;

    // create new tooltip
    const tooltip = document.createElement("div");
    tooltip.id = "tooltip";
    const tooltipContent = document.createElement("div");
    tooltipContent.id = "tooltip_content";
    tooltip.appendChild(tooltipContent);

    // make copy of source node and put in tooltip
    const sourceCopy = makeCopy(source);
    tooltipContent.appendChild(sourceCopy);

    // attach mouse event listeners
    tooltip.addEventListener("click", onTooltipClick);
    tooltip.addEventListener("mousedown", onTooltipClick);
    tooltip.addEventListener("touchstart", onTooltipClick);
    tooltip.addEventListener("mouseleave", onTooltipUnhover);

    // (for interaction with lightbox plugin)
    // transfer click on tooltip copied img to original img
    const sourceImg = source.querySelector("img");
    const sourceCopyImg = sourceCopy.querySelector("img");
    if (sourceImg && sourceCopyImg) {
      const clickImg = function () {
        sourceImg.click();
        closeTooltip();
      };
      sourceCopyImg.addEventListener("click", clickImg);
    }

    return tooltip;
  }

  // make carbon copy of html dom element
  function makeCopy(source) {
    const sourceCopy = source.cloneNode(true);

    // delete elements marked with ignore (eg anchor and jump buttons)
    const deleteFromCopy = sourceCopy.querySelectorAll('[data-ignore="true"]');
    for (const element of deleteFromCopy) element.remove();

    // delete certain element attributes
    const attributes = [
      "id",
      "data-collapsed",
      "data-selected",
      "data-highlighted",
      "data-glow",
      "class",
    ];
    for (const attribute of attributes) {
      sourceCopy.removeAttribute(attribute);
      const elements = sourceCopy.querySelectorAll("[" + attribute + "]");
      for (const element of elements) element.removeAttribute(attribute);
    }

    return sourceCopy;
  }

  // when tooltip is clicked
  function onTooltipClick(event) {
    // when user clicks on tooltip, stop click from transferring
    // outside of tooltip (eg, click off to close tooltip, or eg click
    // off to unhighlight same refs)
    event.stopPropagation();
  }

  // when tooltip is unhovered
  function onTooltipUnhover(event) {
    if (clickClose === "true") return;

    // make sure new mouse/touch/focus no longer over tooltip or any
    // element within it
    const tooltip = document.getElementById("tooltip");
    if (!tooltip) return;
    if (this.contains(event.relatedTarget)) return;

    closeTooltip();
  }

  // make nav bar to go betwen prev/next instances of same reference
  function makeNavBar(link) {
    // find other links to the same source
    const sameLinks = getSameLinks(link);

    // don't show nav bar when singular reference
    if (sameLinks.total <= 1) return;

    // find prev/next links with same target
    const prevLink = getPrevLink(link, sameLinks);
    const nextLink = getNextLink(link, sameLinks);

    // create nav bar
    const navBar = document.createElement("div");
    navBar.id = "tooltip_nav_bar";
    const text = sameLinks.index + 1 + " of " + sameLinks.total;

    // create nav bar prev/next buttons
    const prevButton = document.createElement("button");
    const nextButton = document.createElement("button");
    prevButton.id = "tooltip_prev_button";
    nextButton.id = "tooltip_next_button";
    prevButton.title =
      "Jump to the previous occurence of this item in the document [←]";
    nextButton.title =
      "Jump to the next occurence of this item in the document [→]";
    prevButton.classList.add("icon_button");
    nextButton.classList.add("icon_button");
    prevButton.innerHTML = document.querySelector(".icon_caret_left").innerHTML;
    nextButton.innerHTML =
      document.querySelector(".icon_caret_right").innerHTML;
    navBar.appendChild(prevButton);
    navBar.appendChild(document.createTextNode(text));
    navBar.appendChild(nextButton);

    // attach click listeners to buttons
    prevButton.addEventListener("click", function () {
      onPrevNextClick(link, prevLink);
    });
    nextButton.addEventListener("click", function () {
      onPrevNextClick(link, nextLink);
    });

    return navBar;
  }

  // get previous link with same target
  function getPrevLink(link, sameLinks) {
    if (!sameLinks) sameLinks = getSameLinks(link);
    // wrap index to other side if < 1
    let index;
    if (sameLinks.index - 1 >= 0) index = sameLinks.index - 1;
    else index = sameLinks.total - 1;
    return sameLinks.elements[index];
  }

  // get next link with same target
  function getNextLink(link, sameLinks) {
    if (!sameLinks) sameLinks = getSameLinks(link);
    // wrap index to other side if > total
    let index;
    if (sameLinks.index + 1 <= sameLinks.total - 1) index = sameLinks.index + 1;
    else index = 0;
    return sameLinks.elements[index];
  }

  // get element that is target of link or url hash
  function getSource(link) {
    const hash = link ? link.hash : window.location.hash;
    const id = hash.slice(1);
    let target = document.querySelector('[id="' + id + '"]');
    if (!target) return;

    // if ref or figure, modify target to get expected element
    if (id.indexOf("ref-") === 0) target = target.querySelector(":nth-child(2)");
    else if (id.indexOf("fig:") === 0) target = target.querySelector("figure");

    return target;
  }

  // when prev/next arrow button is clicked
  function onPrevNextClick(link, prevNextLink) {
    if (link && prevNextLink)
      goToElement(prevNextLink, window.innerHeight * 0.5);
  }

  // scroll to and focus element
  function goToElement(element, offset) {
    // expand accordion section if collapsed
    expandElement(element);
    const y =
      getRectInView(element).top -
      getRectInView(document.documentElement).top -
      (offset || 0);
    // trigger any function listening for "onscroll" event
    window.dispatchEvent(new Event("scroll"));
    window.scrollTo(0, y);
    document.activeElement.blur();
    element.focus();
  }

  // determine position to place tooltip based on link position in
  // viewport and tooltip size
  function positionTooltip(link, left, top) {
    const tooltipElement = document.getElementById("tooltip");
    if (!tooltipElement) return;

    // get convenient vars for position/dimensions of
    // link/tooltip/page/view
    link = getRectInPage(link);
    const tooltip = getRectInPage(tooltipElement);
    const view = getRectInPage();

    // horizontal positioning
    if (left)
      // use explicit value
      left = left;
    else if (link.left + tooltip.width < view.right)
      // fit tooltip to right of link
      left = link.left;
    else if (link.right - tooltip.width > view.left)
      // fit tooltip to left of link
      left = link.right - tooltip.width;
    // center tooltip in view
    else left = (view.right - view.left) / 2 - tooltip.width / 2;

    // vertical positioning
    if (top)
      // use explicit value
      top = top;
    else if (link.top - tooltip.height > view.top)
      // fit tooltip above link
      top = link.top - tooltip.height;
    else if (link.bottom + tooltip.height < view.bottom)
      // fit tooltip below link
      top = link.bottom;
    else {
      // center tooltip in view
      top = view.top + view.height / 2 - tooltip.height / 2;
      // nudge off of link to left/right if possible
      if (link.right + tooltip.width < view.right) left = link.right;
      else if (link.left - tooltip.width > view.left)
        left = link.left - tooltip.width;
    }

    tooltipElement.style.left = left + "px";
    tooltipElement.style.top = top + "px";
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
  <!-- modified from: https://fontawesome.com/icons/caret-left -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
    ></path>
  </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
  <!-- modified from: https://fontawesome.com/icons/caret-right -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* tooltip container */
    #tooltip {
      position: absolute;
      width: 50%;
      min-width: 240px;
      max-width: 75%;
      z-index: 1;
    }

    /* tooltip content */
    #tooltip_content {
      margin-bottom: 5px;
      padding: 20px;
      border-radius: 5px;
      border: solid 1px #bdbdbd;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      background: #ffffff;
      overflow-wrap: break-word;
    }

    /* tooltip copy of paragraphs and figures */
    #tooltip_content > p,
    #tooltip_content > figure {
      margin: 0;
      max-height: 320px;
      overflow-y: auto;
    }

    /* tooltip copy of <img> */
    #tooltip_content > figure > img,
    #tooltip_content > figure > svg {
      max-height: 260px;
    }

    /* navigation bar */
    #tooltip_nav_bar {
      margin-top: 10px;
      text-align: center;
    }

    /* navigation bar previous/next buton */
    #tooltip_nav_bar > .icon_button {
      position: relative;
      top: 3px;
    }

    /* navigation bar previous button */
    #tooltip_nav_bar > .icon_button:first-of-type {
      margin-right: 5px;
    }

    /* navigation bar next button */
    #tooltip_nav_bar > .icon_button:last-of-type {
      margin-left: 5px;
    }
  }

  /* always hide tooltip on print */
  @media only print {
    #tooltip {
      display: none;
    }
  }
</style>
<!--
  Analytics Plugin (third-party) 
  
  Copy and paste code from Google Analytics or similar service here.
-->
<!-- 
  Annotations Plugin

  Allows public annotation of the  manuscript. See https://web.hypothes.is/.
-->

<script type="module">
  // configuration
  window.hypothesisConfig = function () {
    return {
      branding: {
        accentColor: "#2196f3",
        appBackgroundColor: "#f8f8f8",
        ctaBackgroundColor: "#f8f8f8",
        ctaTextColor: "#000000",
        selectionFontFamily: "Open Sans, Helvetica, sans serif",
        annotationFontFamily: "Open Sans, Helvetica, sans serif",
      },
    };
  };

  // hypothesis client script
  const embed = "https://hypothes.is/embed.js";
  // hypothesis annotation count query url
  const query = "https://api.hypothes.is/api/search?limit=0&url=";

  // start script
  function start() {
    const button = makeButton();
    document.body.insertBefore(button, document.body.firstChild);
    insertCount(button);
  }

  // make button
  function makeButton() {
    // create button
    const button = document.createElement("button");
    button.id = "hypothesis_button";
    button.innerHTML = document.querySelector(".icon_hypothesis").innerHTML;
    button.title = "Hypothesis annotations";
    button.classList.add("icon_button");

    function onClick(event) {
      onButtonClick(event, button);
    }

    // attach click listeners
    button.addEventListener("click", onClick);

    return button;
  }

  // insert annotations count
  async function insertCount(button) {
    // get annotation count from Hypothesis based on url
    let count = "-";
    try {
      const canonical = document.querySelector('link[rel="canonical"]');
      const location = window.location;
      const url = encodeURIComponent((canonical || location).href);
      const response = await fetch(query + url);
      const json = await response.json();
      count = json.total || "-";
    } catch (error) {
      console.log(error);
    }

    // put count into button
    const counter = document.createElement("span");
    counter.id = "hypothesis_count";
    counter.innerHTML = count;
    button.title = "View " + count + " Hypothesis annotations";
    button.append(counter);
  }

  // when button is clicked
  function onButtonClick(event, button) {
    const script = document.createElement("script");
    script.src = embed;
    document.body.append(script);
    button.remove();
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- hypothesis icon -->

<template class="icon_hypothesis">
  <!-- modified from: https://simpleicons.org/icons/hypothesis.svg / https://git.io/Jf1VB -->
  <svg width="16" height="16" viewBox="0 0 24 24" tabindex="-1">
    <path
      fill="currentColor"
      d="M3.43 0C2.5 0 1.72 .768 1.72 1.72V18.86C1.72 19.8 2.5 20.57 3.43 20.57H9.38L12 24L14.62 20.57H20.57C21.5 20.57 22.29 19.8 22.29 18.86V1.72C22.29 .77 21.5 0 20.57 0H3.43M5.14 3.43H7.72V9.43S8.58 7.72 10.28 7.72C12 7.72 13.74 8.57 13.74 11.24V17.14H11.16V12C11.16 10.61 10.28 10.07 9.43 10.29C8.57 10.5 7.72 11.41 7.72 13.29V17.14H5.14V3.43M18 13.72C18.95 13.72 19.72 14.5 19.72 15.42A1.71 1.71 0 0 1 18 17.13A1.71 1.71 0 0 1 16.29 15.42C16.29 14.5 17.05 13.71 18 13.71Z"
      tabindex="-1"
    ></path>
  </svg>
</template>

<style>
  /* hypothesis activation button */
  #hypothesis_button {
    box-sizing: border-box;
    position: fixed;
    top: 0;
    right: 0;
    width: 60px;
    height: 60px;
    background: #ffffff;
    border-radius: 0;
    border-left: solid 1px #bdbdbd;
    border-bottom: solid 1px #bdbdbd;
    box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
    z-index: 2;
  }

  /* hypothesis button svg */
  #hypothesis_button > svg {
    position: relative;
    top: -4px;
  }

  /* hypothesis annotation count */
  #hypothesis_count {
    position: absolute;
    left: 0;
    right: 0;
    bottom: 5px;
  }

  /* side panel */
  .annotator-frame {
    width: 280px !important;
  }

  /* match highlight color to rest of theme */
  .annotator-highlights-always-on .annotator-hl {
    background-color: #ffeb3b !important;
  }

  /* match focused color to rest of theme */
  .annotator-hl.annotator-hl-focused {
    background-color: #ff8a65 !important;
  }

  /* match bucket bar color to rest of theme */
  .annotator-bucket-bar {
    background: #f5f5f5 !important;
  }

  /* always hide button, toolbar, and tooltip on print */
  @media only print {
    #hypothesis_button {
      display: none;
    }

    .annotator-frame {
      display: none !important;
    }

    hypothesis-adder {
      display: none !important;
    }
  }
</style>
<!-- 
  Mathjax Plugin (third-party) 

  Allows the proper rendering of math/equations written in LaTeX.
  See https://www.mathjax.org/.
-->

<script type="text/x-mathjax-config">
  // configuration
  MathJax.Hub.Config({
    "CommonHTML": { linebreaks: { automatic: true } },
    "HTML-CSS": { linebreaks: { automatic: true } },
    "SVG": { linebreaks: { automatic: true } },
    "fast-preview": { disabled: true }
  });
</script>

<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A=="
  crossorigin="anonymous"
></script>

<style>
  /* mathjax containers */
  .math.display > span:not(.MathJax_Preview) {
    /* turn inline element (no dimensions) into block (allows fixed width and thus scrolling) */
    display: flex !important;
    overflow-x: auto !important;
    overflow-y: hidden !important;
    justify-content: center;
    align-items: center;
    margin: 0 !important;
  }

  /* right click menu */
  .MathJax_Menu {
    border-radius: 5px !important;
    border: solid 1px #bdbdbd !important;
    box-shadow: none !important;
  }

  /* equation auto-number */
  span[id^="eq:"] > span.math.display + span {
    font-weight: 600;
  }

  /* equation */
  span[id^="eq:"] > span.math.display > span {
    /* nudge to make room for equation auto-number and anchor */
    margin-right: 60px !important;
  }
</style>
</body>
</html>
