## Causality in foundation models

There has been an enormous spike of interest in attention-based neural network models, in large part due to the success of Large Language Models (LLMs, [Glossary][Large Language Models]).
While the high performance of LLMs is based on myriad technical improvements, the introduction of attention ([Glossary][Attention (deep learning)]) as an architectural bias ([Glossary][Bias (machine learning)]) has been a major contributor to their success [@doi:10.48550/arXiv.1706.03762].
This has led to the development of attention-based molecular models (most commonly for gene expression), which can also be considered “GPT” models: Generative Pre-trained Transformers [@doi:10.1038/s41592-021-01252-x;@doi:10.1038/s41586-023-06139-9;@doi:10.1101/2023.04.30.538439].
Attention as a learning mechanism enables the integration of non-local information in a flexible manner.
In a molecular model that reasons about gene expression, such as Geneformer, attention allows the integration of distant regulatory elements [@doi:10.1038/s41586-023-06139-9].
Notably, this mechanism comes with a computational cost that increases exponentially with respect to the length of the input sequence [@doi:10.48550/arXiv.2310.05869].

The generalist capabilities of LLMs have led to the designation of “foundation models” [@{https://crfm.stanford.edu/}].
Foundation models ([Glossary][Foundation model]) are models that achieve high performance by training a generic architecture on extremely large amounts of data in a self-supervised manner ([Glossary][Self-supervised learning]).
They can be fine-tuned for more specific tasks, because they are thought to derive generalisable representations and mechanisms by training on an amount of data large enough to learn the complexity of real-world systems.
However, recent molecular foundation model benchmarks highlight clear discrepancies between the “foundational” aspirations of the pre-trained models and the real-world evaluation of their performance [@doi:10.1101/2023.10.16.561085;@doi:10.1101/2023.10.19.563100].
Briefly, the benchmarks found that, on single cell classification tasks, the proposed foundation models did not outperform simple baselines consistently.
State-of-the-art methods such as scVI [@doi:10.1038/s41592-018-0229-2] and even the mere selection of highly variable genes was often statistically indistinguishable from the highly parameterised methods, and sometimes even yielded better classification outcomes.
However, these are early models, and it could still be argued that, in line with the scaling hypothesis ([Glossary][Scaling hypothesis]), models may improve via a combination of the right architecture with sufficient amounts of data [@doi:10.48550/arXiv.2401.04720].

Indeed, molecular foundation models lag behind in size: while current-generation LLMs have around 100 billion parameters or more and are trained on enormous text corpuses (hundreds of billions to trillions of tokens), molecular foundation models have tens of millions of parameters (scGPT: 53M, Geneformer: 10M) and are trained on corpuses of tens of millions of cells, which (optimistically) yields hundreds of billions of individual data points.
Thus, LLMs are currently about 2000 times larger than molecular foundation models, while arguably also dealing with a less complicated system.
The question whether scaling will lead to the emergence of “foundational behaviour” in molecular models is still a matter of much debate [@emergent-abilities].

### Attention - and large amounts of data - is all you need?

Given enough data to train on - and ample funds for compute - is attention “all you need” to induce reliable biases in your model?
While there are doubts regarding the reasoning capabilities of LLMs, GPT arguably “understands” language very well already, to the point where it can flawlessly communicate and synthesise information [@doi:10.1038/d41586-023-02361-7].
This is what the term “foundation model” implies: the model has derived a generalisable representation of language, a tool that can be fine-tuned for a variety of language-related tasks.
This behaviour is not possible without assuming some form of causality, even if it is not explicitly encoded in the model [@doi:10.48550/arXiv.2206.10591].

In this light, what are the reasons to be sceptical about the capacity of molecular foundation models to understand the “grammar” of the cell?

**Explainability**: For one, large transformer models (i.e., billions of parameters) are not explainable due to their high complexity.
As such, there is often no way to scrutinise their reasoning beyond the output they produce [@doi:10.48550/arXiv.2108.07258;@doi:10.3390/diagnostics12071557].
What seems simple in the case of language models - the famous Turing test can be performed by any human with a basic understanding of language - is exceedingly difficult in the molecular space, where many causal relationships are still unknown [@doi:10.1038/d41586-023-02361-7].
Yet the only way to scrutinise and subsequently improve the reasoning capabilities of a model is precisely this explicit validation of its predictions in an interpretable setting.

While the creation of explicit molecular models (e.g., logic, structural causal, or ODE-based models) and the self-supervised training of molecular foundation models are methodically very different, both can provide a hypothesis on causal structure that can be formulated as a network.
Theodoris et al. explore the attention layers of their Geneformer foundation model to explain the model’s reasoning [@doi:10.1038/s41586-023-06139-9].
While some layers show clear patterns of attention, such as attending to highly connected or highly expressed genes, other layers are not as readily interpretable, much less so than explicit molecular models.

**Benchmarking**: Whether these complex layers reflect the true complexity of the underlying biology or are rather evidence for overfitting ([Glossary][Overfitting]) to the training data is not clear.
One argument in favour of overfitting is the poor generalisation of the model in independent benchmarks [@doi:10.1101/2023.10.16.561085; @doi:10.1101/2023.10.19.563100].
To determine whether molecular foundation models indeed capture generalisable causal representations of biology, dedicated benchmarks are needed.

**Causal bias**: The GPT-3 architecture that led to the recent breakthrough in LLM capabilities employs “causal self-attention,” describing an implicit architectural bias that prevents the model from “looking into the future”: for predicting the next token, only the previous tokens in the sentence can be used [@doi:10.48550/arXiv.2310.05869].
This leverages the implicit causality present in language, which incidentally is similar to one of the earliest formal descriptions of causality, that “the effect has regularly followed the cause in the past” [@isbn:9780199211586].
Compared to language, the data that form the input of molecular foundation models do not implicitly contain causal information.
The individual cells are in general not on a known trajectory, and the genes that are masked as part of the training objective are masked at random, not because they are downstream (in some form) of the genes used for prediction.
This fundamental difference between language and molecular models has so far not been explored theoretically or empirically.

### Causal latent spaces

Due to the fundamental limitation of human perception, dimensionality reduction is a popular workflow for data interpretation, typically via methods such as PCA, t-SNE, or UMAP [@doi:10.4236/jdaip.2021.93013].
The hope is that exploration and explanation in the lower-dimensional embedding space may be less challenging than in the original data, which assumes that the most important aspects of variability in the original data are captured in the reduced dimensions [@doi:10.1073/pnas.2319169120].
However, without explicit supervision, which is rare in typical biomedical datasets, the resulting latent spaces are rarely interpretable, and do not lend themselves to causal interpretation.
In addition, they often suffer from biases that result from technical rather than biological factors [@doi:10.1371/journal.pcbi.1011288].
In consequence, biological insight during the exploration of these latent spaces is often challenging due to the dominance of biases over the biological generative mechanism.

Performing causal inference in latent spaces could potentially solve some of these issues.
“Moving through the latent space” reduces the number of variables that change upon intervention, making exploration simpler in theory.
In practice, however, ease and sensibility of exploration depend completely on whether the inductive biases in the embedding process capture the underlying biology.
In addition, latent spaces have no trivial connection to the real-world measurements they are based on.
Each model instance generates its own, independent latent space; in consequence, the exploration of latent spaces is challenging and time-consuming.

Even if a given latent space can be explored, there is often no guarantee that interpolation between sensible latent representations also leads to sensible results.
As an example, consider a prevailing issue of visual generative models in drawing human hands: images of hands typically involve mangled anatomy and an incorrect number of digits [@{https://www.newyorker.com/culture/rabbit-holes/the-uncanny-failures-of-ai-generated-hands}].
Even though there is a section in the latent space that represents hands, this does not represent the concept of a hand, but rather is guided by learning on many diverse pictures of hands.
A section of this latent space may represent only a finger, and carry some information that next to a finger there usually is another finger.
However, when generating the image, there is no mechanism to keep track of how many digits to add to any generated hand, leading to wrong anatomy.
Similarly, when exploring the latent space of a model of molecular signalling, there may be no guarantees that the model respects the concept of a given pathway when generating the signalling molecules involved.

If mastered, exploring and performing interventions in latent spaces promises many benefits: better generalisation and improved sample efficiency [@doi:10.1109/JPROC.2021.3058954], predicting the outcomes of interventions not observed at training time [@doi:10.48550/arXiv.2310.04295], or insights into the effect of different inductive biases in the model [@doi:10.48550/arXiv.2107.00793].
However, to achieve this, gaining a better understanding of properties of the learned embeddings and variables is essential, for instance by performing “imagined interventions” in the latent space [@doi:10.48550/arXiv.2106.16091] or by using model uncertainty for guiding the optimisation process in the latent space [@doi:10.48550/arXiv.2107.00096].
Of note, many of the proposed solutions for more explainable latent spaces depend on architectures that may scale significantly worse than transformers [@doi:10.48550/arXiv.2001.08361;@doi:10.48550/arXiv.2202.05808].
