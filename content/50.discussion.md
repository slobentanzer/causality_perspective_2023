## Discussion / Conclusion

### Dichotomy of scaling (data-driven) and bias injection (knowledge-driven)

The debate between adopting scaling strategies ([Glossary][Scaling hypothesis]) versus the injection of biases ([Glossary][Bias (machine learning)]) from PK ([Glossary][Prior knowledge]) highlights a fundamental tension in modern biomedical research.
The "Bitter Lesson" suggests a preference for general-purpose learning algorithms that scale with computational resources, implicitly learning biases from data.
However, complex models often pose significant computational challenges; many models are limited to network sizes unfeasibly small for biological inference, and feedback loops are often excluded.
Conversely, explicitly injecting biases from PK can lead to more specialised and efficient models that can generalise using relatively little training data, but may not scale.
Hybrid models represent a promising middle ground, combining the scalability of generalist models with the efficiency and specificity provided by tailored biases.
Researchers often rely on intuition to determine which biases to inject, understanding that while no single model may universally excel (reflecting the "No Free Lunch" theorems, [Glossary]['No Free Lunch' Theorems]), the blend of generalisation through scaling and specialisation through bias injection might provide a robust framework for tackling complex biomedical challenges.

### Theoretical foundations: interventions and inductive biases

Theoretical work emphasises the need for interventions in causal discovery but does not yet address the influence of inductive biases [@doi:10.48550/arXiv.1207.1389].
The number of required interventions might be reduced significantly when complemented with high-quality observational data and appropriate biases, as suggested by neural causal models [@doi:10.48550/arXiv.1910.01075].
However, the precise nature of these biases and their impact remains understudied theoretically as well as empirically.
The comparative effectiveness and theoretical underpinnings of explicit models versus implicit models are particularly understudied.
Foundation models ([Glossary][Foundation model]) have embraced causal self-attention as a step towards integrating causality, but this alone may be insufficient.
Empirical studies and more robust theories are needed to understand these dynamics, including the potential existence and validation of causal latent spaces.

### Data types: observational vs. interventional

The choice between training on observational versus interventional data (or a mixture of both) is critical in the development of models.
While large-scale data collection is vital, the type of data collected can significantly influence model performance and the ability to generalise and make accurate causal inferences.
The complexity and high cost of collecting data requires an efficient experimental design to maximise causal discovery with limited resources.
Observational data are more readily available but may lead to confounded or biassed insights.
Interventional data, while more challenging to obtain, provide clearer causal pathways and can greatly enhance the modelâ€™s understanding of underlying biological processes [@discobax;@doi:10.48550/arXiv.2203.02016].
A balanced approach, possibly incorporating both observational and interventional data, coupled with mechanisms for deciding the right number and type of interventions, might provide a more nuanced understanding and improve model robustness and interpretability.
In addition, experimental design decisions such as the inclusion of a temporal axis can improve the amenability of observational data to causal inference.

### Foundation models: architectural biases and the No Free Lunch theorems

Foundation models challenge the "No Free Lunch" theorems by suggesting that certain architectural biases, learned from vast amounts of data, can yield generalisable and high-performing models [@doi:10.48550/arXiv.2304.05366].
These biases, and how to transfer them from LLMs ([Glossary][Large Language Models]) to systems biology, necessitate careful evaluation.
As the biomedical field looks to these models for answers, it becomes crucial to develop frameworks that facilitate rapid development and exploration of ideas [@doi:10.1038/s41587-023-01848-y;@doi:10.48550/arXiv.2210.17283].
A crucial aspect of these frameworks will be establishing benchmarks in the face of missing biological ground truth.

### Systems biology and causality - finding a balance

Systems biology has historically followed both knowledge-driven (bottom-up) and data-driven (top-down) approaches.
Bottom-up systems biology, aiming to understand specific molecular mechanisms driving biological phenomena, has *de facto* been doing CR, despite both fields being largely disconnected.
Meanwhile, top-down systems biology, inspired more by machine learning principles, has struggled with moving from correlation to causality.
New methods and models offer the potential to converge these complementary approaches and scale our understanding to larger, more complex systems.
However, it remains to be seen whether the future of biological modelling will be dominated by the generation of vast datasets for generalist models or by more nuanced, bias-inclusive architectures.

While the allure of generalist models trained on extensive datasets is strong, the unique challenges of biomedical research may necessitate a more tailored approach.
Including explicit favourable biases, informed by deep domain knowledge and specific data types (observational or interventional), could lead to breakthroughs in understanding complex biological systems.
The field must explore these possibilities, balancing the drive for large-scale data with the need for precision and specificity, to realise the full potential of modern systems biology.
