[
  {
    "publisher": "Elsevier BV",
    "issue": "7",
    "DOI": "10.1016/j.cell.2016.11.038",
    "type": "article-journal",
    "page": "1853-1866.e17",
    "source": "Crossref",
    "title": "Perturb-Seq: Dissecting Molecular Circuits with Scalable Single-Cell RNA Profiling of Pooled Genetic Screens",
    "volume": "167",
    "author": [
      {
        "given": "Atray",
        "family": "Dixit"
      },
      {
        "given": "Oren",
        "family": "Parnas"
      },
      {
        "given": "Biyu",
        "family": "Li"
      },
      {
        "given": "Jenny",
        "family": "Chen"
      },
      {
        "given": "Charles P.",
        "family": "Fulco"
      },
      {
        "given": "Livnat",
        "family": "Jerby-Arnon"
      },
      {
        "given": "Nemanja D.",
        "family": "Marjanovic"
      },
      {
        "given": "Danielle",
        "family": "Dionne"
      },
      {
        "given": "Tyler",
        "family": "Burks"
      },
      {
        "given": "Raktima",
        "family": "Raychowdhury"
      },
      {
        "given": "Britt",
        "family": "Adamson"
      },
      {
        "given": "Thomas M.",
        "family": "Norman"
      },
      {
        "given": "Eric S.",
        "family": "Lander"
      },
      {
        "given": "Jonathan S.",
        "family": "Weissman"
      },
      {
        "given": "Nir",
        "family": "Friedman"
      },
      {
        "given": "Aviv",
        "family": "Regev"
      }
    ],
    "container-title": "Cell",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2016,
          12
        ]
      ]
    },
    "URL": "https://doi.org/f9prjd",
    "container-title-short": "Cell",
    "PMCID": "PMC5181115",
    "PMID": "27984732",
    "id": "152yKY5w7",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1016/j.cell.2016.11.038"
  },
  {
    "publisher": "Cambridge University Press",
    "abstract": "<jats:p>Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. Cited in more than 2,100 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interest to students and professionals in a wide variety of fields. Dr Judea Pearl has received the 2011 Rumelhart Prize for his leading research in Artificial Intelligence (AI) and systems from The Cognitive Science Society.</jats:p>",
    "DOI": "10.1017/cbo9780511803161",
    "source": "Crossref",
    "title": "Causality",
    "author": [
      {
        "given": "Judea",
        "family": "Pearl"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2009,
          9,
          14
        ]
      ]
    },
    "URL": "https://doi.org/ggd72q",
    "id": "peGxtHPM",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1017/cbo9780511803161",
    "type": "entry"
  },
  {
    "publisher": "American Psychological Association (APA)",
    "issue": "1",
    "DOI": "10.1037/0033-295x.111.1.3",
    "type": "article-journal",
    "page": "3-32",
    "source": "Crossref",
    "title": "A Theory of Causal Learning in Children: Causal Maps and Bayes Nets.",
    "volume": "111",
    "author": [
      {
        "given": "Alison",
        "family": "Gopnik"
      },
      {
        "given": "Clark",
        "family": "Glymour"
      },
      {
        "given": "David M.",
        "family": "Sobel"
      },
      {
        "given": "Laura E.",
        "family": "Schulz"
      },
      {
        "given": "Tamar",
        "family": "Kushnir"
      },
      {
        "given": "David",
        "family": "Danks"
      }
    ],
    "container-title": "Psychological Review",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "URL": "https://doi.org/fkj59s",
    "container-title-short": "Psychological Review",
    "PMID": "14756583",
    "id": "1DMQH5kzt",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1037/0033-295x.111.1.3"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "7971",
    "DOI": "10.1038/d41586-023-02361-7",
    "type": "article-journal",
    "page": "686-689",
    "source": "Crossref",
    "title": "ChatGPT broke the Turing test — the race is on for new ways to assess AI",
    "volume": "619",
    "author": [
      {
        "given": "Celeste",
        "family": "Biever"
      }
    ],
    "container-title": "Nature",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          7,
          25
        ]
      ]
    },
    "URL": "https://doi.org/gskd92",
    "container-title-short": "Nature",
    "PMID": "37491395",
    "id": "19EQh1DNG",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/d41586-023-02361-7"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "7319",
    "DOI": "10.1038/nature09534",
    "type": "article-journal",
    "page": "1061-1073",
    "source": "Crossref",
    "title": "A map of human genome variation from population-scale sequencing",
    "volume": "467",
    "author": [
      {}
    ],
    "container-title": "Nature",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2010,
          10,
          27
        ]
      ]
    },
    "URL": "https://doi.org/fmk7rw",
    "container-title-short": "Nature",
    "PMCID": "PMC3042601",
    "PMID": "20981092",
    "id": "IxBIB6CT",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/nature09534"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "4",
    "DOI": "10.1038/nmeth.3773",
    "type": "article-journal",
    "page": "310-318",
    "source": "Crossref",
    "title": "Inferring causal molecular networks: empirical assessment through a community-based effort",
    "volume": "13",
    "author": [
      {
        "given": "Steven M",
        "family": "Hill"
      },
      {},
      {
        "given": "Laura M",
        "family": "Heiser"
      },
      {
        "given": "Thomas",
        "family": "Cokelaer"
      },
      {
        "given": "Michael",
        "family": "Unger"
      },
      {
        "given": "Nicole K",
        "family": "Nesser"
      },
      {
        "given": "Daniel E",
        "family": "Carlin"
      },
      {
        "given": "Yang",
        "family": "Zhang"
      },
      {
        "given": "Artem",
        "family": "Sokolov"
      },
      {
        "given": "Evan O",
        "family": "Paull"
      },
      {
        "given": "Chris K",
        "family": "Wong"
      },
      {
        "given": "Kiley",
        "family": "Graim"
      },
      {
        "given": "Adrian",
        "family": "Bivol"
      },
      {
        "given": "Haizhou",
        "family": "Wang"
      },
      {
        "given": "Fan",
        "family": "Zhu"
      },
      {
        "given": "Bahman",
        "family": "Afsari"
      },
      {
        "given": "Ludmila V",
        "family": "Danilova"
      },
      {
        "given": "Alexander V",
        "family": "Favorov"
      },
      {
        "given": "Wai Shing",
        "family": "Lee"
      },
      {
        "given": "Dane",
        "family": "Taylor"
      },
      {
        "given": "Chenyue W",
        "family": "Hu"
      },
      {
        "given": "Byron L",
        "family": "Long"
      },
      {
        "given": "David P",
        "family": "Noren"
      },
      {
        "given": "Alexander J",
        "family": "Bisberg"
      },
      {
        "given": "Gordon B",
        "family": "Mills"
      },
      {
        "given": "Joe W",
        "family": "Gray"
      },
      {
        "given": "Michael",
        "family": "Kellen"
      },
      {
        "given": "Thea",
        "family": "Norman"
      },
      {
        "given": "Stephen",
        "family": "Friend"
      },
      {
        "given": "Amina A",
        "family": "Qutub"
      },
      {
        "given": "Elana J",
        "family": "Fertig"
      },
      {
        "given": "Yuanfang",
        "family": "Guan"
      },
      {
        "given": "Mingzhou",
        "family": "Song"
      },
      {
        "given": "Joshua M",
        "family": "Stuart"
      },
      {
        "given": "Paul T",
        "family": "Spellman"
      },
      {
        "given": "Heinz",
        "family": "Koeppl"
      },
      {
        "given": "Gustavo",
        "family": "Stolovitzky"
      },
      {
        "given": "Julio",
        "family": "Saez-Rodriguez"
      },
      {
        "given": "Sach",
        "family": "Mukherjee"
      }
    ],
    "container-title": "Nature Methods",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2016,
          2,
          22
        ]
      ]
    },
    "URL": "https://doi.org/f3t7t4",
    "container-title-short": "Nat Methods",
    "PMCID": "PMC4854847",
    "PMID": "26901648",
    "id": "qpg6x7P4",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/nmeth.3773"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "11",
    "DOI": "10.1038/nrd3847",
    "type": "article-journal",
    "page": "873-886",
    "source": "Crossref",
    "title": "Vemurafenib: the first drug approved for BRAF-mutant cancer",
    "volume": "11",
    "author": [
      {
        "given": "Gideon",
        "family": "Bollag"
      },
      {
        "given": "James",
        "family": "Tsai"
      },
      {
        "given": "Jiazhong",
        "family": "Zhang"
      },
      {
        "given": "Chao",
        "family": "Zhang"
      },
      {
        "given": "Prabha",
        "family": "Ibrahim"
      },
      {
        "given": "Keith",
        "family": "Nolop"
      },
      {
        "given": "Peter",
        "family": "Hirth"
      }
    ],
    "container-title": "Nature Reviews Drug Discovery",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2012,
          10,
          12
        ]
      ]
    },
    "URL": "https://doi.org/f4b975",
    "container-title-short": "Nat Rev Drug Discov",
    "PMID": "23060265",
    "id": "1BSi2Dk6R",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/nrd3847"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "1",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Deep Learning (DL) has recently enabled unprecedented advances in one of the grand challenges in computational biology: the half-century-old problem of protein structure prediction. In this paper we discuss recent advances, limitations, and future perspectives of DL on five broad areas: protein structure prediction, protein function prediction, genome engineering, systems biology and data integration, and phylogenetic inference. We discuss each application area and cover the main bottlenecks of DL approaches, such as training data, problem scope, and the ability to leverage existing DL architectures in new contexts. To conclude, we provide a summary of the subject-specific and general challenges for DL across the biosciences.</jats:p>",
    "DOI": "10.1038/s41467-022-29268-7",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Current progress and open challenges for applying deep learning across the biosciences",
    "volume": "13",
    "author": [
      {
        "given": "Nicolae",
        "family": "Sapoval"
      },
      {
        "given": "Amirali",
        "family": "Aghazadeh"
      },
      {
        "given": "Michael G.",
        "family": "Nute"
      },
      {
        "given": "Dinler A.",
        "family": "Antunes"
      },
      {
        "given": "Advait",
        "family": "Balaji"
      },
      {
        "given": "Richard",
        "family": "Baraniuk"
      },
      {
        "given": "C. J.",
        "family": "Barberan"
      },
      {
        "given": "Ruth",
        "family": "Dannenfelser"
      },
      {
        "given": "Chen",
        "family": "Dun"
      },
      {
        "given": "Mohammadamin",
        "family": "Edrisi"
      },
      {
        "given": "R. A. Leo",
        "family": "Elworth"
      },
      {
        "given": "Bryce",
        "family": "Kille"
      },
      {
        "given": "Anastasios",
        "family": "Kyrillidis"
      },
      {
        "given": "Luay",
        "family": "Nakhleh"
      },
      {
        "given": "Cameron R.",
        "family": "Wolfe"
      },
      {
        "given": "Zhi",
        "family": "Yan"
      },
      {
        "given": "Vicky",
        "family": "Yao"
      },
      {
        "given": "Todd J.",
        "family": "Treangen"
      }
    ],
    "container-title": "Nature Communications",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2022,
          4,
          1
        ]
      ]
    },
    "URL": "https://doi.org/gp26xk",
    "container-title-short": "Nat Commun",
    "PMCID": "PMC8976012",
    "PMID": "35365602",
    "id": "1AZn5l2ah",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41467-022-29268-7"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "7965",
    "DOI": "10.1038/s41586-023-06139-9",
    "type": "article-journal",
    "page": "616-624",
    "source": "Crossref",
    "title": "Transfer learning enables predictions in network biology",
    "volume": "618",
    "author": [
      {
        "given": "Christina V.",
        "family": "Theodoris"
      },
      {
        "given": "Ling",
        "family": "Xiao"
      },
      {
        "given": "Anant",
        "family": "Chopra"
      },
      {
        "given": "Mark D.",
        "family": "Chaffin"
      },
      {
        "given": "Zeina R.",
        "family": "Al Sayed"
      },
      {
        "given": "Matthew C.",
        "family": "Hill"
      },
      {
        "given": "Helene",
        "family": "Mantineo"
      },
      {
        "given": "Elizabeth M.",
        "family": "Brydon"
      },
      {
        "given": "Zexian",
        "family": "Zeng"
      },
      {
        "given": "X. Shirley",
        "family": "Liu"
      },
      {
        "given": "Patrick T.",
        "family": "Ellinor"
      }
    ],
    "container-title": "Nature",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          5,
          31
        ]
      ]
    },
    "URL": "https://doi.org/gr9x63",
    "container-title-short": "Nature",
    "PMID": "37258680",
    "id": "VmzWBJUJ",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41586-023-06139-9"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "3",
    "DOI": "10.1038/s41587-019-0344-3",
    "type": "article-journal",
    "page": "365-373",
    "source": "Crossref",
    "title": "The functional landscape of the human phosphoproteome",
    "volume": "38",
    "author": [
      {
        "given": "David",
        "family": "Ochoa"
      },
      {
        "given": "Andrew F.",
        "family": "Jarnuczak"
      },
      {
        "given": "Cristina",
        "family": "Viéitez"
      },
      {
        "given": "Maja",
        "family": "Gehre"
      },
      {
        "given": "Margaret",
        "family": "Soucheray"
      },
      {
        "given": "André",
        "family": "Mateus"
      },
      {
        "given": "Askar A.",
        "family": "Kleefeldt"
      },
      {
        "given": "Anthony",
        "family": "Hill"
      },
      {
        "given": "Luz",
        "family": "Garcia-Alonso"
      },
      {
        "given": "Frank",
        "family": "Stein"
      },
      {
        "given": "Nevan J.",
        "family": "Krogan"
      },
      {
        "given": "Mikhail M.",
        "family": "Savitski"
      },
      {
        "given": "Danielle L.",
        "family": "Swaney"
      },
      {
        "given": "Juan A.",
        "family": "Vizcaíno"
      },
      {
        "given": "Kyung-Min",
        "family": "Noh"
      },
      {
        "given": "Pedro",
        "family": "Beltrao"
      }
    ],
    "container-title": "Nature Biotechnology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2019,
          12,
          9
        ]
      ]
    },
    "URL": "https://doi.org/ggd8n7",
    "container-title-short": "Nat Biotechnol",
    "PMCID": "PMC7100915",
    "PMID": "31819260",
    "id": "yCFobrFQ",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41587-019-0344-3"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "8",
    "DOI": "10.1038/s41587-023-01848-y",
    "type": "article-journal",
    "page": "1056-1059",
    "source": "Crossref",
    "title": "Democratizing knowledge representation with BioCypher",
    "volume": "41",
    "author": [
      {
        "given": "Sebastian",
        "family": "Lobentanzer"
      },
      {
        "given": "Patrick",
        "family": "Aloy"
      },
      {
        "given": "Jan",
        "family": "Baumbach"
      },
      {
        "given": "Balazs",
        "family": "Bohar"
      },
      {
        "given": "Vincent J.",
        "family": "Carey"
      },
      {
        "given": "Pornpimol",
        "family": "Charoentong"
      },
      {
        "given": "Katharina",
        "family": "Danhauser"
      },
      {
        "given": "Tunca",
        "family": "Doğan"
      },
      {
        "given": "Johann",
        "family": "Dreo"
      },
      {
        "given": "Ian",
        "family": "Dunham"
      },
      {
        "given": "Elias",
        "family": "Farr"
      },
      {
        "given": "Adrià",
        "family": "Fernandez-Torras"
      },
      {
        "given": "Benjamin M.",
        "family": "Gyori"
      },
      {
        "given": "Michael",
        "family": "Hartung"
      },
      {
        "given": "Charles Tapley",
        "family": "Hoyt"
      },
      {
        "given": "Christoph",
        "family": "Klein"
      },
      {
        "given": "Tamas",
        "family": "Korcsmaros"
      },
      {
        "given": "Andreas",
        "family": "Maier"
      },
      {
        "given": "Matthias",
        "family": "Mann"
      },
      {
        "given": "David",
        "family": "Ochoa"
      },
      {
        "given": "Elena",
        "family": "Pareja-Lorente"
      },
      {
        "given": "Ferdinand",
        "family": "Popp"
      },
      {
        "given": "Martin",
        "family": "Preusse"
      },
      {
        "given": "Niklas",
        "family": "Probul"
      },
      {
        "given": "Benno",
        "family": "Schwikowski"
      },
      {
        "given": "Bünyamin",
        "family": "Sen"
      },
      {
        "given": "Maximilian T.",
        "family": "Strauss"
      },
      {
        "given": "Denes",
        "family": "Turei"
      },
      {
        "given": "Erva",
        "family": "Ulusoy"
      },
      {
        "given": "Dagmar",
        "family": "Waltemath"
      },
      {
        "given": "Judith A. H.",
        "family": "Wodke"
      },
      {
        "given": "Julio",
        "family": "Saez-Rodriguez"
      }
    ],
    "container-title": "Nature Biotechnology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          6,
          19
        ]
      ]
    },
    "URL": "https://doi.org/gszqjr",
    "container-title-short": "Nat Biotechnol",
    "PMID": "37337100",
    "id": "tr1XjZ1R",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41587-023-01848-y"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "12",
    "DOI": "10.1038/s41592-018-0229-2",
    "type": "article-journal",
    "page": "1053-1058",
    "source": "Crossref",
    "title": "Deep generative modeling for single-cell transcriptomics",
    "volume": "15",
    "author": [
      {
        "given": "Romain",
        "family": "Lopez"
      },
      {
        "given": "Jeffrey",
        "family": "Regier"
      },
      {
        "given": "Michael B.",
        "family": "Cole"
      },
      {
        "given": "Michael I.",
        "family": "Jordan"
      },
      {
        "given": "Nir",
        "family": "Yosef"
      }
    ],
    "container-title": "Nature Methods",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2018,
          11,
          30
        ]
      ]
    },
    "URL": "https://doi.org/gfkw5z",
    "container-title-short": "Nat Methods",
    "PMCID": "PMC6289068",
    "PMID": "30504886",
    "id": "1ELFXHA51",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41592-018-0229-2"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "10",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>How noncoding DNA determines gene expression in different cell types is a major unsolved problem, and critical downstream applications in human genetics depend on improved solutions. Here, we report substantially improved gene expression prediction accuracy from DNA sequences through the use of a deep learning architecture, called Enformer, that is able to integrate information from long-range interactions (up to 100 kb away) in the genome. This improvement yielded more accurate variant effect predictions on gene expression for both natural genetic variants and saturation mutagenesis measured by massively parallel reporter assays. Furthermore, Enformer learned to predict enhancer–promoter interactions directly from the DNA sequence competitively with methods that take direct experimental data as input. We expect that these advances will enable more effective fine-mapping of human disease associations and provide a framework to interpret <jats:italic>cis</jats:italic>-regulatory evolution.</jats:p>",
    "DOI": "10.1038/s41592-021-01252-x",
    "type": "article-journal",
    "page": "1196-1203",
    "source": "Crossref",
    "title": "Effective gene expression prediction from sequence by integrating long-range interactions",
    "volume": "18",
    "author": [
      {
        "given": "Žiga",
        "family": "Avsec"
      },
      {
        "given": "Vikram",
        "family": "Agarwal"
      },
      {
        "given": "Daniel",
        "family": "Visentin"
      },
      {
        "given": "Joseph R.",
        "family": "Ledsam"
      },
      {
        "given": "Agnieszka",
        "family": "Grabska-Barwinska"
      },
      {
        "given": "Kyle R.",
        "family": "Taylor"
      },
      {
        "given": "Yannis",
        "family": "Assael"
      },
      {
        "given": "John",
        "family": "Jumper"
      },
      {
        "given": "Pushmeet",
        "family": "Kohli"
      },
      {
        "given": "David R.",
        "family": "Kelley"
      }
    ],
    "container-title": "Nature Methods",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2021,
          10
        ]
      ]
    },
    "URL": "https://doi.org/gm2wv4",
    "container-title-short": "Nat Methods",
    "PMCID": "PMC8490152",
    "PMID": "34608324",
    "id": "gAQyFCbW",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41592-021-01252-x"
  },
  {
    "publisher": "Massachusetts Medical Society",
    "issue": "26",
    "DOI": "10.1056/nejmoa1103782",
    "type": "article-journal",
    "page": "2507-2516",
    "source": "Crossref",
    "title": "Improved Survival with Vemurafenib in Melanoma with BRAF V600E Mutation",
    "volume": "364",
    "author": [
      {
        "given": "Paul B.",
        "family": "Chapman"
      },
      {
        "given": "Axel",
        "family": "Hauschild"
      },
      {
        "given": "Caroline",
        "family": "Robert"
      },
      {
        "given": "John B.",
        "family": "Haanen"
      },
      {
        "given": "Paolo",
        "family": "Ascierto"
      },
      {
        "given": "James",
        "family": "Larkin"
      },
      {
        "given": "Reinhard",
        "family": "Dummer"
      },
      {
        "given": "Claus",
        "family": "Garbe"
      },
      {
        "given": "Alessandro",
        "family": "Testori"
      },
      {
        "given": "Michele",
        "family": "Maio"
      },
      {
        "given": "David",
        "family": "Hogg"
      },
      {
        "given": "Paul",
        "family": "Lorigan"
      },
      {
        "given": "Celeste",
        "family": "Lebbe"
      },
      {
        "given": "Thomas",
        "family": "Jouary"
      },
      {
        "given": "Dirk",
        "family": "Schadendorf"
      },
      {
        "given": "Antoni",
        "family": "Ribas"
      },
      {
        "given": "Steven J.",
        "family": "O'Day"
      },
      {
        "given": "Jeffrey A.",
        "family": "Sosman"
      },
      {
        "given": "John M.",
        "family": "Kirkwood"
      },
      {
        "given": "Alexander M.M.",
        "family": "Eggermont"
      },
      {
        "given": "Brigitte",
        "family": "Dreno"
      },
      {
        "given": "Keith",
        "family": "Nolop"
      },
      {
        "given": "Jiang",
        "family": "Li"
      },
      {
        "given": "Betty",
        "family": "Nelson"
      },
      {
        "given": "Jeannie",
        "family": "Hou"
      },
      {
        "given": "Richard J.",
        "family": "Lee"
      },
      {
        "given": "Keith T.",
        "family": "Flaherty"
      },
      {
        "given": "Grant A.",
        "family": "McArthur"
      }
    ],
    "container-title": "New England Journal of Medicine",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2011,
          6,
          30
        ]
      ]
    },
    "URL": "https://doi.org/dsbxxt",
    "container-title-short": "N Engl J Med",
    "PMCID": "PMC3549296",
    "PMID": "21639808",
    "id": "muFRX2ZL",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1056/nejmoa1103782"
  },
  {
    "publisher": "Informa UK Limited",
    "issue": "434",
    "DOI": "10.1080/01621459.1996.10476902",
    "type": "article-journal",
    "page": "444-455",
    "source": "Crossref",
    "title": "Identification of Causal Effects Using Instrumental Variables",
    "volume": "91",
    "author": [
      {
        "given": "Joshua D.",
        "family": "Angrist"
      },
      {
        "given": "Guido W.",
        "family": "Imbens"
      },
      {
        "given": "Donald B.",
        "family": "Rubin"
      }
    ],
    "container-title": "Journal of the American Statistical Association",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1996,
          6
        ]
      ]
    },
    "URL": "https://doi.org/gdz4f4",
    "container-title-short": "Journal of the American Statistical Association",
    "id": "w47lt0ah",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1080/01621459.1996.10476902"
  },
  {
    "publisher": "The Royal Society",
    "issue": "2266",
    "abstract": "<jats:p>A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopaedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behaviour of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems benefiting from humans’ abilities in terms of flexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.</jats:p>",
    "DOI": "10.1098/rspa.2021.0068",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Inductive biases for deep learning of higher-level cognition",
    "volume": "478",
    "author": [
      {
        "given": "Anirudh",
        "family": "Goyal"
      },
      {
        "given": "Yoshua",
        "family": "Bengio"
      }
    ],
    "container-title": "Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2022,
          10
        ]
      ]
    },
    "URL": "https://doi.org/gs39f8",
    "container-title-short": "Proc. R. Soc. A.",
    "id": "11wELIlTc",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1098/rspa.2021.0068"
  },
  {
    "publisher": "Cold Spring Harbor Laboratory",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Generative pre-trained models have achieved remarkable success in various domains such as natural language processing and computer vision. Specifically, the combination of large-scale diverse datasets and pre-trained transformers has emerged as a promising approach for developing foundation models. Drawing parallels between linguistic constructs and cellular biology — where texts comprise words, similarly, cells are defined by genes — our study probes the applicability of foundation models to advance cellular biology and genetics research. Utilizing the burgeoning single-cell sequencing data, we have pioneered the construction of a foundation model for single-cell biology, scGPT, which is based on generative pre-trained transformer across a repository of over 33 million cells. Our findings illustrate that scGPT, a generative pre-trained transformer, effectively distills critical biological insights concerning genes and cells. Through the further adaptation of transfer learning, scGPT can be optimized to achieve superior performance across diverse downstream applications. This includes tasks such as cell-type annotation, multi-batch integration, multi-omic integration, genetic perturbation prediction, and gene network inference. The scGPT codebase is publicly available at<jats:ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://github.com/bowang-lab/scGPT\">https://github.com/bowang-lab/scGPT</jats:ext-link>.</jats:p>",
    "DOI": "10.1101/2023.04.30.538439",
    "type": "manuscript",
    "source": "Crossref",
    "title": "scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI",
    "author": [
      {
        "given": "Haotian",
        "family": "Cui"
      },
      {
        "given": "Chloe",
        "family": "Wang"
      },
      {
        "given": "Hassaan",
        "family": "Maan"
      },
      {
        "given": "Kuan",
        "family": "Pang"
      },
      {
        "given": "Fengning",
        "family": "Luo"
      },
      {
        "given": "Bo",
        "family": "Wang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023,
          5,
          1
        ]
      ]
    },
    "URL": "https://doi.org/gshk6p",
    "id": "r5y0HbhJ",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1101/2023.04.30.538439"
  },
  {
    "publisher": "Cold Spring Harbor Laboratory",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>The advent and success of foundation models such as GPT has sparked growing interest in their application to single-cell biology. Models like Geneformer and scGPT have emerged with the promise of serving as versatile tools for this specialized field. However, the efficacy of these models, particularly in zero-shot settings where models are not fine-tuned but used without any further training, remains an open question, especially as practical constraints require useful models to function in settings that preclude fine-tuning (e.g., discovery settings where labels are not fully known). This paper presents a rigorous evaluation of the zero-shot performance of these proposed single-cell foundation models. We assess their utility in tasks such as cell type clustering and batch effect correction, and evaluate the generality of their pretraining objectives. Our results indicate that both Geneformer and scGPT exhibit limited reliability in zero-shot settings and often underperform compared to simpler methods. These findings serve as a cautionary note for the deployment of proposed single-cell foundation models and highlight the need for more focused research to realize their potential.<jats:sup>2</jats:sup></jats:p>",
    "DOI": "10.1101/2023.10.16.561085",
    "type": "manuscript",
    "source": "Crossref",
    "title": "Assessing the limits of zero-shot foundation models in single-cell biology",
    "author": [
      {
        "given": "Kasia Z.",
        "family": "Kedzierska"
      },
      {
        "given": "Lorin",
        "family": "Crawford"
      },
      {
        "given": "Ava P.",
        "family": "Amini"
      },
      {
        "given": "Alex X.",
        "family": "Lu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023,
          10,
          17
        ]
      ]
    },
    "URL": "https://doi.org/gszxk9",
    "id": "WEYqVcYG",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1101/2023.10.16.561085"
  },
  {
    "publisher": "Cold Spring Harbor Laboratory",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Large-scale foundation models, which are pre-trained on massive, unlabeled datasets and subsequently fine-tuned on specific tasks, have recently achieved unparalleled success on a wide array of applications, including in healthcare and biology. In this paper, we explore two foundation models recently developed for single-cell RNA sequencing data, scBERT and scGPT. Focusing on the fine-tuning task of cell type annotation, we explore the relative performance of pre-trained models compared to a simple baseline, L1-regularized logistic regression, including in the few-shot setting. We perform ablation studies to understand whether pretraining improves model performance and to better understand the difficulty of the pre-training task in scBERT. Finally, using scBERT as an example, we demonstrate the potential sensitivity of fine-tuning to hyperparameter settings and parameter initializations. Taken together, our results highlight the importance of rigorously testing foundation models against well established baselines, establishing challenging fine-tuning tasks on which to benchmark foundation models, and performing deep introspection into the embeddings learned by the model in order to more effectively harness these models to transform single-cell data analysis. Code is available at<jats:ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://github.com/clinicalml/sc-foundation-eval\">https://github.com/clinicalml/sc-foundation-eval</jats:ext-link>.</jats:p>",
    "DOI": "10.1101/2023.10.19.563100",
    "type": "manuscript",
    "source": "Crossref",
    "title": "A Deep Dive into Single-Cell RNA Sequencing Foundation Models",
    "author": [
      {
        "given": "Rebecca",
        "family": "Boiarsky"
      },
      {
        "given": "Nalini",
        "family": "Singh"
      },
      {
        "given": "Alejandro",
        "family": "Buendia"
      },
      {
        "given": "Gad",
        "family": "Getz"
      },
      {
        "given": "David",
        "family": "Sontag"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023,
          10,
          23
        ]
      ]
    },
    "URL": "https://doi.org/gszxmb",
    "id": "OFczH7ba",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1101/2023.10.19.563100"
  },
  {
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "issue": "5",
    "DOI": "10.1109/jproc.2021.3058954",
    "type": "article-journal",
    "page": "612-634",
    "source": "Crossref",
    "title": "Toward Causal Representation Learning",
    "volume": "109",
    "author": [
      {
        "given": "Bernhard",
        "family": "Scholkopf"
      },
      {
        "given": "Francesco",
        "family": "Locatello"
      },
      {
        "given": "Stefan",
        "family": "Bauer"
      },
      {
        "given": "Nan Rosemary",
        "family": "Ke"
      },
      {
        "given": "Nal",
        "family": "Kalchbrenner"
      },
      {
        "given": "Anirudh",
        "family": "Goyal"
      },
      {
        "given": "Yoshua",
        "family": "Bengio"
      }
    ],
    "container-title": "Proceedings of the IEEE",
    "issued": {
      "date-parts": [
        [
          2021,
          5
        ]
      ]
    },
    "URL": "https://doi.org/gjhqrh",
    "container-title-short": "Proc. IEEE",
    "id": "T7D6XA6s",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1109/jproc.2021.3058954"
  },
  {
    "publisher": "American Association for the Advancement of Science (AAAS)",
    "issue": "6022",
    "abstract": "<jats:p>In coming to understand the world—in learning concepts, acquiring language, and grasping causal relations—our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?</jats:p>",
    "DOI": "10.1126/science.1192788",
    "type": "article-journal",
    "page": "1279-1285",
    "source": "Crossref",
    "title": "How to Grow a Mind: Statistics, Structure, and Abstraction",
    "volume": "331",
    "author": [
      {
        "given": "Joshua B.",
        "family": "Tenenbaum"
      },
      {
        "given": "Charles",
        "family": "Kemp"
      },
      {
        "given": "Thomas L.",
        "family": "Griffiths"
      },
      {
        "given": "Noah D.",
        "family": "Goodman"
      }
    ],
    "container-title": "Science",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2011,
          3,
          11
        ]
      ]
    },
    "URL": "https://doi.org/d8vvm9",
    "container-title-short": "Science",
    "PMID": "21393536",
    "id": "iWcDZtHu",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1126/science.1192788"
  },
  {
    "publisher": "American Association for the Advancement of Science (AAAS)",
    "issue": "565",
    "abstract": "<jats:p>Identifying the targets of “dark” kinases will provide new biological and disease insights.</jats:p>",
    "DOI": "10.1126/scisignal.aau8645",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Illuminating the dark phosphoproteome",
    "volume": "12",
    "author": [
      {
        "given": "Elise J.",
        "family": "Needham"
      },
      {
        "given": "Benjamin L.",
        "family": "Parker"
      },
      {
        "given": "Timur",
        "family": "Burykin"
      },
      {
        "given": "David E.",
        "family": "James"
      },
      {
        "given": "Sean J.",
        "family": "Humphrey"
      }
    ],
    "container-title": "Science Signaling",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2019,
          1,
          22
        ]
      ]
    },
    "URL": "https://doi.org/gf8c3h",
    "container-title-short": "Sci. Signal.",
    "PMID": "30670635",
    "id": "4VxTzwTj",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1126/scisignal.aau8645"
  },
  {
    "publisher": "Annual Reviews",
    "issue": "1",
    "abstract": "<jats:p> Graphical models can represent a multivariate distribution in a convenient and accessible form as a graph. Causal models can be viewed as a special class of graphical models that represent not only the distribution of the observed system but also the distributions under external interventions. They hence enable predictions under hypothetical interventions, which is important for decision making. The challenging task of learning causal models from data always relies on some underlying assumptions. We discuss several recently proposed structure learning algorithms and their assumptions, and we compare their empirical performance under various scenarios. </jats:p>",
    "DOI": "10.1146/annurev-statistics-031017-100630",
    "type": "article-journal",
    "page": "371-391",
    "source": "Crossref",
    "title": "Causal Structure Learning",
    "volume": "5",
    "author": [
      {
        "given": "Christina",
        "family": "Heinze-Deml"
      },
      {
        "given": "Marloes H.",
        "family": "Maathuis"
      },
      {
        "given": "Nicolai",
        "family": "Meinshausen"
      }
    ],
    "container-title": "Annual Review of Statistics and Its Application",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2018,
          3,
          7
        ]
      ]
    },
    "URL": "https://doi.org/ggh4pj",
    "container-title-short": "Annu. Rev. Stat. Appl.",
    "id": "1CPlHia5R",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1146/annurev-statistics-031017-100630"
  },
  {
    "publisher": "Institute of Mathematical Statistics",
    "issue": "none",
    "DOI": "10.1214/09-ss057",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Causal inference in statistics: An overview",
    "volume": "3",
    "author": [
      {
        "given": "Judea",
        "family": "Pearl"
      }
    ],
    "container-title": "Statistics Surveys",
    "issued": {
      "date-parts": [
        [
          2009,
          1,
          1
        ]
      ]
    },
    "URL": "https://doi.org/drt748",
    "container-title-short": "Statist. Surv.",
    "id": "dJUlTjFg",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1214/09-ss057"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "7",
    "DOI": "10.15252/msb.202211036",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Integrating knowledge and omics to decipher mechanisms via large‐scale models of signaling networks",
    "volume": "18",
    "author": [
      {
        "given": "Martin",
        "family": "Garrido‐Rodriguez"
      },
      {
        "given": "Katharina",
        "family": "Zirngibl"
      },
      {
        "given": "Olga",
        "family": "Ivanova"
      },
      {
        "given": "Sebastian",
        "family": "Lobentanzer"
      },
      {
        "given": "Julio",
        "family": "Saez‐Rodriguez"
      }
    ],
    "container-title": "Molecular Systems Biology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2022,
          7
        ]
      ]
    },
    "URL": "https://doi.org/gtb9v8",
    "container-title-short": "Molecular Systems Biology",
    "PMCID": "PMC9316933",
    "PMID": "35880747",
    "id": "TUhGg6tD",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.15252/msb.202211036"
  },
  {
    "publisher": "AI Access Foundation",
    "abstract": "<jats:p>A major problem in machine learning is that of inductive    bias: how to choose a learner's hypothesis space so that it is large    enough to contain a solution to the problem being learnt, yet small    enough to ensure reliable generalization from reasonably-sized    training sets.  Typically such bias is supplied by hand through the    skill and insights of experts. In this paper a model for automatically    learning bias is investigated. The central assumption of the model is    that the learner is embedded within an environment of related learning    tasks. Within such an environment the learner can sample from multiple    tasks, and hence it can search for a hypothesis space that contains    good solutions to many of the problems in the environment. Under    certain restrictions on the set of all hypothesis spaces available to    the learner, we show that a hypothesis space that performs well on a    sufficiently large number of training tasks will also perform well    when learning novel tasks in the same environment.  Explicit bounds    are also derived demonstrating that learning multiple tasks within an    environment of related tasks can potentially give much better    generalization than learning a single task.</jats:p>",
    "DOI": "10.1613/jair.731",
    "type": "article-journal",
    "page": "149-198",
    "source": "Crossref",
    "title": "A Model of Inductive Bias Learning",
    "volume": "12",
    "author": [
      {
        "given": "J.",
        "family": "Baxter"
      }
    ],
    "container-title": "Journal of Artificial Intelligence Research",
    "issued": {
      "date-parts": [
        [
          2000,
          3,
          1
        ]
      ]
    },
    "URL": "https://doi.org/gg66h8",
    "container-title-short": "jair",
    "id": "2RptKLT2",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1613/jair.731"
  },
  {
    "publisher": "MDPI AG",
    "issue": "6",
    "abstract": "<jats:p>The discovery of the role of the RAS/RAF/MEK/ERK pathway in melanomagenesis and its progression have opened a new era in the treatment of this tumor. Vemurafenib was the first specific kinase inhibitor approved for therapy of advanced melanomas harboring BRAF-activating mutations, followed by dabrafenib and encorafenib. However, despite the excellent results of first-generation kinase inhibitors in terms of response rate, the average duration of the response was short, due to the onset of genetic and epigenetic resistance mechanisms. The combination therapy with MEK inhibitors is an excellent strategy to circumvent drug resistance, with the additional advantage of reducing side effects due to the paradoxical reactivation of the MAPK pathway. The recent development of RAS and extracellular signal-related kinases (ERK) inhibitors promises to add new players for the ultimate suppression of this signaling pathway and the control of pathway-related drug resistance. In this review, we analyze the pharmacological, preclinical, and clinical trial data of the various MAPK pathway inhibitors, with a keen interest for their clinical applicability in the management of advanced melanoma.</jats:p>",
    "DOI": "10.3390/ijms20061483",
    "type": "article-journal",
    "page": "1483",
    "source": "Crossref",
    "title": "Targeting the ERK Signaling Pathway in Melanoma",
    "volume": "20",
    "author": [
      {
        "given": "Paola",
        "family": "Savoia"
      },
      {
        "given": "Paolo",
        "family": "Fava"
      },
      {
        "given": "Filippo",
        "family": "Casoni"
      },
      {
        "given": "Ottavio",
        "family": "Cremona"
      }
    ],
    "container-title": "International Journal of Molecular Sciences",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2019,
          3,
          25
        ]
      ]
    },
    "URL": "https://doi.org/gtb9v9",
    "container-title-short": "IJMS",
    "PMCID": "PMC6472057",
    "PMID": "30934534",
    "id": "ZkFzi3Xl",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.3390/ijms20061483"
  },
  {
    "type": "article",
    "id": "12GOinshI",
    "categories": [
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Pearl",
        "given": "Judea"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "abstract": "The do-calculus was developed in 1995 to facilitate the identification of causal effects in non-parametric models. The completeness proofs of [Huang and Valtorta, 2006] and [Shpitser and Pearl, 2006] and the graphical criteria of [Tian and Shpitser, 2010] have laid this identification problem to rest. Recent explorations unveil the usefulness of the do-calculus in three additional areas: mediation analysis [Pearl, 2012], transportability [Pearl and Bareinboim, 2011] and metasynthesis. Meta-synthesis (freshly coined) is the task of fusing empirical results from several diverse studies, conducted on heterogeneous populations and under different conditions, so as to synthesize an estimate of a causal relation in some target environment, potentially different from those under study. The talk surveys these results with emphasis on the challenges posed by meta-synthesis. For background material, see http://bayes.cs.ucla.edu/csl_papers.html",
    "DOI": "10.48550/arxiv.1210.4852",
    "publisher": "arXiv",
    "title": "The Do-Calculus Revisited",
    "URL": "https://doi.org/gtbf4r",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1210.4852"
  },
  {
    "type": "article",
    "id": "rh7nCPVE",
    "categories": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Vaswani",
        "given": "Ashish"
      },
      {
        "family": "Shazeer",
        "given": "Noam"
      },
      {
        "family": "Parmar",
        "given": "Niki"
      },
      {
        "family": "Uszkoreit",
        "given": "Jakob"
      },
      {
        "family": "Jones",
        "given": "Llion"
      },
      {
        "family": "Gomez",
        "given": "Aidan N."
      },
      {
        "family": "Kaiser",
        "given": "Lukasz"
      },
      {
        "family": "Polosukhin",
        "given": "Illia"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "DOI": "10.48550/arxiv.1706.03762",
    "publisher": "arXiv",
    "title": "Attention Is All You Need",
    "URL": "https://doi.org/gpnmtv",
    "version": "7",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1706.03762"
  },
  {
    "type": "article-journal",
    "id": "URCTSFCA",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Locatello",
        "given": "Francesco"
      },
      {
        "family": "Bauer",
        "given": "Stefan"
      },
      {
        "family": "Lucic",
        "given": "Mario"
      },
      {
        "family": "Rätsch",
        "given": "Gunnar"
      },
      {
        "family": "Gelly",
        "given": "Sylvain"
      },
      {
        "family": "Schölkopf",
        "given": "Bernhard"
      },
      {
        "family": "Bachem",
        "given": "Olivier"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "abstract": "The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties ``encouraged'' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.",
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.1811.12359",
    "publisher": "arXiv",
    "title": "Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations",
    "URL": "https://doi.org/grx79c",
    "version": "4",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1811.12359"
  },
  {
    "type": "article",
    "id": "B5WSzZkm",
    "categories": [
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Mehrabi",
        "given": "Ninareh"
      },
      {
        "family": "Morstatter",
        "given": "Fred"
      },
      {
        "family": "Saxena",
        "given": "Nripsuta"
      },
      {
        "family": "Lerman",
        "given": "Kristina"
      },
      {
        "family": "Galstyan",
        "given": "Aram"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "abstract": "With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",
    "DOI": "10.48550/arxiv.1908.09635",
    "publisher": "arXiv",
    "title": "A Survey on Bias and Fairness in Machine Learning",
    "URL": "https://doi.org/gtb9wn",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1908.09635"
  },
  {
    "type": "article",
    "id": "vm2M7mI5",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Aliee",
        "given": "Hananeh"
      },
      {
        "family": "Theis",
        "given": "Fabian J."
      },
      {
        "family": "Kilbertus",
        "given": "Niki"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "abstract": "Spurred by tremendous success in pattern matching and prediction tasks, researchers increasingly resort to machine learning to aid original scientific discovery. Given large amounts of observational data about a system, can we uncover the rules that govern its evolution? Solving this task holds the great promise of fully understanding the causal interactions and being able to make reliable predictions about the system's behavior under interventions. We take a step towards answering this question for time-series data generated from systems of ordinary differential equations (ODEs). While the governing ODEs might not be identifiable from data alone, we show that combining simple regularization schemes with flexible neural ODEs can robustly recover the dynamics and causal structures from time-series data. Our results on a variety of (non)-linear first and second order systems as well as real data validate our method. We conclude by showing that we can also make accurate predictions under interventions on variables or the system itself.",
    "DOI": "10.48550/arxiv.2106.12430",
    "publisher": "arXiv",
    "title": "Beyond Predictions in Neural ODEs: Identification and Interventions",
    "URL": "https://doi.org/gszw4d",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2106.12430"
  },
  {
    "id": "10mjWN2op",
    "type": "article",
    "abstract": "Foundation models are subject to an ongoing heated debate, leaving open the question of progress towards AGI and dividing the community into two camps: the ones who see the arguably impressive results as evidence to the scaling hypothesis, and the others who are worried about the lack of interpretability and reasoning capabilities. By investigating to which extent causal representations might be captured by these large scale language models, we make a humble efforts towards resolving the ongoing philosophical conflicts.",
    "DOI": "10.48550/arxiv.2206.10591",
    "note": "arXiv:2206.10591 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2206.10591",
    "number": "arXiv:2206.10591",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Can Foundation Models Talk Causality?",
    "URL": "https://doi.org/gtb9wb",
    "author": [
      {
        "family": "Willig",
        "given": "Moritz"
      },
      {
        "family": "Zečević",
        "given": "Matej"
      },
      {
        "family": "Dhami",
        "given": "Devendra Singh"
      },
      {
        "family": "Kersting",
        "given": "Kristian"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          1,
          4
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          12,
          23
        ]
      ]
    }
  },
  {
    "type": "article",
    "id": "D3JIQ7Oe",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Goldblum",
        "given": "Micah"
      },
      {
        "family": "Finzi",
        "given": "Marc"
      },
      {
        "family": "Rowan",
        "given": "Keefer"
      },
      {
        "family": "Wilson",
        "given": "Andrew Gordon"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "No free lunch theorems for supervised learning state that no learner can solve all problems or that all learners achieve exactly the same accuracy on average over a uniform distribution on learning problems. Accordingly, these theorems are often referenced in support of the notion that individual problems require specially tailored inductive biases. While virtually all uniformly sampled datasets have high complexity, real-world problems disproportionately generate low-complexity data, and we argue that neural network models share this same preference, formalized using Kolmogorov complexity. Notably, we show that architectures designed for a particular domain, such as computer vision, can compress datasets on a variety of seemingly unrelated domains. Our experiments show that pre-trained and even randomly initialized language models prefer to generate low-complexity sequences. Whereas no free lunch theorems seemingly indicate that individual problems require specialized learners, we explain how tasks that often require human intervention such as picking an appropriately sized model when labeled data is scarce or plentiful can be automated into a single learning algorithm. These observations justify the trend in deep learning of unifying seemingly disparate problems with an increasingly small set of machine learning models.",
    "DOI": "10.48550/arxiv.2304.05366",
    "publisher": "arXiv",
    "title": "The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning",
    "URL": "https://doi.org/gtb9wp",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2304.05366"
  },
  {
    "type": "article",
    "id": "1DSO3BUly",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Han",
        "given": "Insu"
      },
      {
        "family": "Jayaram",
        "given": "Rajesh"
      },
      {
        "family": "Karbasi",
        "given": "Amin"
      },
      {
        "family": "Mirrokni",
        "given": "Vahab"
      },
      {
        "family": "Woodruff",
        "given": "David P."
      },
      {
        "family": "Zandieh",
        "given": "Amir"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to identify large entries, HyperAttention outperforms existing methods, giving significant speed improvements compared to state-of-the-art solutions like FlashAttention. We validate the empirical performance of HyperAttention on a variety of different long-context length datasets. For example, HyperAttention makes the inference time of ChatGLM2 50\\% faster on 32k context length while perplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k, with causal masking, HyperAttention offers 5-fold speedup on a single attention layer.",
    "DOI": "10.48550/arxiv.2310.05869",
    "publisher": "arXiv",
    "title": "HyperAttention: Long-context Attention in Near-Linear Time",
    "URL": "https://doi.org/gtb9wc",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2310.05869"
  },
  {
    "type": "article",
    "id": "17scpieH5",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Listgarten",
        "given": "Jennifer"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Since ChatGPT works so well, are we on the cusp of solving science with AI? Is not AlphaFold2 suggestive that the potential of LLMs in biology and the sciences more broadly is limitless? Can we use AI itself to bridge the lack of data in the sciences in order to then train an AI? Herein we present a discussion of these topics.",
    "DOI": "10.48550/arxiv.2312.00818",
    "publisher": "arXiv",
    "title": "The perpetual motion machine of AI-generated data and the distraction of ChatGPT-as-scientist",
    "URL": "https://doi.org/gs8pnp",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2312.00818"
  },
  {
    "id": "WmT8ZU5I",
    "type": "book",
    "call-number": "B1481 .M55 2007",
    "collection-title": "Oxford world's classics",
    "event-place": "Oxford ; New York",
    "ISBN": "9780199211586",
    "note": "OCLC: ocm84995356\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: isbn:9780199211586",
    "number-of-pages": "238",
    "publisher": "Oxford University Press",
    "publisher-place": "Oxford ; New York",
    "source": "Library of Congress ISBN",
    "title": "An enquiry concerning human understanding",
    "author": [
      {
        "family": "Hume",
        "given": "David"
      },
      {
        "family": "Millican",
        "given": "P. F."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2007"
        ]
      ]
    }
  },
  {
    "id": "185mDnD0M",
    "type": "book",
    "abstract": "\"Everyone has heard the claim, \"Correlation does not imply causation.\" What might sound like a reasonable dictum metastasized in the twentieth century into one of science's biggest obstacles, as a legion of researchers became unwilling to make the claim that one thing could cause another. Even two decades ago, asking a statistician a question like \"Was it the aspirin that stopped my headache?\" would have been like asking if he believed in voodoo, or at best a topic for conversation at a cocktail party rather than a legitimate target of scientific inquiry. Scientists were allowed to posit only that the probability that one thing was associated with another. This all changed with Judea Pearl, whose work on causality was not just a victory for common sense, but a revolution in the study of the world\"--",
    "call-number": "Q175.32.C38",
    "event-place": "New York",
    "ISBN": "9780465097616",
    "number-of-pages": "1",
    "publisher": "Basic Books",
    "publisher-place": "New York",
    "source": "Library of Congress ISBN",
    "title": "The book of why: the new science of cause and effect",
    "title-short": "The book of why",
    "author": [
      {
        "family": "Pearl",
        "given": "Judea"
      },
      {
        "family": "Mackenzie",
        "given": "Dana"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: isbn:9780465097609"
  },
  {
    "type": "book",
    "id": "YcLevYSc",
    "title": "The Organon, Or Logical Treatises, Of Aristole, Vol. 1 Of 2",
    "author": [
      {
        "literal": "Aristotle"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "ISBN": "9781330267608",
    "publisher": "Forgotten Books",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: isbn:9781330267608"
  },
  {
    "id": "712MuGug",
    "type": "book",
    "abstract": "\"In Myth and Measurement, David Card and Alan Krueger presented a powerful challenge to the conventional view that higher minimum wages necessarily reduce jobs for low-wage workers. In a work that has profoundly influenced public policy as well as the direction of economic research, the authors put standard economic theory to the test, using empirical methods and conducting a critical reexamination of existing literature on the minimum wage. Documenting the effects of the minimum wage on family earnings, poverty outcomes, and the stock market valuation of low-wage employers, they presented a wealth of evidence showing that increases in the minimum wage lead to increases in pay, but no loss in jobs. With a new preface discussing new data, Myth and Measurement continues to shift the terms of the debate on the minimum wage\"--Back cover",
    "edition": "Twentieth-anniversary edition",
    "event-place": "Princeton, New Jersey",
    "ISBN": "9781400880874",
    "language": "eng",
    "note": "OCLC: 928384513\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: isbn:9781400880874",
    "publisher": "Princeton University Press",
    "publisher-place": "Princeton, New Jersey",
    "source": "Open WorldCat",
    "title": "Myth and measurement: the new economics of the minimum wage",
    "title-short": "Myth and measurement",
    "author": [
      {
        "family": "Card",
        "given": "David E."
      },
      {
        "family": "Krueger",
        "given": "Alan B."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    }
  },
  {
    "id": "nT3xJkyD",
    "title": "No Free Lunch Theorems for Search",
    "issued": {
      "date-parts": [
        [
          1995
        ]
      ]
    },
    "author": [
      {
        "given": "David H.",
        "family": "Wolpert"
      },
      {
        "given": "William G.",
        "family": "Macready"
      }
    ],
    "URL": "https://ideas.repec.org/p/wop/safiwp/95-02-010.html",
    "publisher": "Santa Fe Institute",
    "number": "95-02-010",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: no-free-lunch",
    "type": "entry"
  },
  {
    "id": "ydpntqD3",
    "type": "webpage",
    "title": "The Bitter Lesson",
    "URL": "http://www.incompleteideas.net/IncIdeas/BitterLesson.html",
    "accessed": {
      "date-parts": [
        [
          "2024",
          1,
          4
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:http://www.incompleteideas.net/IncIdeas/BitterLesson.html"
  },
  {
    "id": "U6LC2Ufe",
    "type": "webpage",
    "title": "Stanford CRFM",
    "URL": "https://crfm.stanford.edu/",
    "accessed": {
      "date-parts": [
        [
          "2024",
          1,
          4
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://crfm.stanford.edu/"
  },
  {
    "id": "10AL1hWhU",
    "type": "article-journal",
    "abstract": "<p>On GPT-3: meta-learning, scaling, implications, and deep theory. The scaling hypothesis: neural nets absorb data &amp; compute, generalizing and becoming more Bayesian as problems get harder, manifesting new abilities even at trivial-by-global-standards-scale. The deep learning revolution has begun as foretold.</p>",
    "language": "en-us",
    "source": "gwern.net",
    "title": "The Scaling Hypothesis",
    "URL": "https://gwern.net/scaling-hypothesis",
    "author": [
      {
        "family": "Branwen",
        "given": "Gwern"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          1,
          4
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2020",
          5,
          28
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://gwern.net/scaling-hypothesis"
  },
  {
    "id": "eT8vyMzx",
    "type": "post-weblog",
    "language": "en-US",
    "title": "A Better Lesson – Rodney Brooks",
    "URL": "https://rodneybrooks.com/a-better-lesson/",
    "accessed": {
      "date-parts": [
        [
          "2024",
          1,
          4
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          3,
          19
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://rodneybrooks.com/a-better-lesson/"
  },
  {
    "id": "AT0GCO31",
    "type": "webpage",
    "abstract": "Thread by @shimon8282: \"Rich Sutton has a new blog post entitled “The Bitter Lesson” (incompleteideas.net/IncIdeas/Bitte…) that I strongly disagreth. In it, he argues that the history of AI teaches us that leveraging computation always eventually wins out over […]\"",
    "language": "en",
    "title": "Thread by @shimon8282: \"Rich Sutton has a new blog post entitled “The Bitter Lesson” (incompleteideas.net/IncIdeas/Bitte…) that I strongly disagree with. In it, he […]\"",
    "title-short": "Thread by @shimon8282",
    "URL": "https://threadreaderapp.com/thread/1106534178676506624.html",
    "author": [
      {
        "literal": "https://twitter.com/shimon8282"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          1,
          4
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://threadreaderapp.com/thread/1106534178676506624.html"
  }
]
