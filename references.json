[
  {
    "id": "12vzXD9U2",
    "title": "Causal Machine Learning: A Survey and Open Problems",
    "author": [
      {
        "given": "Jean",
        "family": "Kaddour"
      },
      {
        "given": "Aengus",
        "family": "Lynch"
      },
      {
        "given": "Qi",
        "family": "Liu"
      },
      {
        "given": "Matt J.",
        "family": "Kusner"
      },
      {
        "given": "Ricardo",
        "family": "Silva"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "URL": "https://arxiv.org/abs/2206.15475",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: causalml",
    "type": "entry"
  },
  {
    "id": "3MP7gokd",
    "title": "DiscoBAX - Discovery of optimal intervention sets in genomic experiment design",
    "author": [
      {
        "given": "Clare",
        "family": "Lyle"
      },
      {
        "given": "Arash",
        "family": "Mehrjou"
      },
      {
        "given": "Pascal",
        "family": "Notin"
      },
      {
        "given": "Andrew",
        "family": "Jesson"
      },
      {
        "given": "Stefan",
        "family": "Bauer"
      },
      {
        "given": "Yarin",
        "family": "Gal"
      },
      {
        "given": "Patrick",
        "family": "Schwab"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "URL": "https://openreview.net/forum?id=mBkUeW8rpD6",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: discobax",
    "type": "entry"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "5",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>In this review, we discuss approaches for learning causal structure from data, also called <jats:italic>causal discovery</jats:italic>. In particular, we focus on approaches for learning directed acyclic graphs and various generalizations which allow for some variables to be unobserved in the available data. We devote special attention to two fundamental combinatorial aspects of causal structure learning. First, we discuss the structure of the search space over causal graphs. Second, we discuss the structure of <jats:italic>equivalence classes</jats:italic> over causal graphs, i.e., sets of graphs which represent what can be learned from observational data alone, and how these equivalence classes can be refined by adding <jats:italic>interventional</jats:italic> data.</jats:p>",
    "DOI": "10.1007/s10208-022-09581-9",
    "type": "article-journal",
    "page": "1781-1815",
    "source": "Crossref",
    "title": "Causal Structure Learning: A Combinatorial Perspective",
    "volume": "23",
    "author": [
      {
        "given": "Chandler",
        "family": "Squires"
      },
      {
        "given": "Caroline",
        "family": "Uhler"
      }
    ],
    "container-title": "Foundations of Computational Mathematics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2022,
          8,
          1
        ]
      ]
    },
    "URL": "https://doi.org/gtdtds",
    "container-title-short": "Found Comput Math",
    "PMCID": "PMC9342837",
    "PMID": "35935470",
    "id": "zmU2wGX7",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1007/s10208-022-09581-9"
  },
  {
    "publisher": "Elsevier BV",
    "issue": "7",
    "DOI": "10.1016/j.cell.2016.11.038",
    "type": "article-journal",
    "page": "1853-1866.e17",
    "source": "Crossref",
    "title": "Perturb-Seq: Dissecting Molecular Circuits with Scalable Single-Cell RNA Profiling of Pooled Genetic Screens",
    "volume": "167",
    "author": [
      {
        "given": "Atray",
        "family": "Dixit"
      },
      {
        "given": "Oren",
        "family": "Parnas"
      },
      {
        "given": "Biyu",
        "family": "Li"
      },
      {
        "given": "Jenny",
        "family": "Chen"
      },
      {
        "given": "Charles P.",
        "family": "Fulco"
      },
      {
        "given": "Livnat",
        "family": "Jerby-Arnon"
      },
      {
        "given": "Nemanja D.",
        "family": "Marjanovic"
      },
      {
        "given": "Danielle",
        "family": "Dionne"
      },
      {
        "given": "Tyler",
        "family": "Burks"
      },
      {
        "given": "Raktima",
        "family": "Raychowdhury"
      },
      {
        "given": "Britt",
        "family": "Adamson"
      },
      {
        "given": "Thomas M.",
        "family": "Norman"
      },
      {
        "given": "Eric S.",
        "family": "Lander"
      },
      {
        "given": "Jonathan S.",
        "family": "Weissman"
      },
      {
        "given": "Nir",
        "family": "Friedman"
      },
      {
        "given": "Aviv",
        "family": "Regev"
      }
    ],
    "container-title": "Cell",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2016,
          12
        ]
      ]
    },
    "URL": "https://doi.org/f9prjd",
    "container-title-short": "Cell",
    "PMCID": "PMC5181115",
    "PMID": "27984732",
    "id": "152yKY5w7",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1016/j.cell.2016.11.038"
  },
  {
    "publisher": "Elsevier BV",
    "issue": "2",
    "DOI": "10.1016/j.cels.2020.11.013",
    "type": "article-journal",
    "page": "128-140.e4",
    "source": "Crossref",
    "title": "CellBox: Interpretable Machine Learning for Perturbation Biology with Application to the Design of Cancer Combination Therapy",
    "volume": "12",
    "author": [
      {
        "given": "Bo",
        "family": "Yuan"
      },
      {
        "given": "Ciyue",
        "family": "Shen"
      },
      {
        "given": "Augustin",
        "family": "Luna"
      },
      {
        "given": "Anil",
        "family": "Korkut"
      },
      {
        "given": "Debora S.",
        "family": "Marks"
      },
      {
        "given": "John",
        "family": "Ingraham"
      },
      {
        "given": "Chris",
        "family": "Sander"
      }
    ],
    "container-title": "Cell Systems",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2021,
          2
        ]
      ]
    },
    "URL": "https://doi.org/ght4v6",
    "container-title-short": "Cell Systems",
    "PMID": "33373583",
    "id": "1Xej0UJj",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1016/j.cels.2020.11.013"
  },
  {
    "publisher": "Elsevier BV",
    "issue": "2",
    "DOI": "10.1016/j.jeconom.2007.05.001",
    "type": "article-journal",
    "page": "615-635",
    "source": "Crossref",
    "title": "Regression discontinuity designs: A guide to practice",
    "volume": "142",
    "author": [
      {
        "given": "Guido W.",
        "family": "Imbens"
      },
      {
        "given": "Thomas",
        "family": "Lemieux"
      }
    ],
    "container-title": "Journal of Econometrics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2008,
          2
        ]
      ]
    },
    "URL": "https://doi.org/bzx7rb",
    "container-title-short": "Journal of Econometrics",
    "id": "1A5MoTSGF",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1016/j.jeconom.2007.05.001"
  },
  {
    "publisher": "Cambridge University Press",
    "abstract": "<jats:p>Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. Cited in more than 2,100 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interest to students and professionals in a wide variety of fields. Dr Judea Pearl has received the 2011 Rumelhart Prize for his leading research in Artificial Intelligence (AI) and systems from The Cognitive Science Society.</jats:p>",
    "DOI": "10.1017/cbo9780511803161",
    "source": "Crossref",
    "title": "Causality",
    "author": [
      {
        "given": "Judea",
        "family": "Pearl"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2009,
          9,
          14
        ]
      ]
    },
    "URL": "https://doi.org/ggd72q",
    "id": "peGxtHPM",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1017/cbo9780511803161",
    "type": "entry"
  },
  {
    "publisher": "American Psychological Association (APA)",
    "issue": "1",
    "DOI": "10.1037/0033-295x.111.1.3",
    "type": "article-journal",
    "page": "3-32",
    "source": "Crossref",
    "title": "A Theory of Causal Learning in Children: Causal Maps and Bayes Nets.",
    "volume": "111",
    "author": [
      {
        "given": "Alison",
        "family": "Gopnik"
      },
      {
        "given": "Clark",
        "family": "Glymour"
      },
      {
        "given": "David M.",
        "family": "Sobel"
      },
      {
        "given": "Laura E.",
        "family": "Schulz"
      },
      {
        "given": "Tamar",
        "family": "Kushnir"
      },
      {
        "given": "David",
        "family": "Danks"
      }
    ],
    "container-title": "Psychological Review",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "URL": "https://doi.org/fkj59s",
    "container-title-short": "Psychological Review",
    "PMID": "14756583",
    "id": "1DMQH5kzt",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1037/0033-295x.111.1.3"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "7971",
    "DOI": "10.1038/d41586-023-02361-7",
    "type": "article-journal",
    "page": "686-689",
    "source": "Crossref",
    "title": "ChatGPT broke the Turing test — the race is on for new ways to assess AI",
    "volume": "619",
    "author": [
      {
        "given": "Celeste",
        "family": "Biever"
      }
    ],
    "container-title": "Nature",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          7,
          25
        ]
      ]
    },
    "URL": "https://doi.org/gskd92",
    "container-title-short": "Nature",
    "PMID": "37491395",
    "id": "19EQh1DNG",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/d41586-023-02361-7"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "7319",
    "DOI": "10.1038/nature09534",
    "type": "article-journal",
    "page": "1061-1073",
    "source": "Crossref",
    "title": "A map of human genome variation from population-scale sequencing",
    "volume": "467",
    "author": [
      {}
    ],
    "container-title": "Nature",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2010,
          10,
          27
        ]
      ]
    },
    "URL": "https://doi.org/fmk7rw",
    "container-title-short": "Nature",
    "PMCID": "PMC3042601",
    "PMID": "20981092",
    "id": "IxBIB6CT",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/nature09534"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "4",
    "DOI": "10.1038/nmeth.3773",
    "type": "article-journal",
    "page": "310-318",
    "source": "Crossref",
    "title": "Inferring causal molecular networks: empirical assessment through a community-based effort",
    "volume": "13",
    "author": [
      {
        "given": "Steven M",
        "family": "Hill"
      },
      {},
      {
        "given": "Laura M",
        "family": "Heiser"
      },
      {
        "given": "Thomas",
        "family": "Cokelaer"
      },
      {
        "given": "Michael",
        "family": "Unger"
      },
      {
        "given": "Nicole K",
        "family": "Nesser"
      },
      {
        "given": "Daniel E",
        "family": "Carlin"
      },
      {
        "given": "Yang",
        "family": "Zhang"
      },
      {
        "given": "Artem",
        "family": "Sokolov"
      },
      {
        "given": "Evan O",
        "family": "Paull"
      },
      {
        "given": "Chris K",
        "family": "Wong"
      },
      {
        "given": "Kiley",
        "family": "Graim"
      },
      {
        "given": "Adrian",
        "family": "Bivol"
      },
      {
        "given": "Haizhou",
        "family": "Wang"
      },
      {
        "given": "Fan",
        "family": "Zhu"
      },
      {
        "given": "Bahman",
        "family": "Afsari"
      },
      {
        "given": "Ludmila V",
        "family": "Danilova"
      },
      {
        "given": "Alexander V",
        "family": "Favorov"
      },
      {
        "given": "Wai Shing",
        "family": "Lee"
      },
      {
        "given": "Dane",
        "family": "Taylor"
      },
      {
        "given": "Chenyue W",
        "family": "Hu"
      },
      {
        "given": "Byron L",
        "family": "Long"
      },
      {
        "given": "David P",
        "family": "Noren"
      },
      {
        "given": "Alexander J",
        "family": "Bisberg"
      },
      {
        "given": "Gordon B",
        "family": "Mills"
      },
      {
        "given": "Joe W",
        "family": "Gray"
      },
      {
        "given": "Michael",
        "family": "Kellen"
      },
      {
        "given": "Thea",
        "family": "Norman"
      },
      {
        "given": "Stephen",
        "family": "Friend"
      },
      {
        "given": "Amina A",
        "family": "Qutub"
      },
      {
        "given": "Elana J",
        "family": "Fertig"
      },
      {
        "given": "Yuanfang",
        "family": "Guan"
      },
      {
        "given": "Mingzhou",
        "family": "Song"
      },
      {
        "given": "Joshua M",
        "family": "Stuart"
      },
      {
        "given": "Paul T",
        "family": "Spellman"
      },
      {
        "given": "Heinz",
        "family": "Koeppl"
      },
      {
        "given": "Gustavo",
        "family": "Stolovitzky"
      },
      {
        "given": "Julio",
        "family": "Saez-Rodriguez"
      },
      {
        "given": "Sach",
        "family": "Mukherjee"
      }
    ],
    "container-title": "Nature Methods",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2016,
          2,
          22
        ]
      ]
    },
    "URL": "https://doi.org/f3t7t4",
    "container-title-short": "Nat Methods",
    "PMCID": "PMC4854847",
    "PMID": "26901648",
    "id": "qpg6x7P4",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/nmeth.3773"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "11",
    "DOI": "10.1038/nrd3847",
    "type": "article-journal",
    "page": "873-886",
    "source": "Crossref",
    "title": "Vemurafenib: the first drug approved for BRAF-mutant cancer",
    "volume": "11",
    "author": [
      {
        "given": "Gideon",
        "family": "Bollag"
      },
      {
        "given": "James",
        "family": "Tsai"
      },
      {
        "given": "Jiazhong",
        "family": "Zhang"
      },
      {
        "given": "Chao",
        "family": "Zhang"
      },
      {
        "given": "Prabha",
        "family": "Ibrahim"
      },
      {
        "given": "Keith",
        "family": "Nolop"
      },
      {
        "given": "Peter",
        "family": "Hirth"
      }
    ],
    "container-title": "Nature Reviews Drug Discovery",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2012,
          10,
          12
        ]
      ]
    },
    "URL": "https://doi.org/f4b975",
    "container-title-short": "Nat Rev Drug Discov",
    "PMID": "23060265",
    "id": "1BSi2Dk6R",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/nrd3847"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "2",
    "DOI": "10.1038/nrg3643",
    "type": "article-journal",
    "page": "107-120",
    "source": "Crossref",
    "title": "Constraint-based models predict metabolic and associated cellular functions",
    "volume": "15",
    "author": [
      {
        "given": "Aarash",
        "family": "Bordbar"
      },
      {
        "given": "Jonathan M.",
        "family": "Monk"
      },
      {
        "given": "Zachary A.",
        "family": "King"
      },
      {
        "given": "Bernhard O.",
        "family": "Palsson"
      }
    ],
    "container-title": "Nature Reviews Genetics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2014,
          1,
          16
        ]
      ]
    },
    "URL": "https://doi.org/f5sk8s",
    "container-title-short": "Nat Rev Genet",
    "PMID": "24430943",
    "id": "94lBUqNp",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/nrg3643"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "3",
    "DOI": "10.1038/nrg3885",
    "type": "article-journal",
    "page": "146-158",
    "source": "Crossref",
    "title": "Quantitative and logic modelling of molecular and gene networks",
    "volume": "16",
    "author": [
      {
        "given": "Nicolas",
        "family": "Le Novère"
      }
    ],
    "container-title": "Nature Reviews Genetics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2015,
          2,
          3
        ]
      ]
    },
    "URL": "https://doi.org/f6299z",
    "container-title-short": "Nat Rev Genet",
    "PMCID": "PMC4604653",
    "PMID": "25645874",
    "id": "s2EFuVEM",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/nrg3885"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "1",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Single-cell RNA-sequencing (scRNA-seq) is a powerful high-throughput technique that enables researchers to measure genome-wide transcription levels at the resolution of single cells. Because of the low amount of RNA present in a single cell, some genes may fail to be detected even though they are expressed; these genes are usually referred to as dropouts. Here, we present a general and flexible zero-inflated negative binomial model (ZINB-WaVE), which leads to low-dimensional representations of the data that account for zero inflation (dropouts), over-dispersion, and the count nature of the data. We demonstrate, with simulated and real data, that the model and its associated estimation procedure are able to give a more stable and accurate low-dimensional representation of the data than principal component analysis (PCA) and zero-inflated factor analysis (ZIFA), without the need for a preliminary normalization step.</jats:p>",
    "DOI": "10.1038/s41467-017-02554-5",
    "type": "article-journal",
    "source": "Crossref",
    "title": "A general and flexible method for signal extraction from single-cell RNA-seq data",
    "volume": "9",
    "author": [
      {
        "given": "Davide",
        "family": "Risso"
      },
      {
        "given": "Fanny",
        "family": "Perraudeau"
      },
      {
        "given": "Svetlana",
        "family": "Gribkova"
      },
      {
        "given": "Sandrine",
        "family": "Dudoit"
      },
      {
        "given": "Jean-Philippe",
        "family": "Vert"
      }
    ],
    "container-title": "Nature Communications",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2018,
          1,
          18
        ]
      ]
    },
    "URL": "https://doi.org/gcv5w7",
    "container-title-short": "Nat Commun",
    "PMCID": "PMC5773593",
    "PMID": "29348443",
    "id": "cKqFMtL2",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41467-017-02554-5"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "1",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Ben Glocker (an expert in machine learning for medical imaging, Imperial College London), Mirco Musolesi (a data science and digital health expert, University College London), Jonathan Richens (an expert in diagnostic machine learning models, Babylon Health) and Caroline Uhler (a computational biology expert, MIT) talked to Nature Communications about their research interests in causality inference and how this can provide a robust framework for digital medicine studies and their implementation, across different fields of application.</jats:p>",
    "DOI": "10.1038/s41467-021-25743-9",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Causality in digital medicine",
    "volume": "12",
    "container-title": "Nature Communications",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2021,
          9,
          15
        ]
      ]
    },
    "URL": "https://doi.org/gtcfvh",
    "container-title-short": "Nat Commun",
    "PMCID": "PMC8443583",
    "PMID": "34526509",
    "id": "17kSbeSiE",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41467-021-25743-9"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "1",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Deep Learning (DL) has recently enabled unprecedented advances in one of the grand challenges in computational biology: the half-century-old problem of protein structure prediction. In this paper we discuss recent advances, limitations, and future perspectives of DL on five broad areas: protein structure prediction, protein function prediction, genome engineering, systems biology and data integration, and phylogenetic inference. We discuss each application area and cover the main bottlenecks of DL approaches, such as training data, problem scope, and the ability to leverage existing DL architectures in new contexts. To conclude, we provide a summary of the subject-specific and general challenges for DL across the biosciences.</jats:p>",
    "DOI": "10.1038/s41467-022-29268-7",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Current progress and open challenges for applying deep learning across the biosciences",
    "volume": "13",
    "author": [
      {
        "given": "Nicolae",
        "family": "Sapoval"
      },
      {
        "given": "Amirali",
        "family": "Aghazadeh"
      },
      {
        "given": "Michael G.",
        "family": "Nute"
      },
      {
        "given": "Dinler A.",
        "family": "Antunes"
      },
      {
        "given": "Advait",
        "family": "Balaji"
      },
      {
        "given": "Richard",
        "family": "Baraniuk"
      },
      {
        "given": "C. J.",
        "family": "Barberan"
      },
      {
        "given": "Ruth",
        "family": "Dannenfelser"
      },
      {
        "given": "Chen",
        "family": "Dun"
      },
      {
        "given": "Mohammadamin",
        "family": "Edrisi"
      },
      {
        "given": "R. A. Leo",
        "family": "Elworth"
      },
      {
        "given": "Bryce",
        "family": "Kille"
      },
      {
        "given": "Anastasios",
        "family": "Kyrillidis"
      },
      {
        "given": "Luay",
        "family": "Nakhleh"
      },
      {
        "given": "Cameron R.",
        "family": "Wolfe"
      },
      {
        "given": "Zhi",
        "family": "Yan"
      },
      {
        "given": "Vicky",
        "family": "Yao"
      },
      {
        "given": "Todd J.",
        "family": "Treangen"
      }
    ],
    "container-title": "Nature Communications",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2022,
          4,
          1
        ]
      ]
    },
    "URL": "https://doi.org/gp26xk",
    "container-title-short": "Nat Commun",
    "PMCID": "PMC8976012",
    "PMID": "35365602",
    "id": "1AZn5l2ah",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41467-022-29268-7"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "1",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Mammalian cells adapt their functional state in response to external signals in form of ligands that bind receptors on the cell-surface. Mechanistically, this involves signal-processing through a complex network of molecular interactions that govern transcription factor activity patterns. Computer simulations of the information flow through this network could help predict cellular responses in health and disease. Here we develop a recurrent neural network framework constrained by prior knowledge of the signaling network with ligand-concentrations as input and transcription factor-activity as output. Applied to synthetic data, it predicts unseen test-data (Pearson correlation<jats:italic>r</jats:italic> = 0.98) and the effects of gene knockouts (<jats:italic>r</jats:italic> = 0.8). We stimulate macrophages with 59 different ligands, with and without the addition of lipopolysaccharide, and collect transcriptomics data. The framework predicts this data under cross-validation (<jats:italic>r</jats:italic> = 0.8) and knockout simulations suggest a role for RIPK1 in modulating the lipopolysaccharide response. This work demonstrates the feasibility of genome-scale simulations of intracellular signaling.</jats:p>",
    "DOI": "10.1038/s41467-022-30684-y",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Artificial neural networks enable genome-scale simulations of intracellular signaling",
    "volume": "13",
    "author": [
      {
        "given": "Avlant",
        "family": "Nilsson"
      },
      {
        "given": "Joshua M.",
        "family": "Peters"
      },
      {
        "given": "Nikolaos",
        "family": "Meimetis"
      },
      {
        "given": "Bryan",
        "family": "Bryson"
      },
      {
        "given": "Douglas A.",
        "family": "Lauffenburger"
      }
    ],
    "container-title": "Nature Communications",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2022,
          6,
          2
        ]
      ]
    },
    "URL": "https://doi.org/gqd9j9",
    "container-title-short": "Nat Commun",
    "PMCID": "PMC9163072",
    "PMID": "35654811",
    "id": "KZ19R8ZY",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41467-022-30684-y"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "1",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Constraint-based metabolic models have been used for decades to predict the phenotype of microorganisms in different environments. However, quantitative predictions are limited unless labor-intensive measurements of media uptake fluxes are performed. We show how hybrid neural-mechanistic models can serve as an architecture for machine learning providing a way to improve phenotype predictions. We illustrate our hybrid models with growth rate predictions of <jats:italic>Escherichia coli</jats:italic> and <jats:italic>Pseudomonas putida</jats:italic> grown in different media and with phenotype predictions of gene knocked-out <jats:italic>Escherichia coli</jats:italic> mutants. Our neural-mechanistic models systematically outperform constraint-based models and require training set sizes orders of magnitude smaller than classical machine learning methods. Our hybrid approach opens a doorway to enhancing constraint-based modeling: instead of constraining mechanistic models with additional experimental measurements, our hybrid models grasp the power of machine learning while fulfilling mechanistic constrains, thus saving time and resources in typical systems biology or biological engineering projects.</jats:p>",
    "DOI": "10.1038/s41467-023-40380-0",
    "type": "article-journal",
    "source": "Crossref",
    "title": "A neural-mechanistic hybrid approach improving the predictive power of genome-scale metabolic models",
    "volume": "14",
    "author": [
      {
        "given": "Léon",
        "family": "Faure"
      },
      {
        "given": "Bastien",
        "family": "Mollet"
      },
      {
        "given": "Wolfram",
        "family": "Liebermeister"
      },
      {
        "given": "Jean-Loup",
        "family": "Faulon"
      }
    ],
    "container-title": "Nature Communications",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          8,
          3
        ]
      ]
    },
    "URL": "https://doi.org/gtb96p",
    "container-title-short": "Nat Commun",
    "PMCID": "PMC10400647",
    "PMID": "37537192",
    "id": "aLhTIs9",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41467-023-40380-0"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "1",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Deep neural networks display impressive performance but suffer from limited interpretability. Biology-inspired deep learning, where the architecture of the computational graph is based on biological knowledge, enables unique interpretability where real-world concepts are encoded in hidden nodes, which can be ranked by importance and thereby interpreted. In such models trained on single-cell transcriptomes, we previously demonstrated that node-level interpretations lack robustness upon repeated training and are influenced by biases in biological knowledge. Similar studies are missing for related models. Here, we test and extend our methodology for reliable interpretability in P-NET, a biology-inspired model trained on patient mutation data. We observe variability of interpretations and susceptibility to knowledge biases, and identify the network properties that drive interpretation biases. We further present an approach to control the robustness and biases of interpretations, which leads to more specific interpretations. In summary, our study reveals the broad importance of methods to ensure robust and bias-aware interpretability in biology-inspired deep learning.</jats:p>",
    "DOI": "10.1038/s41540-023-00310-8",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Reliable interpretability of biology-inspired deep neural networks",
    "volume": "9",
    "author": [
      {
        "given": "Wolfgang",
        "family": "Esser-Skala"
      },
      {
        "given": "Nikolaus",
        "family": "Fortelny"
      }
    ],
    "container-title": "npj Systems Biology and Applications",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          10,
          10
        ]
      ]
    },
    "URL": "https://doi.org/gtb95c",
    "container-title-short": "npj Syst Biol Appl",
    "PMCID": "PMC10564878",
    "PMID": "37816807",
    "id": "3e4s1VLp",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41540-023-00310-8"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>The increasing availability of large-scale single-cell atlases has enabled the detailed description of cell states. In parallel, advances in deep learning allow rapid analysis of newly generated query datasets by mapping them into reference atlases. However, existing data transformations learned to map query data are not easily explainable using biologically known concepts such as genes or pathways. Here we propose expiMap, a biologically informed deep-learning architecture that enables single-cell reference mapping. ExpiMap learns to map cells into biologically understandable components representing known ‘gene programs’. The activity of each cell for a gene program is learned while simultaneously refining them and learning de novo programs. We show that expiMap compares favourably to existing methods while bringing an additional layer of interpretability to integrative single-cell analysis. Furthermore, we demonstrate its applicability to analyse single-cell perturbation responses in different tissues and species and resolve responses of patients who have coronavirus disease 2019 to different treatments across cell types.</jats:p>",
    "DOI": "10.1038/s41556-022-01072-x",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Biologically informed deep learning to query gene programs in single-cell atlases",
    "author": [
      {
        "given": "Mohammad",
        "family": "Lotfollahi"
      },
      {
        "given": "Sergei",
        "family": "Rybakov"
      },
      {
        "given": "Karin",
        "family": "Hrovatin"
      },
      {
        "given": "Soroor",
        "family": "Hediyeh-zadeh"
      },
      {
        "given": "Carlos",
        "family": "Talavera-López"
      },
      {
        "given": "Alexander V.",
        "family": "Misharin"
      },
      {
        "given": "Fabian J.",
        "family": "Theis"
      }
    ],
    "container-title": "Nature Cell Biology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          2,
          2
        ]
      ]
    },
    "URL": "https://doi.org/gtb96q",
    "container-title-short": "Nat Cell Biol",
    "PMCID": "PMC9928587",
    "PMID": "36732632",
    "id": "JkqcQgM7",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41556-022-01072-x"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "7965",
    "DOI": "10.1038/s41586-023-06139-9",
    "type": "article-journal",
    "page": "616-624",
    "source": "Crossref",
    "title": "Transfer learning enables predictions in network biology",
    "volume": "618",
    "author": [
      {
        "given": "Christina V.",
        "family": "Theodoris"
      },
      {
        "given": "Ling",
        "family": "Xiao"
      },
      {
        "given": "Anant",
        "family": "Chopra"
      },
      {
        "given": "Mark D.",
        "family": "Chaffin"
      },
      {
        "given": "Zeina R.",
        "family": "Al Sayed"
      },
      {
        "given": "Matthew C.",
        "family": "Hill"
      },
      {
        "given": "Helene",
        "family": "Mantineo"
      },
      {
        "given": "Elizabeth M.",
        "family": "Brydon"
      },
      {
        "given": "Zexian",
        "family": "Zeng"
      },
      {
        "given": "X. Shirley",
        "family": "Liu"
      },
      {
        "given": "Patrick T.",
        "family": "Ellinor"
      }
    ],
    "container-title": "Nature",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          5,
          31
        ]
      ]
    },
    "URL": "https://doi.org/gr9x63",
    "container-title-short": "Nature",
    "PMID": "37258680",
    "id": "VmzWBJUJ",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41586-023-06139-9"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "3",
    "DOI": "10.1038/s41587-019-0344-3",
    "type": "article-journal",
    "page": "365-373",
    "source": "Crossref",
    "title": "The functional landscape of the human phosphoproteome",
    "volume": "38",
    "author": [
      {
        "given": "David",
        "family": "Ochoa"
      },
      {
        "given": "Andrew F.",
        "family": "Jarnuczak"
      },
      {
        "given": "Cristina",
        "family": "Viéitez"
      },
      {
        "given": "Maja",
        "family": "Gehre"
      },
      {
        "given": "Margaret",
        "family": "Soucheray"
      },
      {
        "given": "André",
        "family": "Mateus"
      },
      {
        "given": "Askar A.",
        "family": "Kleefeldt"
      },
      {
        "given": "Anthony",
        "family": "Hill"
      },
      {
        "given": "Luz",
        "family": "Garcia-Alonso"
      },
      {
        "given": "Frank",
        "family": "Stein"
      },
      {
        "given": "Nevan J.",
        "family": "Krogan"
      },
      {
        "given": "Mikhail M.",
        "family": "Savitski"
      },
      {
        "given": "Danielle L.",
        "family": "Swaney"
      },
      {
        "given": "Juan A.",
        "family": "Vizcaíno"
      },
      {
        "given": "Kyung-Min",
        "family": "Noh"
      },
      {
        "given": "Pedro",
        "family": "Beltrao"
      }
    ],
    "container-title": "Nature Biotechnology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2019,
          12,
          9
        ]
      ]
    },
    "URL": "https://doi.org/ggd8n7",
    "container-title-short": "Nat Biotechnol",
    "PMCID": "PMC7100915",
    "PMID": "31819260",
    "id": "yCFobrFQ",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41587-019-0344-3"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "8",
    "DOI": "10.1038/s41587-023-01848-y",
    "type": "article-journal",
    "page": "1056-1059",
    "source": "Crossref",
    "title": "Democratizing knowledge representation with BioCypher",
    "volume": "41",
    "author": [
      {
        "given": "Sebastian",
        "family": "Lobentanzer"
      },
      {
        "given": "Patrick",
        "family": "Aloy"
      },
      {
        "given": "Jan",
        "family": "Baumbach"
      },
      {
        "given": "Balazs",
        "family": "Bohar"
      },
      {
        "given": "Vincent J.",
        "family": "Carey"
      },
      {
        "given": "Pornpimol",
        "family": "Charoentong"
      },
      {
        "given": "Katharina",
        "family": "Danhauser"
      },
      {
        "given": "Tunca",
        "family": "Doğan"
      },
      {
        "given": "Johann",
        "family": "Dreo"
      },
      {
        "given": "Ian",
        "family": "Dunham"
      },
      {
        "given": "Elias",
        "family": "Farr"
      },
      {
        "given": "Adrià",
        "family": "Fernandez-Torras"
      },
      {
        "given": "Benjamin M.",
        "family": "Gyori"
      },
      {
        "given": "Michael",
        "family": "Hartung"
      },
      {
        "given": "Charles Tapley",
        "family": "Hoyt"
      },
      {
        "given": "Christoph",
        "family": "Klein"
      },
      {
        "given": "Tamas",
        "family": "Korcsmaros"
      },
      {
        "given": "Andreas",
        "family": "Maier"
      },
      {
        "given": "Matthias",
        "family": "Mann"
      },
      {
        "given": "David",
        "family": "Ochoa"
      },
      {
        "given": "Elena",
        "family": "Pareja-Lorente"
      },
      {
        "given": "Ferdinand",
        "family": "Popp"
      },
      {
        "given": "Martin",
        "family": "Preusse"
      },
      {
        "given": "Niklas",
        "family": "Probul"
      },
      {
        "given": "Benno",
        "family": "Schwikowski"
      },
      {
        "given": "Bünyamin",
        "family": "Sen"
      },
      {
        "given": "Maximilian T.",
        "family": "Strauss"
      },
      {
        "given": "Denes",
        "family": "Turei"
      },
      {
        "given": "Erva",
        "family": "Ulusoy"
      },
      {
        "given": "Dagmar",
        "family": "Waltemath"
      },
      {
        "given": "Judith A. H.",
        "family": "Wodke"
      },
      {
        "given": "Julio",
        "family": "Saez-Rodriguez"
      }
    ],
    "container-title": "Nature Biotechnology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          6,
          19
        ]
      ]
    },
    "URL": "https://doi.org/gszqjr",
    "container-title-short": "Nat Biotechnol",
    "PMID": "37337100",
    "id": "tr1XjZ1R",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41587-023-01848-y"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Understanding cellular responses to genetic perturbation is central to numerous biomedical applications, from identifying genetic interactions involved in cancer to developing methods for regenerative medicine. However, the combinatorial explosion in the number of possible multigene perturbations severely limits experimental interrogation. Here, we present graph-enhanced gene activation and repression simulator (GEARS), a method that integrates deep learning with a knowledge graph of gene–gene relationships to predict transcriptional responses to both single and multigene perturbations using single-cell RNA-sequencing data from perturbational screens. GEARS is able to predict outcomes of perturbing combinations consisting of genes that were never experimentally perturbed. GEARS exhibited 40% higher precision than existing approaches in predicting four distinct genetic interaction subtypes in a combinatorial perturbation screen and identified the strongest interactions twice as well as prior approaches. Overall, GEARS can predict phenotypically distinct effects of multigene perturbations and thus guide the design of perturbational experiments.</jats:p>",
    "DOI": "10.1038/s41587-023-01905-6",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Predicting transcriptional outcomes of novel multigene perturbations with GEARS",
    "author": [
      {
        "given": "Yusuf",
        "family": "Roohani"
      },
      {
        "given": "Kexin",
        "family": "Huang"
      },
      {
        "given": "Jure",
        "family": "Leskovec"
      }
    ],
    "container-title": "Nature Biotechnology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          8,
          17
        ]
      ]
    },
    "URL": "https://doi.org/gtb96r",
    "container-title-short": "Nat Biotechnol",
    "PMID": "37592036",
    "id": "4smautjA",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41587-023-01905-6"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "12",
    "DOI": "10.1038/s41592-018-0229-2",
    "type": "article-journal",
    "page": "1053-1058",
    "source": "Crossref",
    "title": "Deep generative modeling for single-cell transcriptomics",
    "volume": "15",
    "author": [
      {
        "given": "Romain",
        "family": "Lopez"
      },
      {
        "given": "Jeffrey",
        "family": "Regier"
      },
      {
        "given": "Michael B.",
        "family": "Cole"
      },
      {
        "given": "Michael I.",
        "family": "Jordan"
      },
      {
        "given": "Nir",
        "family": "Yosef"
      }
    ],
    "container-title": "Nature Methods",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2018,
          11,
          30
        ]
      ]
    },
    "URL": "https://doi.org/gfkw5z",
    "container-title-short": "Nat Methods",
    "PMCID": "PMC6289068",
    "PMID": "30504886",
    "id": "1ELFXHA51",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41592-018-0229-2"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "10",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>How noncoding DNA determines gene expression in different cell types is a major unsolved problem, and critical downstream applications in human genetics depend on improved solutions. Here, we report substantially improved gene expression prediction accuracy from DNA sequences through the use of a deep learning architecture, called Enformer, that is able to integrate information from long-range interactions (up to 100 kb away) in the genome. This improvement yielded more accurate variant effect predictions on gene expression for both natural genetic variants and saturation mutagenesis measured by massively parallel reporter assays. Furthermore, Enformer learned to predict enhancer–promoter interactions directly from the DNA sequence competitively with methods that take direct experimental data as input. We expect that these advances will enable more effective fine-mapping of human disease associations and provide a framework to interpret <jats:italic>cis</jats:italic>-regulatory evolution.</jats:p>",
    "DOI": "10.1038/s41592-021-01252-x",
    "type": "article-journal",
    "page": "1196-1203",
    "source": "Crossref",
    "title": "Effective gene expression prediction from sequence by integrating long-range interactions",
    "volume": "18",
    "author": [
      {
        "given": "Žiga",
        "family": "Avsec"
      },
      {
        "given": "Vikram",
        "family": "Agarwal"
      },
      {
        "given": "Daniel",
        "family": "Visentin"
      },
      {
        "given": "Joseph R.",
        "family": "Ledsam"
      },
      {
        "given": "Agnieszka",
        "family": "Grabska-Barwinska"
      },
      {
        "given": "Kyle R.",
        "family": "Taylor"
      },
      {
        "given": "Yannis",
        "family": "Assael"
      },
      {
        "given": "John",
        "family": "Jumper"
      },
      {
        "given": "Pushmeet",
        "family": "Kohli"
      },
      {
        "given": "David R.",
        "family": "Kelley"
      }
    ],
    "container-title": "Nature Methods",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2021,
          10
        ]
      ]
    },
    "URL": "https://doi.org/gm2wv4",
    "container-title-short": "Nat Methods",
    "PMCID": "PMC8490152",
    "PMID": "34608324",
    "id": "gAQyFCbW",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41592-021-01252-x"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "10",
    "DOI": "10.1038/s41592-021-01283-4",
    "type": "article-journal",
    "page": "1169-1180",
    "source": "Crossref",
    "title": "Differentiable biology: using deep learning for biophysics-based and data-driven modeling of molecular mechanisms",
    "volume": "18",
    "author": [
      {
        "given": "Mohammed",
        "family": "AlQuraishi"
      },
      {
        "given": "Peter K.",
        "family": "Sorger"
      }
    ],
    "container-title": "Nature Methods",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2021,
          10
        ]
      ]
    },
    "URL": "https://doi.org/gm2b58",
    "container-title-short": "Nat Methods",
    "PMCID": "PMC8793939",
    "PMID": "34608321",
    "id": "bMaT0Vyc",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41592-021-01283-4"
  },
  {
    "publisher": "Massachusetts Medical Society",
    "issue": "26",
    "DOI": "10.1056/nejmoa1103782",
    "type": "article-journal",
    "page": "2507-2516",
    "source": "Crossref",
    "title": "Improved Survival with Vemurafenib in Melanoma with BRAF V600E Mutation",
    "volume": "364",
    "author": [
      {
        "given": "Paul B.",
        "family": "Chapman"
      },
      {
        "given": "Axel",
        "family": "Hauschild"
      },
      {
        "given": "Caroline",
        "family": "Robert"
      },
      {
        "given": "John B.",
        "family": "Haanen"
      },
      {
        "given": "Paolo",
        "family": "Ascierto"
      },
      {
        "given": "James",
        "family": "Larkin"
      },
      {
        "given": "Reinhard",
        "family": "Dummer"
      },
      {
        "given": "Claus",
        "family": "Garbe"
      },
      {
        "given": "Alessandro",
        "family": "Testori"
      },
      {
        "given": "Michele",
        "family": "Maio"
      },
      {
        "given": "David",
        "family": "Hogg"
      },
      {
        "given": "Paul",
        "family": "Lorigan"
      },
      {
        "given": "Celeste",
        "family": "Lebbe"
      },
      {
        "given": "Thomas",
        "family": "Jouary"
      },
      {
        "given": "Dirk",
        "family": "Schadendorf"
      },
      {
        "given": "Antoni",
        "family": "Ribas"
      },
      {
        "given": "Steven J.",
        "family": "O'Day"
      },
      {
        "given": "Jeffrey A.",
        "family": "Sosman"
      },
      {
        "given": "John M.",
        "family": "Kirkwood"
      },
      {
        "given": "Alexander M.M.",
        "family": "Eggermont"
      },
      {
        "given": "Brigitte",
        "family": "Dreno"
      },
      {
        "given": "Keith",
        "family": "Nolop"
      },
      {
        "given": "Jiang",
        "family": "Li"
      },
      {
        "given": "Betty",
        "family": "Nelson"
      },
      {
        "given": "Jeannie",
        "family": "Hou"
      },
      {
        "given": "Richard J.",
        "family": "Lee"
      },
      {
        "given": "Keith T.",
        "family": "Flaherty"
      },
      {
        "given": "Grant A.",
        "family": "McArthur"
      }
    ],
    "container-title": "New England Journal of Medicine",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2011,
          6,
          30
        ]
      ]
    },
    "URL": "https://doi.org/dsbxxt",
    "container-title-short": "N Engl J Med",
    "PMCID": "PMC3549296",
    "PMID": "21639808",
    "id": "muFRX2ZL",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1056/nejmoa1103782"
  },
  {
    "publisher": "Proceedings of the National Academy of Sciences",
    "issue": "52",
    "DOI": "10.1073/pnas.2319169120",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Why the simplest explanation isn’t always the best",
    "volume": "120",
    "author": [
      {
        "given": "Eva L.",
        "family": "Dyer"
      },
      {
        "given": "Konrad",
        "family": "Kording"
      }
    ],
    "container-title": "Proceedings of the National Academy of Sciences",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          12,
          20
        ]
      ]
    },
    "URL": "https://doi.org/gtbqms",
    "container-title-short": "Proc. Natl. Acad. Sci. U.S.A.",
    "PMCID": "PMC10756184",
    "PMID": "38117857",
    "id": "AO84A4MA",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1073/pnas.2319169120"
  },
  {
    "publisher": "Informa UK Limited",
    "issue": "434",
    "DOI": "10.1080/01621459.1996.10476902",
    "type": "article-journal",
    "page": "444-455",
    "source": "Crossref",
    "title": "Identification of Causal Effects Using Instrumental Variables",
    "volume": "91",
    "author": [
      {
        "given": "Joshua D.",
        "family": "Angrist"
      },
      {
        "given": "Guido W.",
        "family": "Imbens"
      },
      {
        "given": "Donald B.",
        "family": "Rubin"
      }
    ],
    "container-title": "Journal of the American Statistical Association",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1996,
          6
        ]
      ]
    },
    "URL": "https://doi.org/gdz4f4",
    "container-title-short": "Journal of the American Statistical Association",
    "id": "w47lt0ah",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1080/01621459.1996.10476902"
  },
  {
    "publisher": "The Royal Society",
    "issue": "2266",
    "abstract": "<jats:p>A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopaedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behaviour of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems benefiting from humans’ abilities in terms of flexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.</jats:p>",
    "DOI": "10.1098/rspa.2021.0068",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Inductive biases for deep learning of higher-level cognition",
    "volume": "478",
    "author": [
      {
        "given": "Anirudh",
        "family": "Goyal"
      },
      {
        "given": "Yoshua",
        "family": "Bengio"
      }
    ],
    "container-title": "Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2022,
          10
        ]
      ]
    },
    "URL": "https://doi.org/gs39f8",
    "container-title-short": "Proc. R. Soc. A.",
    "id": "11wELIlTc",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1098/rspa.2021.0068"
  },
  {
    "publisher": "Cold Spring Harbor Laboratory",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Generative pre-trained models have achieved remarkable success in various domains such as natural language processing and computer vision. Specifically, the combination of large-scale diverse datasets and pre-trained transformers has emerged as a promising approach for developing foundation models. Drawing parallels between linguistic constructs and cellular biology — where texts comprise words, similarly, cells are defined by genes — our study probes the applicability of foundation models to advance cellular biology and genetics research. Utilizing the burgeoning single-cell sequencing data, we have pioneered the construction of a foundation model for single-cell biology, scGPT, which is based on generative pre-trained transformer across a repository of over 33 million cells. Our findings illustrate that scGPT, a generative pre-trained transformer, effectively distills critical biological insights concerning genes and cells. Through the further adaptation of transfer learning, scGPT can be optimized to achieve superior performance across diverse downstream applications. This includes tasks such as cell-type annotation, multi-batch integration, multi-omic integration, genetic perturbation prediction, and gene network inference. The scGPT codebase is publicly available at<jats:ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://github.com/bowang-lab/scGPT\">https://github.com/bowang-lab/scGPT</jats:ext-link>.</jats:p>",
    "DOI": "10.1101/2023.04.30.538439",
    "type": "manuscript",
    "source": "Crossref",
    "title": "scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI",
    "author": [
      {
        "given": "Haotian",
        "family": "Cui"
      },
      {
        "given": "Chloe",
        "family": "Wang"
      },
      {
        "given": "Hassaan",
        "family": "Maan"
      },
      {
        "given": "Kuan",
        "family": "Pang"
      },
      {
        "given": "Fengning",
        "family": "Luo"
      },
      {
        "given": "Bo",
        "family": "Wang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023,
          5,
          1
        ]
      ]
    },
    "URL": "https://doi.org/gshk6p",
    "id": "r5y0HbhJ",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1101/2023.04.30.538439"
  },
  {
    "publisher": "Cold Spring Harbor Laboratory",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>The advent and success of foundation models such as GPT has sparked growing interest in their application to single-cell biology. Models like Geneformer and scGPT have emerged with the promise of serving as versatile tools for this specialized field. However, the efficacy of these models, particularly in zero-shot settings where models are not fine-tuned but used without any further training, remains an open question, especially as practical constraints require useful models to function in settings that preclude fine-tuning (e.g., discovery settings where labels are not fully known). This paper presents a rigorous evaluation of the zero-shot performance of these proposed single-cell foundation models. We assess their utility in tasks such as cell type clustering and batch effect correction, and evaluate the generality of their pretraining objectives. Our results indicate that both Geneformer and scGPT exhibit limited reliability in zero-shot settings and often underperform compared to simpler methods. These findings serve as a cautionary note for the deployment of proposed single-cell foundation models and highlight the need for more focused research to realize their potential.<jats:sup>2</jats:sup></jats:p>",
    "DOI": "10.1101/2023.10.16.561085",
    "type": "manuscript",
    "source": "Crossref",
    "title": "Assessing the limits of zero-shot foundation models in single-cell biology",
    "author": [
      {
        "given": "Kasia Z.",
        "family": "Kedzierska"
      },
      {
        "given": "Lorin",
        "family": "Crawford"
      },
      {
        "given": "Ava P.",
        "family": "Amini"
      },
      {
        "given": "Alex X.",
        "family": "Lu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023,
          10,
          17
        ]
      ]
    },
    "URL": "https://doi.org/gszxk9",
    "id": "WEYqVcYG",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1101/2023.10.16.561085"
  },
  {
    "publisher": "Cold Spring Harbor Laboratory",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Large-scale foundation models, which are pre-trained on massive, unlabeled datasets and subsequently fine-tuned on specific tasks, have recently achieved unparalleled success on a wide array of applications, including in healthcare and biology. In this paper, we explore two foundation models recently developed for single-cell RNA sequencing data, scBERT and scGPT. Focusing on the fine-tuning task of cell type annotation, we explore the relative performance of pre-trained models compared to a simple baseline, L1-regularized logistic regression, including in the few-shot setting. We perform ablation studies to understand whether pretraining improves model performance and to better understand the difficulty of the pre-training task in scBERT. Finally, using scBERT as an example, we demonstrate the potential sensitivity of fine-tuning to hyperparameter settings and parameter initializations. Taken together, our results highlight the importance of rigorously testing foundation models against well established baselines, establishing challenging fine-tuning tasks on which to benchmark foundation models, and performing deep introspection into the embeddings learned by the model in order to more effectively harness these models to transform single-cell data analysis. Code is available at<jats:ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://github.com/clinicalml/sc-foundation-eval\">https://github.com/clinicalml/sc-foundation-eval</jats:ext-link>.</jats:p>",
    "DOI": "10.1101/2023.10.19.563100",
    "type": "manuscript",
    "source": "Crossref",
    "title": "A Deep Dive into Single-Cell RNA Sequencing Foundation Models",
    "author": [
      {
        "given": "Rebecca",
        "family": "Boiarsky"
      },
      {
        "given": "Nalini",
        "family": "Singh"
      },
      {
        "given": "Alejandro",
        "family": "Buendia"
      },
      {
        "given": "Gad",
        "family": "Getz"
      },
      {
        "given": "David",
        "family": "Sontag"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023,
          10,
          23
        ]
      ]
    },
    "URL": "https://doi.org/gszxmb",
    "id": "OFczH7ba",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1101/2023.10.19.563100"
  },
  {
    "publisher": "American Physical Society (APS)",
    "issue": "4",
    "DOI": "10.1103/physrevresearch.5.043252",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Model scale versus domain knowledge in statistical forecasting of chaotic systems",
    "volume": "5",
    "author": [
      {
        "given": "William",
        "family": "Gilpin"
      }
    ],
    "container-title": "Physical Review Research",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          12,
          15
        ]
      ]
    },
    "URL": "https://doi.org/gs9xz3",
    "container-title-short": "Phys. Rev. Research",
    "id": "xjVh8IQ5",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1103/physrevresearch.5.043252"
  },
  {
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "issue": "5",
    "DOI": "10.1109/jproc.2021.3058954",
    "type": "article-journal",
    "page": "612-634",
    "source": "Crossref",
    "title": "Toward Causal Representation Learning",
    "volume": "109",
    "author": [
      {
        "given": "Bernhard",
        "family": "Scholkopf"
      },
      {
        "given": "Francesco",
        "family": "Locatello"
      },
      {
        "given": "Stefan",
        "family": "Bauer"
      },
      {
        "given": "Nan Rosemary",
        "family": "Ke"
      },
      {
        "given": "Nal",
        "family": "Kalchbrenner"
      },
      {
        "given": "Anirudh",
        "family": "Goyal"
      },
      {
        "given": "Yoshua",
        "family": "Bengio"
      }
    ],
    "container-title": "Proceedings of the IEEE",
    "issued": {
      "date-parts": [
        [
          2021,
          5
        ]
      ]
    },
    "URL": "https://doi.org/gjhqrh",
    "container-title-short": "Proc. IEEE",
    "id": "T7D6XA6s",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1109/jproc.2021.3058954"
  },
  {
    "publisher": "Wiley",
    "issue": "1",
    "abstract": "<jats:title>SUMMARY</jats:title><jats:p>We propose a new method for estimation in linear models. The ‘lasso’ minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree‐based models are briefly described.</jats:p>",
    "DOI": "10.1111/j.2517-6161.1996.tb02080.x",
    "type": "article-journal",
    "page": "267-288",
    "source": "Crossref",
    "title": "Regression Shrinkage and Selection Via the Lasso",
    "volume": "58",
    "author": [
      {
        "given": "Robert",
        "family": "Tibshirani"
      }
    ],
    "container-title": "Journal of the Royal Statistical Society: Series B (Methodological)",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1996,
          1
        ]
      ]
    },
    "URL": "https://doi.org/gfn45m",
    "container-title-short": "Journal of the Royal Statistical Society: Series B (Methodological)",
    "id": "kX2zf6UE",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1111/j.2517-6161.1996.tb02080.x"
  },
  {
    "publisher": "American Association for the Advancement of Science (AAAS)",
    "issue": "6022",
    "abstract": "<jats:p>In coming to understand the world—in learning concepts, acquiring language, and grasping causal relations—our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?</jats:p>",
    "DOI": "10.1126/science.1192788",
    "type": "article-journal",
    "page": "1279-1285",
    "source": "Crossref",
    "title": "How to Grow a Mind: Statistics, Structure, and Abstraction",
    "volume": "331",
    "author": [
      {
        "given": "Joshua B.",
        "family": "Tenenbaum"
      },
      {
        "given": "Charles",
        "family": "Kemp"
      },
      {
        "given": "Thomas L.",
        "family": "Griffiths"
      },
      {
        "given": "Noah D.",
        "family": "Goodman"
      }
    ],
    "container-title": "Science",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2011,
          3,
          11
        ]
      ]
    },
    "URL": "https://doi.org/d8vvm9",
    "container-title-short": "Science",
    "PMID": "21393536",
    "id": "iWcDZtHu",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1126/science.1192788"
  },
  {
    "publisher": "American Association for the Advancement of Science (AAAS)",
    "issue": "565",
    "abstract": "<jats:p>Identifying the targets of “dark” kinases will provide new biological and disease insights.</jats:p>",
    "DOI": "10.1126/scisignal.aau8645",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Illuminating the dark phosphoproteome",
    "volume": "12",
    "author": [
      {
        "given": "Elise J.",
        "family": "Needham"
      },
      {
        "given": "Benjamin L.",
        "family": "Parker"
      },
      {
        "given": "Timur",
        "family": "Burykin"
      },
      {
        "given": "David E.",
        "family": "James"
      },
      {
        "given": "Sean J.",
        "family": "Humphrey"
      }
    ],
    "container-title": "Science Signaling",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2019,
          1,
          22
        ]
      ]
    },
    "URL": "https://doi.org/gf8c3h",
    "container-title-short": "Sci. Signal.",
    "PMID": "30670635",
    "id": "4VxTzwTj",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1126/scisignal.aau8645"
  },
  {
    "publisher": "Annual Reviews",
    "issue": "1",
    "abstract": "<jats:p> Graphical models can represent a multivariate distribution in a convenient and accessible form as a graph. Causal models can be viewed as a special class of graphical models that represent not only the distribution of the observed system but also the distributions under external interventions. They hence enable predictions under hypothetical interventions, which is important for decision making. The challenging task of learning causal models from data always relies on some underlying assumptions. We discuss several recently proposed structure learning algorithms and their assumptions, and we compare their empirical performance under various scenarios. </jats:p>",
    "DOI": "10.1146/annurev-statistics-031017-100630",
    "type": "article-journal",
    "page": "371-391",
    "source": "Crossref",
    "title": "Causal Structure Learning",
    "volume": "5",
    "author": [
      {
        "given": "Christina",
        "family": "Heinze-Deml"
      },
      {
        "given": "Marloes H.",
        "family": "Maathuis"
      },
      {
        "given": "Nicolai",
        "family": "Meinshausen"
      }
    ],
    "container-title": "Annual Review of Statistics and Its Application",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2018,
          3,
          7
        ]
      ]
    },
    "URL": "https://doi.org/ggh4pj",
    "container-title-short": "Annu. Rev. Stat. Appl.",
    "id": "1CPlHia5R",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1146/annurev-statistics-031017-100630"
  },
  {
    "publisher": "MIT Press - Journals",
    "issue": "4",
    "abstract": "<jats:p> The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification. </jats:p>",
    "DOI": "10.1162/neco.1989.1.4.541",
    "type": "article-journal",
    "page": "541-551",
    "source": "Crossref",
    "title": "Backpropagation Applied to Handwritten Zip Code Recognition",
    "volume": "1",
    "author": [
      {
        "given": "Y.",
        "family": "LeCun"
      },
      {
        "given": "B.",
        "family": "Boser"
      },
      {
        "given": "J. S.",
        "family": "Denker"
      },
      {
        "given": "D.",
        "family": "Henderson"
      },
      {
        "given": "R. E.",
        "family": "Howard"
      },
      {
        "given": "W.",
        "family": "Hubbard"
      },
      {
        "given": "L. D.",
        "family": "Jackel"
      }
    ],
    "container-title": "Neural Computation",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1989,
          12
        ]
      ]
    },
    "URL": "https://doi.org/bknd8g",
    "container-title-short": "Neural Computation",
    "id": "bZ3hxYDX",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1162/neco.1989.1.4.541"
  },
  {
    "publisher": "MIT Press - Journals",
    "issue": "8",
    "abstract": "<jats:p> Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms. </jats:p>",
    "DOI": "10.1162/neco.1997.9.8.1735",
    "type": "article-journal",
    "page": "1735-1780",
    "source": "Crossref",
    "title": "Long Short-Term Memory",
    "volume": "9",
    "author": [
      {
        "given": "Sepp",
        "family": "Hochreiter"
      },
      {
        "given": "Jürgen",
        "family": "Schmidhuber"
      }
    ],
    "container-title": "Neural Computation",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1997,
          11,
          1
        ]
      ]
    },
    "URL": "https://doi.org/bxd65w",
    "container-title-short": "Neural Computation",
    "PMID": "9377276",
    "id": "x4dbEYer",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1162/neco.1997.9.8.1735"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "1",
    "abstract": "<jats:title>Abstract</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>Deep learning has emerged as a versatile approach for predicting complex biological phenomena. However, its utility for biological discovery has so far been limited, given that generic deep neural networks provide little insight into the biological mechanisms that underlie a successful prediction. Here we demonstrate deep learning on biological networks, where every node has a molecular equivalent, such as a protein or gene, and every edge has a mechanistic interpretation, such as a regulatory interaction along a signaling pathway.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>With knowledge-primed neural networks (KPNNs), we exploit the ability of deep learning algorithms to assign meaningful weights in multi-layered networks, resulting in a widely applicable approach for interpretable deep learning. We present a learning method that enhances the interpretability of trained KPNNs by stabilizing node weights in the presence of redundancy, enhancing the quantitative interpretability of node weights, and controlling for uneven connectivity in biological networks. We validate KPNNs on simulated data with known ground truth and demonstrate their practical use and utility in five biological applications with single-cell RNA-seq data for cancer and immune cells.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>We introduce KPNNs as a method that combines the predictive power of deep learning with the interpretability of biological networks. While demonstrated here on single-cell sequencing data, this method is broadly relevant to other research areas where prior domain knowledge can be represented as networks.</jats:p></jats:sec>",
    "DOI": "10.1186/s13059-020-02100-5",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Knowledge-primed neural networks enable biologically interpretable deep learning on single-cell sequencing data",
    "volume": "21",
    "author": [
      {
        "given": "Nikolaus",
        "family": "Fortelny"
      },
      {
        "given": "Christoph",
        "family": "Bock"
      }
    ],
    "container-title": "Genome Biology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2020,
          8,
          3
        ]
      ]
    },
    "URL": "https://doi.org/gg8ws9",
    "container-title-short": "Genome Biol",
    "PMCID": "PMC7397672",
    "PMID": "32746932",
    "id": "YOm0bqVR",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1186/s13059-020-02100-5"
  },
  {
    "publisher": "Institute of Mathematical Statistics",
    "issue": "none",
    "DOI": "10.1214/09-ss057",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Causal inference in statistics: An overview",
    "volume": "3",
    "author": [
      {
        "given": "Judea",
        "family": "Pearl"
      }
    ],
    "container-title": "Statistics Surveys",
    "issued": {
      "date-parts": [
        [
          2009,
          1,
          1
        ]
      ]
    },
    "URL": "https://doi.org/drt748",
    "container-title-short": "Statist. Surv.",
    "id": "dJUlTjFg",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1214/09-ss057"
  },
  {
    "publisher": "Public Library of Science (PLoS)",
    "issue": "8",
    "abstract": "<jats:p>Dimensionality reduction is standard practice for filtering noise and identifying relevant features in large-scale data analyses. In biology, single-cell genomics studies typically begin with reduction to 2 or 3 dimensions to produce “all-in-one” visuals of the data that are amenable to the human eye, and these are subsequently used for qualitative and quantitative exploratory analysis. However, there is little theoretical support for this practice, and we show that extreme dimension reduction, from hundreds or thousands of dimensions to 2, inevitably induces significant distortion of high-dimensional datasets. We therefore examine the practical implications of low-dimensional embedding of single-cell data and find that extensive distortions and inconsistent practices make such embeddings counter-productive for exploratory, biological analyses. In lieu of this, we discuss alternative approaches for conducting targeted embedding and feature exploration to enable hypothesis-driven biological discovery.</jats:p>",
    "DOI": "10.1371/journal.pcbi.1011288",
    "type": "article-journal",
    "page": "e1011288",
    "source": "Crossref",
    "title": "The specious art of single-cell genomics",
    "volume": "19",
    "author": [
      {
        "given": "Tara",
        "family": "Chari"
      },
      {
        "given": "Lior",
        "family": "Pachter"
      }
    ],
    "container-title": "PLOS Computational Biology",
    "language": "en",
    "editor": [
      {
        "given": "Jason A.",
        "family": "Papin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023,
          8,
          17
        ]
      ]
    },
    "URL": "https://doi.org/gtb96t",
    "container-title-short": "PLoS Comput Biol",
    "PMCID": "PMC10434946",
    "PMID": "37590228",
    "id": "6McXkHVo",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1371/journal.pcbi.1011288"
  },
  {
    "publisher": "Public Library of Science (PLoS)",
    "issue": "11",
    "DOI": "10.1371/journal.pone.0027755",
    "type": "article-journal",
    "page": "e27755",
    "source": "Crossref",
    "title": "Structural Identifiability of Systems Biology Models: A Critical Comparison of Methods",
    "volume": "6",
    "author": [
      {
        "given": "Oana-Teodora",
        "family": "Chis"
      },
      {
        "given": "Julio R.",
        "family": "Banga"
      },
      {
        "given": "Eva",
        "family": "Balsa-Canto"
      }
    ],
    "container-title": "PLoS ONE",
    "language": "en",
    "editor": [
      {
        "given": "Johannes",
        "family": "Jaeger"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2011,
          11,
          22
        ]
      ]
    },
    "URL": "https://doi.org/fch6rc",
    "container-title-short": "PLoS ONE",
    "PMCID": "PMC3222653",
    "PMID": "22132135",
    "id": "15RkhPQiZ",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1371/journal.pone.0027755"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "7",
    "DOI": "10.15252/msb.202211036",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Integrating knowledge and omics to decipher mechanisms via large‐scale models of signaling networks",
    "volume": "18",
    "author": [
      {
        "given": "Martin",
        "family": "Garrido‐Rodriguez"
      },
      {
        "given": "Katharina",
        "family": "Zirngibl"
      },
      {
        "given": "Olga",
        "family": "Ivanova"
      },
      {
        "given": "Sebastian",
        "family": "Lobentanzer"
      },
      {
        "given": "Julio",
        "family": "Saez‐Rodriguez"
      }
    ],
    "container-title": "Molecular Systems Biology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2022,
          7
        ]
      ]
    },
    "URL": "https://doi.org/gtb9v8",
    "container-title-short": "Molecular Systems Biology",
    "PMCID": "PMC9316933",
    "PMID": "35880747",
    "id": "TUhGg6tD",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.15252/msb.202211036"
  },
  {
    "publisher": "AI Access Foundation",
    "abstract": "<jats:p>A major problem in machine learning is that of inductive    bias: how to choose a learner's hypothesis space so that it is large    enough to contain a solution to the problem being learnt, yet small    enough to ensure reliable generalization from reasonably-sized    training sets.  Typically such bias is supplied by hand through the    skill and insights of experts. In this paper a model for automatically    learning bias is investigated. The central assumption of the model is    that the learner is embedded within an environment of related learning    tasks. Within such an environment the learner can sample from multiple    tasks, and hence it can search for a hypothesis space that contains    good solutions to many of the problems in the environment. Under    certain restrictions on the set of all hypothesis spaces available to    the learner, we show that a hypothesis space that performs well on a    sufficiently large number of training tasks will also perform well    when learning novel tasks in the same environment.  Explicit bounds    are also derived demonstrating that learning multiple tasks within an    environment of related tasks can potentially give much better    generalization than learning a single task.</jats:p>",
    "DOI": "10.1613/jair.731",
    "type": "article-journal",
    "page": "149-198",
    "source": "Crossref",
    "title": "A Model of Inductive Bias Learning",
    "volume": "12",
    "author": [
      {
        "given": "J.",
        "family": "Baxter"
      }
    ],
    "container-title": "Journal of Artificial Intelligence Research",
    "issued": {
      "date-parts": [
        [
          2000,
          3,
          1
        ]
      ]
    },
    "URL": "https://doi.org/gg66h8",
    "container-title-short": "jair",
    "id": "2RptKLT2",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1613/jair.731"
  },
  {
    "publisher": "MDPI AG",
    "issue": "7",
    "abstract": "<jats:p>The lack of interpretability in artificial intelligence models (i.e., deep learning, machine learning, and rules-based) is an obstacle to their widespread adoption in the healthcare domain. The absence of understandability and transparency frequently leads to (i) inadequate accountability and (ii) a consequent reduction in the quality of the predictive results of the models. On the other hand, the existence of interpretability in the predictions of AI models will facilitate the understanding and trust of the clinicians in these complex models. The data protection regulations worldwide emphasize the relevance of the plausibility and verifiability of AI models’ predictions. In response and to take a role in tackling this challenge, we designed the interpretability-based model with algorithms that achieve human-like reasoning abilities through statistical analysis of the datasets by calculating the relative weights of the variables of the features from the medical images and the patient symptoms. The relative weights represented the importance of the variables in predictive decision-making. In addition, the relative weights were used to find the positive and negative probabilities of having the disease, which indicated high fidelity explanations. Hence, the primary goal of our model is to shed light and give insights into the prediction process of the models, as well as to explain how the model predictions have resulted. Consequently, our model contributes by demonstrating accuracy. Furthermore, two experiments on COVID-19 datasets demonstrated the effectiveness and interpretability of the new model.</jats:p>",
    "DOI": "10.3390/diagnostics12071557",
    "type": "article-journal",
    "page": "1557",
    "source": "Crossref",
    "title": "Designing an Interpretability-Based Model to Explain the Artificial Intelligence Algorithms in Healthcare",
    "volume": "12",
    "author": [
      {
        "given": "Mohammad",
        "family": "Ennab"
      },
      {
        "given": "Hamid",
        "family": "Mcheick"
      }
    ],
    "container-title": "Diagnostics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2022,
          6,
          26
        ]
      ]
    },
    "URL": "https://doi.org/gtdf94",
    "container-title-short": "Diagnostics",
    "PMCID": "PMC9319389",
    "PMID": "35885463",
    "id": "15hYXC3QB",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.3390/diagnostics12071557"
  },
  {
    "publisher": "MDPI AG",
    "issue": "6",
    "abstract": "<jats:p>The discovery of the role of the RAS/RAF/MEK/ERK pathway in melanomagenesis and its progression have opened a new era in the treatment of this tumor. Vemurafenib was the first specific kinase inhibitor approved for therapy of advanced melanomas harboring BRAF-activating mutations, followed by dabrafenib and encorafenib. However, despite the excellent results of first-generation kinase inhibitors in terms of response rate, the average duration of the response was short, due to the onset of genetic and epigenetic resistance mechanisms. The combination therapy with MEK inhibitors is an excellent strategy to circumvent drug resistance, with the additional advantage of reducing side effects due to the paradoxical reactivation of the MAPK pathway. The recent development of RAS and extracellular signal-related kinases (ERK) inhibitors promises to add new players for the ultimate suppression of this signaling pathway and the control of pathway-related drug resistance. In this review, we analyze the pharmacological, preclinical, and clinical trial data of the various MAPK pathway inhibitors, with a keen interest for their clinical applicability in the management of advanced melanoma.</jats:p>",
    "DOI": "10.3390/ijms20061483",
    "type": "article-journal",
    "page": "1483",
    "source": "Crossref",
    "title": "Targeting the ERK Signaling Pathway in Melanoma",
    "volume": "20",
    "author": [
      {
        "given": "Paola",
        "family": "Savoia"
      },
      {
        "given": "Paolo",
        "family": "Fava"
      },
      {
        "given": "Filippo",
        "family": "Casoni"
      },
      {
        "given": "Ottavio",
        "family": "Cremona"
      }
    ],
    "container-title": "International Journal of Molecular Sciences",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2019,
          3,
          25
        ]
      ]
    },
    "URL": "https://doi.org/gtb9v9",
    "container-title-short": "IJMS",
    "PMCID": "PMC6472057",
    "PMID": "30934534",
    "id": "ZkFzi3Xl",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.3390/ijms20061483"
  },
  {
    "publisher": "Scientific Research Publishing, Inc.",
    "issue": "03",
    "DOI": "10.4236/jdaip.2021.93013",
    "type": "article-journal",
    "page": "189-231",
    "source": "Crossref",
    "title": "Review of Dimension Reduction Methods",
    "volume": "09",
    "author": [
      {
        "given": "Salifu",
        "family": "Nanga"
      },
      {
        "given": "Ahmed Tijani",
        "family": "Bawah"
      },
      {
        "given": "Benjamin Ansah",
        "family": "Acquaye"
      },
      {
        "given": "Mac-Issaka",
        "family": "Billa"
      },
      {
        "given": "Francis Delali",
        "family": "Baeta"
      },
      {
        "given": "Nii Afotey",
        "family": "Odai"
      },
      {
        "given": "Samuel Kwaku",
        "family": "Obeng"
      },
      {
        "given": "Ampem Darko",
        "family": "Nsiah"
      }
    ],
    "container-title": "Journal of Data Analysis and Information Processing",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "URL": "https://doi.org/gtb96v",
    "container-title-short": "JDAIP",
    "id": "13qoNo4Fj",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.4236/jdaip.2021.93013"
  },
  {
    "type": "article",
    "id": "S1VP202R",
    "categories": [
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Eberhardt",
        "given": "Frederick"
      },
      {
        "family": "Glymour",
        "given": "Clark"
      },
      {
        "family": "Scheines",
        "given": "Richard"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "abstract": "We show that if any number of variables are allowed to be simultaneously and independently randomized in any one experiment, log2(N) + 1 experiments are sufficient and in the worst case necessary to determine the causal relations among N &gt;= 2 variables when no latent variables, no sample selection bias and no feedback cycles are present. For all K, 0 &lt; K &lt; 1/(2N) we provide an upper bound on the number experiments required to determine causal structure when each experiment simultaneously randomizes K variables. For large N, these bounds are significantly lower than the N - 1 bound required when each experiment randomizes at most one variable. For kmax &lt; N/2, we show that (N/kmax-1)+N/(2kmax)log2(kmax) experiments aresufficient and in the worst case necessary. We over a conjecture as to the minimal number of experiments that are in the worst case sufficient to identify all causal relations among N observed variables that are a subset of the vertices of a DAG.",
    "DOI": "10.48550/arxiv.1207.1389",
    "publisher": "arXiv",
    "title": "On the Number of Experiments Sufficient and in the Worst Case Necessary to Identify All Causal Relations Among N Variables",
    "URL": "https://doi.org/gtb997",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1207.1389"
  },
  {
    "type": "article",
    "id": "12GOinshI",
    "categories": [
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Pearl",
        "given": "Judea"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "abstract": "The do-calculus was developed in 1995 to facilitate the identification of causal effects in non-parametric models. The completeness proofs of [Huang and Valtorta, 2006] and [Shpitser and Pearl, 2006] and the graphical criteria of [Tian and Shpitser, 2010] have laid this identification problem to rest. Recent explorations unveil the usefulness of the do-calculus in three additional areas: mediation analysis [Pearl, 2012], transportability [Pearl and Bareinboim, 2011] and metasynthesis. Meta-synthesis (freshly coined) is the task of fusing empirical results from several diverse studies, conducted on heterogeneous populations and under different conditions, so as to synthesize an estimate of a causal relation in some target environment, potentially different from those under study. The talk surveys these results with emphasis on the challenges posed by meta-synthesis. For background material, see http://bayes.cs.ucla.edu/csl_papers.html",
    "DOI": "10.48550/arxiv.1210.4852",
    "publisher": "arXiv",
    "title": "The Do-Calculus Revisited",
    "URL": "https://doi.org/gtbf4r",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1210.4852"
  },
  {
    "type": "article",
    "id": "rh7nCPVE",
    "categories": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Vaswani",
        "given": "Ashish"
      },
      {
        "family": "Shazeer",
        "given": "Noam"
      },
      {
        "family": "Parmar",
        "given": "Niki"
      },
      {
        "family": "Uszkoreit",
        "given": "Jakob"
      },
      {
        "family": "Jones",
        "given": "Llion"
      },
      {
        "family": "Gomez",
        "given": "Aidan N."
      },
      {
        "family": "Kaiser",
        "given": "Lukasz"
      },
      {
        "family": "Polosukhin",
        "given": "Illia"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "DOI": "10.48550/arxiv.1706.03762",
    "publisher": "arXiv",
    "title": "Attention Is All You Need",
    "URL": "https://doi.org/gpnmtv",
    "version": "7",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1706.03762"
  },
  {
    "type": "article-journal",
    "id": "URCTSFCA",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Locatello",
        "given": "Francesco"
      },
      {
        "family": "Bauer",
        "given": "Stefan"
      },
      {
        "family": "Lucic",
        "given": "Mario"
      },
      {
        "family": "Rätsch",
        "given": "Gunnar"
      },
      {
        "family": "Gelly",
        "given": "Sylvain"
      },
      {
        "family": "Schölkopf",
        "given": "Bernhard"
      },
      {
        "family": "Bachem",
        "given": "Olivier"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "abstract": "The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties ``encouraged'' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.",
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.1811.12359",
    "publisher": "arXiv",
    "title": "Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations",
    "URL": "https://doi.org/grx79c",
    "version": "4",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1811.12359"
  },
  {
    "type": "article",
    "id": "B5WSzZkm",
    "categories": [
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Mehrabi",
        "given": "Ninareh"
      },
      {
        "family": "Morstatter",
        "given": "Fred"
      },
      {
        "family": "Saxena",
        "given": "Nripsuta"
      },
      {
        "family": "Lerman",
        "given": "Kristina"
      },
      {
        "family": "Galstyan",
        "given": "Aram"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "abstract": "With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",
    "DOI": "10.48550/arxiv.1908.09635",
    "publisher": "arXiv",
    "title": "A Survey on Bias and Fairness in Machine Learning",
    "URL": "https://doi.org/gtb9wn",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1908.09635"
  },
  {
    "type": "article",
    "id": "7HxYpmt4",
    "categories": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Ke",
        "given": "Nan Rosemary"
      },
      {
        "family": "Bilaniuk",
        "given": "Olexa"
      },
      {
        "family": "Goyal",
        "given": "Anirudh"
      },
      {
        "family": "Bauer",
        "given": "Stefan"
      },
      {
        "family": "Larochelle",
        "given": "Hugo"
      },
      {
        "family": "Schölkopf",
        "given": "Bernhard"
      },
      {
        "family": "Mozer",
        "given": "Michael C."
      },
      {
        "family": "Pal",
        "given": "Chris"
      },
      {
        "family": "Bengio",
        "given": "Yoshua"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "abstract": "Promising results have driven a recent surge of interest in continuous optimization methods for Bayesian network structure learning from observational data. However, there are theoretical limitations on the identifiability of underlying structures obtained from observational data alone. Interventional data provides much richer information about the underlying data-generating process. However, the extension and application of methods designed for observational data to include interventions is not straightforward and remains an open problem. In this paper we provide a general framework based on continuous optimization and neural networks to create models for the combination of observational and interventional data. The proposed method is even applicable in the challenging and realistic case that the identity of the intervened upon variable is unknown. We examine the proposed method in the setting of graph recovery both de novo and from a partially-known edge set. We establish strong benchmark results on several structure learning tasks, including structure recovery of both synthetic graphs as well as standard graphs from the Bayesian Network Repository.",
    "DOI": "10.48550/arxiv.1910.01075",
    "publisher": "arXiv",
    "title": "Learning Neural Causal Models from Unknown Interventions",
    "URL": "https://doi.org/grw6nc",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1910.01075"
  },
  {
    "type": "article",
    "id": "6W1y3ZrT",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Kaplan",
        "given": "Jared"
      },
      {
        "family": "McCandlish",
        "given": "Sam"
      },
      {
        "family": "Henighan",
        "given": "Tom"
      },
      {
        "family": "Brown",
        "given": "Tom B."
      },
      {
        "family": "Chess",
        "given": "Benjamin"
      },
      {
        "family": "Child",
        "given": "Rewon"
      },
      {
        "family": "Gray",
        "given": "Scott"
      },
      {
        "family": "Radford",
        "given": "Alec"
      },
      {
        "family": "Wu",
        "given": "Jeffrey"
      },
      {
        "family": "Amodei",
        "given": "Dario"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "abstract": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
    "DOI": "10.48550/arxiv.2001.08361",
    "publisher": "arXiv",
    "title": "Scaling Laws for Neural Language Models",
    "URL": "https://doi.org/gtb96w",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2001.08361"
  },
  {
    "type": "article",
    "id": "vm2M7mI5",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Aliee",
        "given": "Hananeh"
      },
      {
        "family": "Theis",
        "given": "Fabian J."
      },
      {
        "family": "Kilbertus",
        "given": "Niki"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "abstract": "Spurred by tremendous success in pattern matching and prediction tasks, researchers increasingly resort to machine learning to aid original scientific discovery. Given large amounts of observational data about a system, can we uncover the rules that govern its evolution? Solving this task holds the great promise of fully understanding the causal interactions and being able to make reliable predictions about the system's behavior under interventions. We take a step towards answering this question for time-series data generated from systems of ordinary differential equations (ODEs). While the governing ODEs might not be identifiable from data alone, we show that combining simple regularization schemes with flexible neural ODEs can robustly recover the dynamics and causal structures from time-series data. Our results on a variety of (non)-linear first and second order systems as well as real data validate our method. We conclude by showing that we can also make accurate predictions under interventions on variables or the system itself.",
    "DOI": "10.48550/arxiv.2106.12430",
    "publisher": "arXiv",
    "title": "Beyond Predictions in Neural ODEs: Identification and Interventions",
    "URL": "https://doi.org/gszw4d",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2106.12430"
  },
  {
    "type": "article",
    "id": "r5mjeIhV",
    "categories": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Leeb",
        "given": "Felix"
      },
      {
        "family": "Bauer",
        "given": "Stefan"
      },
      {
        "family": "Besserve",
        "given": "Michel"
      },
      {
        "family": "Schölkopf",
        "given": "Bernhard"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "abstract": "Autoencoders exhibit impressive abilities to embed the data manifold into a low-dimensional latent space, making them a staple of representation learning methods. However, without explicit supervision, which is often unavailable, the representation is usually uninterpretable, making analysis and principled progress challenging. We propose a framework, called latent responses, which exploits the locally contractive behavior exhibited by variational autoencoders to explore the learned manifold. More specifically, we develop tools to probe the representation using interventions in the latent space to quantify the relationships between latent variables. We extend the notion of disentanglement to take the learned generative process into account and consequently avoid the limitations of existing metrics that may rely on spurious correlations. Our analyses underscore the importance of studying the causal structure of the representation to improve performance on downstream tasks such as generation, interpolation, and inference of the factors of variation.",
    "DOI": "10.48550/arxiv.2106.16091",
    "publisher": "arXiv",
    "title": "Exploring the Latent Space of Autoencoders with Interventional Assays",
    "URL": "https://doi.org/gtb96x",
    "version": "4",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2106.16091"
  },
  {
    "type": "article",
    "id": "eEfUqiI4",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Notin",
        "given": "Pascal"
      },
      {
        "family": "Hernández-Lobato",
        "given": "José Miguel"
      },
      {
        "family": "Gal",
        "given": "Yarin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "abstract": "Optimization in the latent space of variational autoencoders is a promising approach to generate high-dimensional discrete objects that maximize an expensive black-box property (e.g., drug-likeness in molecular generation, function approximation with arithmetic expressions). However, existing methods lack robustness as they may decide to explore areas of the latent space for which no data was available during training and where the decoder can be unreliable, leading to the generation of unrealistic or invalid objects. We propose to leverage the epistemic uncertainty of the decoder to guide the optimization process. This is not trivial though, as a naive estimation of uncertainty in the high-dimensional and structured settings we consider would result in high estimator variance. To solve this problem, we introduce an importance sampling-based estimator that provides more robust estimates of epistemic uncertainty. Our uncertainty-guided optimization approach does not require modifications of the model architecture nor the training process. It produces samples with a better trade-off between black-box objective and validity of the generated samples, sometimes improving both simultaneously. We illustrate these advantages across several experimental settings in digit generation, arithmetic expression approximation and molecule generation for drug design.",
    "DOI": "10.48550/arxiv.2107.00096",
    "publisher": "arXiv",
    "title": "Improving black-box optimization in VAE latent space using decoder uncertainty",
    "URL": "https://doi.org/gtb962",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2107.00096"
  },
  {
    "type": "article",
    "id": "FVLAWGsX",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Xia",
        "given": "Kevin"
      },
      {
        "family": "Lee",
        "given": "Kai-Zhan"
      },
      {
        "family": "Bengio",
        "given": "Yoshua"
      },
      {
        "family": "Bareinboim",
        "given": "Elias"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "abstract": "One of the central elements of any causal inference is an object called structural causal model (SCM), which represents a collection of mechanisms and exogenous sources of random variation of the system under investigation (Pearl, 2000). An important property of many kinds of neural networks is universal approximability: the ability to approximate any function to arbitrary precision. Given this property, one may be tempted to surmise that a collection of neural nets is capable of learning any SCM by training on data generated by that SCM. In this paper, we show this is not the case by disentangling the notions of expressivity and learnability. Specifically, we show that the causal hierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits of what can be learned from data, still holds for neural models. For instance, an arbitrarily complex and expressive neural net is unable to predict the effects of interventions given observational data alone. Given this result, we introduce a special type of SCM called a neural causal model (NCM), and formalize a new type of inductive bias to encode structural constraints necessary for performing causal inferences. Building on this new class of models, we focus on solving two canonical tasks found in the literature known as causal identification and estimation. Leveraging the neural toolbox, we develop an algorithm that is both sufficient and necessary to determine whether a causal effect can be learned from data (i.e., causal identifiability); it then estimates the effect whenever identifiability holds (causal estimation). Simulations corroborate the proposed approach.",
    "DOI": "10.48550/arxiv.2107.00793",
    "publisher": "arXiv",
    "title": "The Causal-Neural Connection: Expressiveness, Learnability, and Inference",
    "URL": "https://doi.org/grw6m7",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2107.00793"
  },
  {
    "type": "article",
    "id": "1GbAsSOZV",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Bommasani",
        "given": "Rishi"
      },
      {
        "family": "Hudson",
        "given": "Drew A."
      },
      {
        "family": "Adeli",
        "given": "Ehsan"
      },
      {
        "family": "Altman",
        "given": "Russ"
      },
      {
        "family": "Arora",
        "given": "Simran"
      },
      {
        "family": "von Arx",
        "given": "Sydney"
      },
      {
        "family": "Bernstein",
        "given": "Michael S."
      },
      {
        "family": "Bohg",
        "given": "Jeannette"
      },
      {
        "family": "Bosselut",
        "given": "Antoine"
      },
      {
        "family": "Brunskill",
        "given": "Emma"
      },
      {
        "family": "Brynjolfsson",
        "given": "Erik"
      },
      {
        "family": "Buch",
        "given": "Shyamal"
      },
      {
        "family": "Card",
        "given": "Dallas"
      },
      {
        "family": "Castellon",
        "given": "Rodrigo"
      },
      {
        "family": "Chatterji",
        "given": "Niladri"
      },
      {
        "family": "Chen",
        "given": "Annie"
      },
      {
        "family": "Creel",
        "given": "Kathleen"
      },
      {
        "family": "Davis",
        "given": "Jared Quincy"
      },
      {
        "family": "Demszky",
        "given": "Dora"
      },
      {
        "family": "Donahue",
        "given": "Chris"
      },
      {
        "family": "Doumbouya",
        "given": "Moussa"
      },
      {
        "family": "Durmus",
        "given": "Esin"
      },
      {
        "family": "Ermon",
        "given": "Stefano"
      },
      {
        "family": "Etchemendy",
        "given": "John"
      },
      {
        "family": "Ethayarajh",
        "given": "Kawin"
      },
      {
        "family": "Fei-Fei",
        "given": "Li"
      },
      {
        "family": "Finn",
        "given": "Chelsea"
      },
      {
        "family": "Gale",
        "given": "Trevor"
      },
      {
        "family": "Gillespie",
        "given": "Lauren"
      },
      {
        "family": "Goel",
        "given": "Karan"
      },
      {
        "family": "Goodman",
        "given": "Noah"
      },
      {
        "family": "Grossman",
        "given": "Shelby"
      },
      {
        "family": "Guha",
        "given": "Neel"
      },
      {
        "family": "Hashimoto",
        "given": "Tatsunori"
      },
      {
        "family": "Henderson",
        "given": "Peter"
      },
      {
        "family": "Hewitt",
        "given": "John"
      },
      {
        "family": "Ho",
        "given": "Daniel E."
      },
      {
        "family": "Hong",
        "given": "Jenny"
      },
      {
        "family": "Hsu",
        "given": "Kyle"
      },
      {
        "family": "Huang",
        "given": "Jing"
      },
      {
        "family": "Icard",
        "given": "Thomas"
      },
      {
        "family": "Jain",
        "given": "Saahil"
      },
      {
        "family": "Jurafsky",
        "given": "Dan"
      },
      {
        "family": "Kalluri",
        "given": "Pratyusha"
      },
      {
        "family": "Karamcheti",
        "given": "Siddharth"
      },
      {
        "family": "Keeling",
        "given": "Geoff"
      },
      {
        "family": "Khani",
        "given": "Fereshte"
      },
      {
        "family": "Khattab",
        "given": "Omar"
      },
      {
        "family": "Koh",
        "given": "Pang Wei"
      },
      {
        "family": "Krass",
        "given": "Mark"
      },
      {
        "family": "Krishna",
        "given": "Ranjay"
      },
      {
        "family": "Kuditipudi",
        "given": "Rohith"
      },
      {
        "family": "Kumar",
        "given": "Ananya"
      },
      {
        "family": "Ladhak",
        "given": "Faisal"
      },
      {
        "family": "Lee",
        "given": "Mina"
      },
      {
        "family": "Lee",
        "given": "Tony"
      },
      {
        "family": "Leskovec",
        "given": "Jure"
      },
      {
        "family": "Levent",
        "given": "Isabelle"
      },
      {
        "family": "Li",
        "given": "Xiang Lisa"
      },
      {
        "family": "Li",
        "given": "Xuechen"
      },
      {
        "family": "Ma",
        "given": "Tengyu"
      },
      {
        "family": "Malik",
        "given": "Ali"
      },
      {
        "family": "Manning",
        "given": "Christopher D."
      },
      {
        "family": "Mirchandani",
        "given": "Suvir"
      },
      {
        "family": "Mitchell",
        "given": "Eric"
      },
      {
        "family": "Munyikwa",
        "given": "Zanele"
      },
      {
        "family": "Nair",
        "given": "Suraj"
      },
      {
        "family": "Narayan",
        "given": "Avanika"
      },
      {
        "family": "Narayanan",
        "given": "Deepak"
      },
      {
        "family": "Newman",
        "given": "Ben"
      },
      {
        "family": "Nie",
        "given": "Allen"
      },
      {
        "family": "Niebles",
        "given": "Juan Carlos"
      },
      {
        "family": "Nilforoshan",
        "given": "Hamed"
      },
      {
        "family": "Nyarko",
        "given": "Julian"
      },
      {
        "family": "Ogut",
        "given": "Giray"
      },
      {
        "family": "Orr",
        "given": "Laurel"
      },
      {
        "family": "Papadimitriou",
        "given": "Isabel"
      },
      {
        "family": "Park",
        "given": "Joon Sung"
      },
      {
        "family": "Piech",
        "given": "Chris"
      },
      {
        "family": "Portelance",
        "given": "Eva"
      },
      {
        "family": "Potts",
        "given": "Christopher"
      },
      {
        "family": "Raghunathan",
        "given": "Aditi"
      },
      {
        "family": "Reich",
        "given": "Rob"
      },
      {
        "family": "Ren",
        "given": "Hongyu"
      },
      {
        "family": "Rong",
        "given": "Frieda"
      },
      {
        "family": "Roohani",
        "given": "Yusuf"
      },
      {
        "family": "Ruiz",
        "given": "Camilo"
      },
      {
        "family": "Ryan",
        "given": "Jack"
      },
      {
        "family": "Ré",
        "given": "Christopher"
      },
      {
        "family": "Sadigh",
        "given": "Dorsa"
      },
      {
        "family": "Sagawa",
        "given": "Shiori"
      },
      {
        "family": "Santhanam",
        "given": "Keshav"
      },
      {
        "family": "Shih",
        "given": "Andy"
      },
      {
        "family": "Srinivasan",
        "given": "Krishnan"
      },
      {
        "family": "Tamkin",
        "given": "Alex"
      },
      {
        "family": "Taori",
        "given": "Rohan"
      },
      {
        "family": "Thomas",
        "given": "Armin W."
      },
      {
        "family": "Tramèr",
        "given": "Florian"
      },
      {
        "family": "Wang",
        "given": "Rose E."
      },
      {
        "family": "Wang",
        "given": "William"
      },
      {
        "family": "Wu",
        "given": "Bohan"
      },
      {
        "family": "Wu",
        "given": "Jiajun"
      },
      {
        "family": "Wu",
        "given": "Yuhuai"
      },
      {
        "family": "Xie",
        "given": "Sang Michael"
      },
      {
        "family": "Yasunaga",
        "given": "Michihiro"
      },
      {
        "family": "You",
        "given": "Jiaxuan"
      },
      {
        "family": "Zaharia",
        "given": "Matei"
      },
      {
        "family": "Zhang",
        "given": "Michael"
      },
      {
        "family": "Zhang",
        "given": "Tianyi"
      },
      {
        "family": "Zhang",
        "given": "Xikun"
      },
      {
        "family": "Zhang",
        "given": "Yuhui"
      },
      {
        "family": "Zheng",
        "given": "Lucia"
      },
      {
        "family": "Zhou",
        "given": "Kaitlyn"
      },
      {
        "family": "Liang",
        "given": "Percy"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "abstract": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.",
    "DOI": "10.48550/arxiv.2108.07258",
    "publisher": "arXiv",
    "title": "On the Opportunities and Risks of Foundation Models",
    "URL": "https://doi.org/hw3v",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2108.07258"
  },
  {
    "type": "article",
    "id": "OlEfQKqu",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neurons and Cognition (q-bio.NC)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences",
      "FOS: Biological sciences",
      "FOS: Biological sciences"
    ],
    "author": [
      {
        "family": "Ghosh",
        "given": "Arna"
      },
      {
        "family": "Mondal",
        "given": "Arnab Kumar"
      },
      {
        "family": "Agrawal",
        "given": "Kumar Krishna"
      },
      {
        "family": "Richards",
        "given": "Blake"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Representation learning that leverages large-scale labelled datasets, is central to recent progress in machine learning. Access to task relevant labels at scale is often scarce or expensive, motivating the need to learn from unlabelled datasets with self-supervised learning (SSL). Such large unlabelled datasets (with data augmentations) often provide a good coverage of the underlying input distribution. However evaluating the representations learned by SSL algorithms still requires task-specific labelled samples in the training pipeline. Additionally, the generalization of task-specific encoding is often sensitive to potential distribution shift. Inspired by recent advances in theoretical machine learning and vision neuroscience, we observe that the eigenspectrum of the empirical feature covariance matrix often follows a power law. For visual representations, we estimate the coefficient of the power law, $α$, across three key attributes which influence representation learning: learning objective (supervised, SimCLR, Barlow Twins and BYOL), network architecture (VGG, ResNet and Vision Transformer), and tasks (object and scene recognition). We observe that under mild conditions, proximity of $α$ to 1, is strongly correlated to the downstream generalization performance. Furthermore, $α\\approx 1$ is a strong indicator of robustness to label noise during fine-tuning. Notably, $α$ is computable from the representations without knowledge of any labels, thereby offering a framework to evaluate the quality of representations in unlabelled datasets.",
    "DOI": "10.48550/arxiv.2202.05808",
    "publisher": "arXiv",
    "title": "Investigating Power laws in Deep Representation Learning",
    "URL": "https://doi.org/gtb966",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2202.05808"
  },
  {
    "type": "article",
    "id": "zXrfFfft",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Tigas",
        "given": "Panagiotis"
      },
      {
        "family": "Annadani",
        "given": "Yashas"
      },
      {
        "family": "Jesson",
        "given": "Andrew"
      },
      {
        "family": "Schölkopf",
        "given": "Bernhard"
      },
      {
        "family": "Gal",
        "given": "Yarin"
      },
      {
        "family": "Bauer",
        "given": "Stefan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Causal discovery from observational and interventional data is challenging due to limited data and non-identifiability: factors that introduce uncertainty in estimating the underlying structural causal model (SCM). Selecting experiments (interventions) based on the uncertainty arising from both factors can expedite the identification of the SCM. Existing methods in experimental design for causal discovery from limited data either rely on linear assumptions for the SCM or select only the intervention target. This work incorporates recent advances in Bayesian causal discovery into the Bayesian optimal experimental design framework, allowing for active causal discovery of large, nonlinear SCMs while selecting both the interventional target and the value. We demonstrate the performance of the proposed method on synthetic graphs (Erdos-Rènyi, Scale Free) for both linear and nonlinear SCMs as well as on the \\emph{in-silico} single-cell gene regulatory network dataset, DREAM.",
    "DOI": "10.48550/arxiv.2203.02016",
    "publisher": "arXiv",
    "title": "Interventions, Where and How? Experimental Design for Causal Models at Scale",
    "URL": "https://doi.org/gtcfvk",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2203.02016"
  },
  {
    "type": "article",
    "id": "10mjWN2op",
    "categories": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Willig",
        "given": "Moritz"
      },
      {
        "family": "Zečević",
        "given": "Matej"
      },
      {
        "family": "Dhami",
        "given": "Devendra Singh"
      },
      {
        "family": "Kersting",
        "given": "Kristian"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Foundation models are subject to an ongoing heated debate, leaving open the question of progress towards AGI and dividing the community into two camps: the ones who see the arguably impressive results as evidence to the scaling hypothesis, and the others who are worried about the lack of interpretability and reasoning capabilities. By investigating to which extent causal representations might be captured by these large scale language models, we make a humble efforts towards resolving the ongoing philosophical conflicts.",
    "DOI": "10.48550/arxiv.2206.10591",
    "publisher": "arXiv",
    "title": "Can Foundation Models Talk Causality?",
    "URL": "https://doi.org/gtb9wb",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2206.10591"
  },
  {
    "type": "article",
    "id": "Ex1JrMxh",
    "categories": [
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Chevalley",
        "given": "Mathieu"
      },
      {
        "family": "Roohani",
        "given": "Yusuf"
      },
      {
        "family": "Mehrjou",
        "given": "Arash"
      },
      {
        "family": "Leskovec",
        "given": "Jure"
      },
      {
        "family": "Schwab",
        "given": "Patrick"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Causal inference is a vital aspect of multiple scientific disciplines and is routinely applied to high-impact applications such as medicine. However, evaluating the performance of causal inference methods in real-world environments is challenging due to the need for observations under both interventional and control conditions. Traditional evaluations conducted on synthetic datasets do not reflect the performance in real-world systems. To address this, we introduce CausalBench, a benchmark suite for evaluating network inference methods on real-world interventional data from large-scale single-cell perturbation experiments. CausalBench incorporates biologically-motivated performance metrics, including new distribution-based interventional metrics. A systematic evaluation of state-of-the-art causal inference methods using our CausalBench suite highlights how poor scalability of current methods limits performance. Moreover, methods that use interventional information do not outperform those that only use observational data, contrary to what is observed on synthetic benchmarks. Thus, CausalBench opens new avenues in causal network inference research and provides a principled and reliable way to track progress in leveraging real-world interventional data.",
    "DOI": "10.48550/arxiv.2210.17283",
    "publisher": "arXiv",
    "title": "CausalBench: A Large-scale Benchmark for Network Inference from Single-cell Perturbation Data",
    "URL": "https://doi.org/gtcbbc",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2210.17283"
  },
  {
    "type": "article",
    "id": "D3JIQ7Oe",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Goldblum",
        "given": "Micah"
      },
      {
        "family": "Finzi",
        "given": "Marc"
      },
      {
        "family": "Rowan",
        "given": "Keefer"
      },
      {
        "family": "Wilson",
        "given": "Andrew Gordon"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "No free lunch theorems for supervised learning state that no learner can solve all problems or that all learners achieve exactly the same accuracy on average over a uniform distribution on learning problems. Accordingly, these theorems are often referenced in support of the notion that individual problems require specially tailored inductive biases. While virtually all uniformly sampled datasets have high complexity, real-world problems disproportionately generate low-complexity data, and we argue that neural network models share this same preference, formalized using Kolmogorov complexity. Notably, we show that architectures designed for a particular domain, such as computer vision, can compress datasets on a variety of seemingly unrelated domains. Our experiments show that pre-trained and even randomly initialized language models prefer to generate low-complexity sequences. Whereas no free lunch theorems seemingly indicate that individual problems require specialized learners, we explain how tasks that often require human intervention such as picking an appropriately sized model when labeled data is scarce or plentiful can be automated into a single learning algorithm. These observations justify the trend in deep learning of unifying seemingly disparate problems with an increasingly small set of machine learning models.",
    "DOI": "10.48550/arxiv.2304.05366",
    "publisher": "arXiv",
    "title": "The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning",
    "URL": "https://doi.org/gtb9wp",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2304.05366"
  },
  {
    "type": "article",
    "id": "18RVn064W",
    "categories": [
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences",
      "I.2; I.2.6"
    ],
    "author": [
      {
        "family": "Carloni",
        "given": "Gianluca"
      },
      {
        "family": "Berti",
        "given": "Andrea"
      },
      {
        "family": "Colantonio",
        "given": "Sara"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Causality and eXplainable Artificial Intelligence (XAI) have developed as separate fields in computer science, even though the underlying concepts of causation and explanation share common ancient roots. This is further enforced by the lack of review works jointly covering these two fields. In this paper, we investigate the literature to try to understand how and to what extent causality and XAI are intertwined. More precisely, we seek to uncover what kinds of relationships exist between the two concepts and how one can benefit from them, for instance, in building trust in AI systems. As a result, three main perspectives are identified. In the first one, the lack of causality is seen as one of the major limitations of current AI and XAI approaches, and the \"optimal\" form of explanations is investigated. The second is a pragmatic perspective and considers XAI as a tool to foster scientific exploration for causal inquiry, via the identification of pursue-worthy experimental manipulations. Finally, the third perspective supports the idea that causality is propaedeutic to XAI in three possible manners: exploiting concepts borrowed from causality to support or improve XAI, utilizing counterfactuals for explainability, and considering accessing a causal model as explaining itself. To complement our analysis, we also provide relevant software solutions used to automate causal tasks. We believe our work provides a unified view of the two fields of causality and XAI by highlighting potential domain bridges and uncovering possible limitations.",
    "DOI": "10.48550/arxiv.2309.09901",
    "publisher": "arXiv",
    "title": "The role of causality in explainable artificial intelligence",
    "URL": "https://doi.org/gtb95k",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2309.09901"
  },
  {
    "type": "article",
    "id": "MhOZ3PWC",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Saengkyongam",
        "given": "Sorawit"
      },
      {
        "family": "Rosenfeld",
        "given": "Elan"
      },
      {
        "family": "Ravikumar",
        "given": "Pradeep"
      },
      {
        "family": "Pfister",
        "given": "Niklas"
      },
      {
        "family": "Peters",
        "given": "Jonas"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "The premise of identifiable and causal representation learning is to improve the current representation learning paradigm in terms of generalizability or robustness. Despite recent progress in questions of identifiability, more theoretical results demonstrating concrete advantages of these methods for downstream tasks are needed. In this paper, we consider the task of intervention extrapolation: predicting how interventions affect an outcome, even when those interventions are not observed at training time, and show that identifiable representations can provide an effective solution to this task even if the interventions affect the outcome non-linearly. Our setup includes an outcome Y, observed features X, which are generated as a non-linear transformation of latent features Z, and exogenous action variables A, which influence Z. The objective of intervention extrapolation is to predict how interventions on A that lie outside the training support of A affect Y. Here, extrapolation becomes possible if the effect of A on Z is linear and the residual when regressing Z on A has full support. As Z is latent, we combine the task of intervention extrapolation with identifiable representation learning, which we call Rep4Ex: we aim to map the observed features X into a subspace that allows for non-linear extrapolation in A. We show using Wiener's Tauberian theorem that the hidden representation is identifiable up to an affine transformation in Z-space, which is sufficient for intervention extrapolation. The identifiability is characterized by a novel constraint describing the linearity assumption of A on Z. Based on this insight, we propose a method that enforces the linear invariance constraint and can be combined with any type of autoencoder. We validate our theoretical findings through synthetic experiments and show that our approach succeeds in predicting the effects of unseen interventions.",
    "DOI": "10.48550/arxiv.2310.04295",
    "publisher": "arXiv",
    "title": "Identifying Representations for Intervention Extrapolation",
    "URL": "https://doi.org/gtb97m",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2310.04295"
  },
  {
    "type": "article",
    "id": "1DSO3BUly",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Han",
        "given": "Insu"
      },
      {
        "family": "Jayaram",
        "given": "Rajesh"
      },
      {
        "family": "Karbasi",
        "given": "Amin"
      },
      {
        "family": "Mirrokni",
        "given": "Vahab"
      },
      {
        "family": "Woodruff",
        "given": "David P."
      },
      {
        "family": "Zandieh",
        "given": "Amir"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to identify large entries, HyperAttention outperforms existing methods, giving significant speed improvements compared to state-of-the-art solutions like FlashAttention. We validate the empirical performance of HyperAttention on a variety of different long-context length datasets. For example, HyperAttention makes the inference time of ChatGLM2 50\\% faster on 32k context length while perplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k, with causal masking, HyperAttention offers 5-fold speedup on a single attention layer.",
    "DOI": "10.48550/arxiv.2310.05869",
    "publisher": "arXiv",
    "title": "HyperAttention: Long-context Attention in Near-Linear Time",
    "URL": "https://doi.org/gtb9wc",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2310.05869"
  },
  {
    "type": "article",
    "id": "4Dq1NQZ8",
    "categories": [
      "Machine Learning (cs.LG)",
      "Genomics (q-bio.GN)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences",
      "FOS: Biological sciences",
      "FOS: Biological sciences"
    ],
    "author": [
      {
        "family": "Tejada-Lapuerta",
        "given": "Alejandro"
      },
      {
        "family": "Bertin",
        "given": "Paul"
      },
      {
        "family": "Bauer",
        "given": "Stefan"
      },
      {
        "family": "Aliee",
        "given": "Hananeh"
      },
      {
        "family": "Bengio",
        "given": "Yoshua"
      },
      {
        "family": "Theis",
        "given": "Fabian J."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Advances in single-cell omics allow for unprecedented insights into the transcription profiles of individual cells. When combined with large-scale perturbation screens, through which specific biological mechanisms can be targeted, these technologies allow for measuring the effect of targeted perturbations on the whole transcriptome. These advances provide an opportunity to better understand the causative role of genes in complex biological processes such as gene regulation, disease progression or cellular development. However, the high-dimensional nature of the data, coupled with the intricate complexity of biological systems renders this task nontrivial. Within the machine learning community, there has been a recent increase of interest in causality, with a focus on adapting established causal techniques and algorithms to handle high-dimensional data. In this perspective, we delineate the application of these methodologies within the realm of single-cell genomics and their challenges. We first present the model that underlies most of current causal approaches to single-cell biology and discuss and challenge the assumptions it entails from the biological point of view. We then identify open problems in the application of causal approaches to single-cell data: generalising to unseen environments, learning interpretable models, and learning causal models of dynamics. For each problem, we discuss how various research directions - including the development of computational approaches and the adaptation of experimental protocols - may offer ways forward, or on the contrary pose some difficulties. With the advent of single cell atlases and increasing perturbation data, we expect causal models to become a crucial tool for informed experimental design.",
    "DOI": "10.48550/arxiv.2310.14935",
    "publisher": "arXiv",
    "title": "Causal machine learning for single-cell genomics",
    "URL": "https://doi.org/gtb97p",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2310.14935"
  },
  {
    "type": "article",
    "id": "17scpieH5",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Listgarten",
        "given": "Jennifer"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Since ChatGPT works so well, are we on the cusp of solving science with AI? Is not AlphaFold2 suggestive that the potential of LLMs in biology and the sciences more broadly is limitless? Can we use AI itself to bridge the lack of data in the sciences in order to then train an AI? Herein we present a discussion of these topics.",
    "DOI": "10.48550/arxiv.2312.00818",
    "publisher": "arXiv",
    "title": "The perpetual motion machine of AI-generated data and the distraction of ChatGPT-as-scientist",
    "URL": "https://doi.org/gs8pnp",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2312.00818"
  },
  {
    "type": "article",
    "id": "1GGrqZlzU",
    "categories": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Roth",
        "given": "Benedikt"
      },
      {
        "family": "Koch",
        "given": "Valentin"
      },
      {
        "family": "Wagner",
        "given": "Sophia J."
      },
      {
        "family": "Schnabel",
        "given": "Julia A."
      },
      {
        "family": "Marr",
        "given": "Carsten"
      },
      {
        "family": "Peng",
        "given": "Tingying"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "To handle the large scale of whole slide images in computational pathology, most approaches first tessellate the images into smaller patches, extract features from these patches, and finally aggregate the feature vectors with weakly-supervised learning. The performance of this workflow strongly depends on the quality of the extracted features. Recently, foundation models in computer vision showed that leveraging huge amounts of data through supervised or self-supervised learning improves feature quality and generalizability for a variety of tasks. In this study, we benchmark the most popular vision foundation models as feature extractors for histopathology data. We evaluate the models in two settings: slide-level classification and patch-level classification. We show that foundation models are a strong baseline. Our experiments demonstrate that by finetuning a foundation model on a single GPU for only two hours or three days depending on the dataset, we can match or outperform state-of-the-art feature extractors for computational pathology. These findings imply that even with little resources one can finetune a feature extractor tailored towards a specific downstream task and dataset. This is a considerable shift from the current state, where only few institutions with large amounts of resources and datasets are able to train a feature extractor. We publish all code used for training and evaluation as well as the finetuned models.",
    "DOI": "10.48550/arxiv.2401.04720",
    "publisher": "arXiv",
    "title": "Low-resource finetuning of foundation models beats state-of-the-art in histopathology",
    "URL": "https://doi.org/gtdf95",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2401.04720"
  },
  {
    "id": "iMrJ9TDG",
    "title": "Are Emergent Abilities of Large Language Models a Mirage?",
    "author": [
      {
        "given": "Rylan",
        "family": "Schaeffer"
      },
      {
        "given": "Brando",
        "family": "Miranda"
      },
      {
        "given": "Sanmi",
        "family": "Koyejo"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "URL": "https://openreview.net/forum?id=ITw9edRDlD",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: emergent-abilities",
    "type": "entry"
  },
  {
    "id": "WmT8ZU5I",
    "type": "book",
    "call-number": "B1481 .M55 2007",
    "collection-title": "Oxford world's classics",
    "event-place": "Oxford ; New York",
    "ISBN": "9780199211586",
    "note": "OCLC: ocm84995356\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: isbn:9780199211586",
    "number-of-pages": "238",
    "publisher": "Oxford University Press",
    "publisher-place": "Oxford ; New York",
    "source": "Library of Congress ISBN",
    "title": "An enquiry concerning human understanding",
    "author": [
      {
        "family": "Hume",
        "given": "David"
      },
      {
        "family": "Millican",
        "given": "P. F."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2007"
        ]
      ]
    }
  },
  {
    "id": "185mDnD0M",
    "type": "book",
    "abstract": "\"Everyone has heard the claim, \"Correlation does not imply causation.\" What might sound like a reasonable dictum metastasized in the twentieth century into one of science's biggest obstacles, as a legion of researchers became unwilling to make the claim that one thing could cause another. Even two decades ago, asking a statistician a question like \"Was it the aspirin that stopped my headache?\" would have been like asking if he believed in voodoo, or at best a topic for conversation at a cocktail party rather than a legitimate target of scientific inquiry. Scientists were allowed to posit only that the probability that one thing was associated with another. This all changed with Judea Pearl, whose work on causality was not just a victory for common sense, but a revolution in the study of the world\"--",
    "call-number": "Q175.32.C38",
    "event-place": "New York",
    "ISBN": "9780465097616",
    "number-of-pages": "1",
    "publisher": "Basic Books",
    "publisher-place": "New York",
    "source": "Library of Congress ISBN",
    "title": "The book of why: the new science of cause and effect",
    "title-short": "The book of why",
    "author": [
      {
        "family": "Pearl",
        "given": "Judea"
      },
      {
        "family": "Mackenzie",
        "given": "Dana"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: isbn:9780465097609"
  },
  {
    "type": "book",
    "id": "UFFRFvH4",
    "title": "ORGANON OR LOGICAL TREATISES O",
    "author": [
      {
        "literal": "Aristotle"
      },
      {
        "literal": "Octavius Freire 1816?-1873 Owen"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "ISBN": "9781372537233",
    "publisher": "Wentworth Press",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: isbn:9781372537233"
  },
  {
    "id": "712MuGug",
    "type": "book",
    "abstract": "\"In Myth and Measurement, David Card and Alan Krueger presented a powerful challenge to the conventional view that higher minimum wages necessarily reduce jobs for low-wage workers. In a work that has profoundly influenced public policy as well as the direction of economic research, the authors put standard economic theory to the test, using empirical methods and conducting a critical reexamination of existing literature on the minimum wage. Documenting the effects of the minimum wage on family earnings, poverty outcomes, and the stock market valuation of low-wage employers, they presented a wealth of evidence showing that increases in the minimum wage lead to increases in pay, but no loss in jobs. With a new preface discussing new data, Myth and Measurement continues to shift the terms of the debate on the minimum wage\"--Back cover",
    "edition": "Twentieth-anniversary edition",
    "event-place": "Princeton, New Jersey",
    "ISBN": "9781400880874",
    "language": "eng",
    "note": "OCLC: 928384513\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: isbn:9781400880874",
    "publisher": "Princeton University Press",
    "publisher-place": "Princeton, New Jersey",
    "source": "Open WorldCat",
    "title": "Myth and measurement: the new economics of the minimum wage",
    "title-short": "Myth and measurement",
    "author": [
      {
        "family": "Card",
        "given": "David E."
      },
      {
        "family": "Krueger",
        "given": "Alan B."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2016"
        ]
      ]
    }
  },
  {
    "id": "nT3xJkyD",
    "title": "No Free Lunch Theorems for Search",
    "issued": {
      "date-parts": [
        [
          1995
        ]
      ]
    },
    "author": [
      {
        "given": "David H.",
        "family": "Wolpert"
      },
      {
        "given": "William G.",
        "family": "Macready"
      }
    ],
    "URL": "https://ideas.repec.org/p/wop/safiwp/95-02-010.html",
    "publisher": "Santa Fe Institute",
    "number": "95-02-010",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: no-free-lunch",
    "type": "entry"
  },
  {
    "id": "ydpntqD3",
    "type": "webpage",
    "title": "The Bitter Lesson",
    "URL": "http://www.incompleteideas.net/IncIdeas/BitterLesson.html",
    "accessed": {
      "date-parts": [
        [
          "2024",
          1,
          17
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:http://www.incompleteideas.net/IncIdeas/BitterLesson.html"
  },
  {
    "id": "U6LC2Ufe",
    "type": "webpage",
    "title": "Stanford CRFM",
    "URL": "https://crfm.stanford.edu/",
    "accessed": {
      "date-parts": [
        [
          "2024",
          1,
          17
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://crfm.stanford.edu/"
  },
  {
    "id": "10AL1hWhU",
    "type": "article-journal",
    "abstract": "<p>On GPT-3: meta-learning, scaling, implications, and deep theory. The scaling hypothesis: neural nets absorb data &amp; compute, generalizing and becoming more Bayesian as problems get harder, manifesting new abilities even at trivial-by-global-standards-scale. The deep learning revolution has begun as foretold.</p>",
    "language": "en-us",
    "source": "gwern.net",
    "title": "The Scaling Hypothesis",
    "URL": "https://gwern.net/scaling-hypothesis",
    "author": [
      {
        "family": "Branwen",
        "given": "Gwern"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          1,
          17
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2020",
          5,
          28
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://gwern.net/scaling-hypothesis"
  },
  {
    "id": "eT8vyMzx",
    "type": "post-weblog",
    "language": "en-US",
    "title": "A Better Lesson – Rodney Brooks",
    "URL": "https://rodneybrooks.com/a-better-lesson/",
    "accessed": {
      "date-parts": [
        [
          "2024",
          1,
          17
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          3,
          19
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://rodneybrooks.com/a-better-lesson/"
  },
  {
    "id": "AT0GCO31",
    "type": "webpage",
    "abstract": "Thread by @shimon8282: \"Rich Sutton has a new blog post entitled “The Bitter Lesson” (incompleteideas.net/IncIdeas/Bitte…) that I strongly disagreth. In it, he argues that the history of AI teaches us that leveraging computation always eventually wins out over […]\"",
    "language": "en",
    "title": "Thread by @shimon8282: \"Rich Sutton has a new blog post entitled “The Bitter Lesson” (incompleteideas.net/IncIdeas/Bitte…) that I strongly disagree with. In it, he […]\"",
    "title-short": "Thread by @shimon8282",
    "URL": "https://threadreaderapp.com/thread/1106534178676506624.html",
    "author": [
      {
        "literal": "https://twitter.com/shimon8282"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          1,
          17
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://threadreaderapp.com/thread/1106534178676506624.html"
  },
  {
    "id": "iE5sGWcB",
    "type": "article-magazine",
    "abstract": "When it comes to one of humanity’s most important features, machines can grasp small patterns but not the unifying whole.",
    "container-title": "The New Yorker",
    "ISSN": "0028-792X",
    "language": "en-US",
    "source": "www.newyorker.com",
    "title": "The Uncanny Failure of A.I.-Generated Hands",
    "URL": "https://www.newyorker.com/culture/rabbit-holes/the-uncanny-failures-of-ai-generated-hands",
    "author": [
      {
        "family": "Chayka",
        "given": "Kyle"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          1,
          17
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          3,
          10
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://www.newyorker.com/culture/rabbit-holes/the-uncanny-failures-of-ai-generated-hands"
  }
]
